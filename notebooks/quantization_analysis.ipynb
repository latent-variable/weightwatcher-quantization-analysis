{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# WeightWatcher Quantization Analysis - Qwen2.5 0.5B\n\nThis notebook analyzes how different quantization levels (FP16, 8-bit, 4-bit, 2-bit) affect the **alpha (α)** metric computed by WeightWatcher.\n\n## Background\n\n**WeightWatcher** uses Heavy-Tailed Random Matrix Theory (HTRMT) to analyze neural network layers without needing training or test data. The key metric is **alpha (α)**:\n\n- α ∈ [2, 6]: Well-trained layer\n- α > 6: Undertrained or poorly regularized\n- α < 2: Over-regularized or corrupted\n\n**Research Question**: How does quantization compression affect these metrics?"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import sys\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM\n",
    "import weightwatcher as ww\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "\n",
    "# Set plotting style\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (14, 8)\n",
    "\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"WeightWatcher version: {ww.__version__}\")\n",
    "print(f\"Platform: {sys.platform}\")\n",
    "print(f\"MPS available: {torch.backends.mps.is_available() if sys.platform == 'darwin' else 'N/A'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Configuration\nMODEL_NAME = \"Qwen/Qwen2.5-0.5B-Instruct\"\nRESULTS_DIR = Path(\"../results/metrics\")\nRESULTS_DIR.mkdir(parents=True, exist_ok=True)\n\n# Device selection\nif torch.cuda.is_available():\n    DEVICE = \"cuda\"\nelif torch.backends.mps.is_available():\n    DEVICE = \"mps\"\nelse:\n    DEVICE = \"cpu\"\n\nprint(f\"Using device: {DEVICE}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "def analyze_model_ww(model, model_name, quantization):\n    \"\"\"Run WeightWatcher analysis on a model.\"\"\"\n    print(f\"Analyzing {quantization} model...\")\n    \n    watcher = ww.WeightWatcher(model=model)\n    results = watcher.analyze()  # WeightWatcher automatically computes alpha\n    \n    # Add metadata\n    results['model_name'] = model_name\n    results['quantization'] = quantization\n    \n    # Summary stats\n    print(f\"  Layers analyzed: {len(results)}\")\n    print(f\"  Alpha range: [{results['alpha'].min():.2f}, {results['alpha'].max():.2f}]\")\n    print(f\"  Alpha mean: {results['alpha'].mean():.2f}\")\n    \n    optimal = ((results['alpha'] >= 2) & (results['alpha'] <= 6)).sum()\n    print(f\"  Layers in optimal range [2,6]: {optimal}/{len(results)}\")\n    \n    return results\n\ndef clear_memory():\n    \"\"\"Clear GPU/MPS memory.\"\"\"\n    import gc\n    gc.collect()\n    if torch.cuda.is_available():\n        torch.cuda.empty_cache()\n    elif torch.backends.mps.is_available():\n        torch.mps.empty_cache()"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load and Analyze FP16 Model (Baseline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Loading FP16 model...\")\n",
    "model_fp16 = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"auto\",\n",
    "    low_cpu_mem_usage=True\n",
    ")\n",
    "\n",
    "print(f\"Model loaded: {model_fp16.__class__.__name__}\")\n",
    "print(f\"Total parameters: {sum(p.numel() for p in model_fp16.parameters()) / 1e9:.2f}B\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze FP16\n",
    "results_fp16 = analyze_model_ww(model_fp16, MODEL_NAME, \"fp16\")\n",
    "\n",
    "# Save results\n",
    "results_fp16.to_csv(RESULTS_DIR / \"results_fp16.csv\", index=False)\n",
    "print(\"\\nResults saved!\")\n",
    "\n",
    "# Display first few rows\n",
    "results_fp16.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean up\n",
    "del model_fp16\n",
    "clear_memory()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load and Analyze 8-bit Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "print(\"Loading 8-bit model...\")\n\nif sys.platform == \"darwin\":\n    # Mac: Use simulated quantization\n    print(\"  Using simulated 8-bit quantization (Mac-compatible)\")\n    from quantization_utils import apply_quantization_to_model\n    \n    model_8bit = AutoModelForCausalLM.from_pretrained(\n        MODEL_NAME,\n        torch_dtype=torch.float16,\n        device_map=\"auto\",\n        low_cpu_mem_usage=True\n    )\n    model_8bit = apply_quantization_to_model(model_8bit, bits=8, symmetric=True)\nelse:\n    # CUDA: Use bitsandbytes\n    print(\"  Using bitsandbytes (CUDA)\")\n    model_8bit = AutoModelForCausalLM.from_pretrained(\n        MODEL_NAME,\n        load_in_8bit=True,\n        device_map=\"auto\",\n        low_cpu_mem_usage=True\n    )\n\nprint(\"Model loaded!\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze 8-bit\n",
    "results_8bit = analyze_model_ww(model_8bit, MODEL_NAME, \"8bit\")\n",
    "\n",
    "# Save results\n",
    "results_8bit.to_csv(RESULTS_DIR / \"results_8bit.csv\", index=False)\n",
    "print(\"\\nResults saved!\")\n",
    "\n",
    "results_8bit.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean up\n",
    "del model_8bit\n",
    "clear_memory()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Load and Analyze 4-bit Model (if available)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only run on CUDA systems with bitsandbytes\n",
    "if torch.cuda.is_available():\n",
    "    print(\"Loading 4-bit model...\")\n",
    "    from transformers import BitsAndBytesConfig\n",
    "    \n",
    "    bnb_config = BitsAndBytesConfig(\n",
    "        load_in_4bit=True,\n",
    "        bnb_4bit_quant_type=\"nf4\",\n",
    "        bnb_4bit_compute_dtype=torch.float16,\n",
    "        bnb_4bit_use_double_quant=True\n",
    "    )\n",
    "    \n",
    "    model_4bit = AutoModelForCausalLM.from_pretrained(\n",
    "        MODEL_NAME,\n",
    "        quantization_config=bnb_config,\n",
    "        device_map=\"auto\",\n",
    "        low_cpu_mem_usage=True\n",
    "    )\n",
    "    \n",
    "    print(\"Model loaded!\")\n",
    "else:\n",
    "    print(\"4-bit quantization requires CUDA. Skipping...\")\n",
    "    model_4bit = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if model_4bit is not None:\n",
    "    # Analyze 4-bit\n",
    "    results_4bit = analyze_model_ww(model_4bit, MODEL_NAME, \"4bit\")\n",
    "    \n",
    "    # Save results\n",
    "    results_4bit.to_csv(RESULTS_DIR / \"results_4bit.csv\", index=False)\n",
    "    print(\"\\nResults saved!\")\n",
    "    \n",
    "    display(results_4bit.head(10))\n",
    "    \n",
    "    # Clean up\n",
    "    del model_4bit\n",
    "    clear_memory()\n",
    "else:\n",
    "    results_4bit = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Comparative Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect all results\n",
    "all_results = {\n",
    "    'FP16': results_fp16,\n",
    "    '8-bit': results_8bit,\n",
    "}\n",
    "\n",
    "if results_4bit is not None:\n",
    "    all_results['4-bit'] = results_4bit\n",
    "\n",
    "print(f\"Comparing {len(all_results)} quantization levels\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Alpha Distribution Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, len(all_results), figsize=(6*len(all_results), 5))\n",
    "\n",
    "if len(all_results) == 1:\n",
    "    axes = [axes]\n",
    "\n",
    "for ax, (quant, results) in zip(axes, all_results.items()):\n",
    "    ax.hist(results['alpha'], bins=30, alpha=0.7, edgecolor='black')\n",
    "    ax.axvspan(2, 6, alpha=0.2, color='green', label='Optimal [2,6]')\n",
    "    ax.axvline(results['alpha'].mean(), color='red', linestyle='--', \n",
    "               linewidth=2, label=f'Mean: {results[\"alpha\"].mean():.2f}')\n",
    "    \n",
    "    ax.set_title(f'Alpha Distribution - {quant}', fontsize=14, fontweight='bold')\n",
    "    ax.set_xlabel('Alpha (α)')\n",
    "    ax.set_ylabel('Number of Layers')\n",
    "    ax.legend()\n",
    "    ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('../results/plots/alpha_distributions_notebook.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Boxplot Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(12, 7))\n",
    "\n",
    "data = [results['alpha'] for results in all_results.values()]\n",
    "labels = list(all_results.keys())\n",
    "\n",
    "bp = ax.boxplot(data, labels=labels, patch_artist=True, notch=True, showmeans=True)\n",
    "\n",
    "colors = sns.color_palette('husl', len(data))\n",
    "for patch, color in zip(bp['boxes'], colors):\n",
    "    patch.set_facecolor(color)\n",
    "    patch.set_alpha(0.7)\n",
    "\n",
    "ax.axhspan(2, 6, alpha=0.15, color='green', label='Optimal Range [2,6]')\n",
    "ax.set_title('Alpha Distribution Comparison', fontsize=16, fontweight='bold')\n",
    "ax.set_ylabel('Alpha (α)', fontsize=14)\n",
    "ax.set_xlabel('Quantization Level', fontsize=14)\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('../results/plots/alpha_boxplot_notebook.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary Statistics Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_data = []\n",
    "\n",
    "for quant, results in all_results.items():\n",
    "    optimal = ((results['alpha'] >= 2) & (results['alpha'] <= 6)).sum()\n",
    "    \n",
    "    summary_data.append({\n",
    "        'Quantization': quant,\n",
    "        'Layers': len(results),\n",
    "        'Mean α': f\"{results['alpha'].mean():.3f}\",\n",
    "        'Std α': f\"{results['alpha'].std():.3f}\",\n",
    "        'Median α': f\"{results['alpha'].median():.3f}\",\n",
    "        'Min α': f\"{results['alpha'].min():.3f}\",\n",
    "        'Max α': f\"{results['alpha'].max():.3f}\",\n",
    "        'Optimal Layers': f\"{optimal}/{len(results)}\",\n",
    "        'Optimal %': f\"{(optimal/len(results)*100):.1f}%\"\n",
    "    })\n",
    "\n",
    "summary_df = pd.DataFrame(summary_data)\n",
    "print(\"\\nSUMMARY STATISTICS\")\n",
    "print(\"=\"*100)\n",
    "display(summary_df)\n",
    "\n",
    "summary_df.to_csv(RESULTS_DIR / '../quantization_comparison_notebook.csv', index=False)\n",
    "print(\"\\nSaved to: quantization_comparison_notebook.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Change from Baseline (FP16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline_mean = results_fp16['alpha'].mean()\n",
    "\n",
    "print(f\"Baseline (FP16) mean alpha: {baseline_mean:.3f}\\n\")\n",
    "print(\"Change from baseline:\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "for quant, results in all_results.items():\n",
    "    if quant == 'FP16':\n",
    "        continue\n",
    "    \n",
    "    mean_diff = results['alpha'].mean() - baseline_mean\n",
    "    pct_change = (mean_diff / baseline_mean) * 100\n",
    "    \n",
    "    print(f\"{quant:10s}: {mean_diff:+.3f} ({pct_change:+.2f}%)\")\n",
    "    \n",
    "    # Layer-wise correlation\n",
    "    if len(results) == len(results_fp16):\n",
    "        corr = results['alpha'].corr(results_fp16['alpha'])\n",
    "        print(f\"            Correlation with FP16: {corr:.3f}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Layer-wise Alpha Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(16, 7))\n",
    "\n",
    "colors = sns.color_palette('husl', len(all_results))\n",
    "\n",
    "for idx, (quant, results) in enumerate(all_results.items()):\n",
    "    x = range(len(results))\n",
    "    ax.plot(x, results['alpha'], marker='o', markersize=3, \n",
    "            linewidth=1.5, alpha=0.7, label=quant, color=colors[idx])\n",
    "\n",
    "ax.axhspan(2, 6, alpha=0.1, color='green', label='Optimal Range')\n",
    "ax.set_title('Layer-wise Alpha Comparison', fontsize=16, fontweight='bold')\n",
    "ax.set_xlabel('Layer Index', fontsize=14)\n",
    "ax.set_ylabel('Alpha (α)', fontsize=14)\n",
    "ax.legend(loc='best')\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('../results/plots/layerwise_comparison_notebook.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Insights and Conclusions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*80)\n",
    "print(\"KEY INSIGHTS\")\n",
    "print(\"=\"*80)\n",
    "print()\n",
    "print(\"1. Alpha Metric Stability:\")\n",
    "print(\"   - Alpha values indicate layer quality based on spectral properties\")\n",
    "print(\"   - Optimal range is [2, 6] for well-generalized layers\")\n",
    "print()\n",
    "print(\"2. Quantization Impact:\")\n",
    "for quant, results in all_results.items():\n",
    "    optimal_pct = ((results['alpha'] >= 2) & (results['alpha'] <= 6)).sum() / len(results) * 100\n",
    "    print(f\"   - {quant}: {optimal_pct:.1f}% of layers in optimal range\")\n",
    "print()\n",
    "print(\"3. Recommendations:\")\n",
    "print(\"   - If alpha degrades significantly, quantization may hurt generalization\")\n",
    "print(\"   - Layers with alpha > 6 after quantization may need special handling\")\n",
    "print(\"   - Consider layer-wise mixed precision for critical layers\")\n",
    "print()\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Export Results\n",
    "\n",
    "All results have been saved to:\n",
    "- `../results/metrics/results_*.csv` - Individual quantization results\n",
    "- `../results/plots/*.png` - Generated visualizations\n",
    "- `../results/quantization_comparison_notebook.csv` - Summary comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Analysis complete!\")\n",
    "print(f\"\\nResults saved to: {RESULTS_DIR}\")\n",
    "print(f\"Plots saved to: ../results/plots/\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}