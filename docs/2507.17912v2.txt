SETOL: A Semi-Empirical Theory of (Deep) Learning
Charles H. Martin∗ Christopher Hinrichs†
Abstract
We present a Semi-Empirical Theory of Learning (SETOL) that explains the remarkable performance of State-of-the-Art (SOTA) Neural Networks (NNs). We provide a formal explanation of the origin of the fundamental quantities in the phenomenological theory of Heavy-Tailed
Self-Regularization (HTSR), the Heavy-Tailed Power Law Layer Quality metrics, AlphaHat (α)
and AlphaHat (αˆ). In prior work, these metrics have been shown to predict trends in the
test accuracies of pretrained SOTA NN models, and, importantly, without needing access
to the testing or even training data. Our SETOL uses techniques from Statistical Mechanics
(StatMech) as well as advanced methods from Random Matrix Theory (RMT) and Quantum
Chemistry. Our derivation suggests new mathematical preconditions for Ideal learning, including the new ERG metric (which is equivalent to applying a single step of the Wilson Exact
Renormalization Group). We test the assumptions and predictions of our SETOL on a simple
3-layer Multi-Layer Perceptron (MLP), demonstrating excellent agreement with the key theoretical assumptions. For SOTA NN models, we show how to estimate the individual layer
Qualities of a trained NN by simply computing the Empirical Spectral Density (ESD) of the
layer weight matrices and then plugging this ESD into our SETOL formulae. Notably, we examine the performance of the HTSR α and the SETOL ERG Layer Quality metrics, and find
that they align remarkably well, both on our MLP and SOTA NNs.
∗Calculation Consulting, 8 Locksley Ave, 6B, San Francisco, CA 94122, charles@CalculationConsulting.com.
†Onyx Point Systems, chris@onyxpointsystems.com
1
arXiv:2507.17912v2 [cs.LG] 27 Jul 2025
Contents
1 Introduction 5
1.1 Statistical Mechanics (StatMech) vs. Statistical Learning Theory (SLT) . . . . . . . 5
1.2 Heavy-Tailed Self-Regularization (HTSR) . . . . . . . . . . . . . . . . . . . . . . . . . . 6
1.3 What is a Semi-Empirical Theory? . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 7
1.4 A Semi-Empirical Theory of Learning (SETOL) . . . . . . . . . . . . . . . . . . . . . . 8
2 Heavy-Tailed Self-Regularization (HTSR) 11
2.1 The HTSR Setup . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 11
2.2 Gaussian and Heavy-Tailed Universality . . . . . . . . . . . . . . . . . . . . . . . . . . 13
2.2.1 Random Matrix Theory (RMT): Marchenko-Pastur (MP) Theory and TracyWidom (TW) Fluctuations . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 14
2.2.2 Heavy-Tailed Random Matrix Theory (HTRMT) and Power Law (PL) fits . . 15
2.3 Data-Free Shape and Scale Quality Metrics . . . . . . . . . . . . . . . . . . . . . . . . 17
3 A Semi-Empirical Theory of (Deep) Learning (SETOL) 19
3.1 SETOL Overview . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 19
3.2 Comparing SETOL with HTSR: Conditions for Ideal Learning . . . . . . . . . . . . 23
3.3 Detecting Non-Ideal Learning Conditions . . . . . . . . . . . . . . . . . . . . . . . . . 24
3.3.1 Correlation Traps . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 25
3.3.2 Over-Regularization . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 27
4 Statistical Mechanics of Generalization (SMOG) 29
4.1 StatMech: the SMOG approach and the SETOL approach . . . . . . . . . . . . . . . . . 29
4.2 Mathematical Preliminaries of Statistical Mechanics . . . . . . . . . . . . . . . . . . . 31
4.2.1 Setup . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 32
4.2.2 BraKets, Expected Values, and Thermal Averages . . . . . . . . . . . . . . . 34
4.2.3 Free Energies and Generating Functions . . . . . . . . . . . . . . . . . . . . . 38
4.2.4 The Annealed Approximation (AA) and the High-Temperature Approximation (high-T) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 39
4.2.5 Average Training and Generalization Errors and their Generating Functions 41
4.2.6 The Quality (Q¯) and its Generating Function (ΓQ¯) . . . . . . . . . . . . . . . 43
4.2.7 The Thermodynamic limit . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 44
4.2.8 From the ST Perceptron to a Matrix Model. . . . . . . . . . . . . . . . . . . . 46
4.3 Student-Teacher Perceptron . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 48
4.3.1 Operational Setup . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 49
4.3.2 Theoretical Student-Teacher Average Generalization Error (E¯ST
gen) . . . . . . 55
5 Semi-Empirical Theory of the HTSR Phenomenology 60
5.1 Multi-Layer Setup: MLP3 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 61
5.1.1 Data-Dependent Multi-Layer ST Self-Overlap (η(S, T)) . . . . . . . . . . . . 62
5.1.2 A Single Layer Matrix Model . . . . . . . . . . . . . . . . . . . . . . . . . . . . 62
5.1.3 The Matrix-Generalized ST Overlap (η(S, T)). . . . . . . . . . . . . . . . . . 63
5.2 Quality Metrics of an Individual Layer as an HCIZ Integral . . . . . . . . . . . . . . 63
5.2.1 A Generating Function Approach to Average Quality-Squared of a Layer . . 63
5.2.2 Evaluating the Average Quality (Squared) Generating Function . . . . . . . 65
5.2.3 The Effective Correlation Space (ECS) . . . . . . . . . . . . . . . . . . . . . . . 66
5.2.4 Two Simplifying Assumptions: the IFA and ERG Condition . . . . . . . . . . 67
5.3 Evaluating the Layer Quality (Q¯) in the Large-N Limit . . . . . . . . . . . . . . . . 69
2
5.4 Modeling the R-Transform . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 70
5.4.1 Elementary Random Matrix Theory . . . . . . . . . . . . . . . . . . . . . . . . 70
5.4.2 Known R-transforms and Analytic (Formal) Models . . . . . . . . . . . . . . 71
5.4.3 Discrete Model: Bulk+Spikes, MHT, HT . . . . . . . . . . . . . . . . . . . . . 71
5.4.4 Free Cauchy Model (α = 2) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 73
5.4.5 Inverse Marchenko-Pastur Model of Ideal Learning . . . . . . . . . . . . . . . 73
5.4.6 The Multiplicative-Wishart (MW) model . . . . . . . . . . . . . . . . . . . . . 75
5.4.7 Levy-Wigner Models and the AlphaHat Metric . . . . . . . . . . . . . . . . . 75
5.4.8 Summary of heuristic models. . . . . . . . . . . . . . . . . . . . . . . . . . . . . 76
5.5 Computational Random Matrix Analysis . . . . . . . . . . . . . . . . . . . . . . . . . 78
6 Empirical Studies 79
6.1 HTSR Phenomenology: Predicting Model Quality via the Alpha metric . . . . . . . . 80
6.2 Testing the Effective Correlation Space . . . . . . . . . . . . . . . . . . . . . . . . . . . 82
6.2.1 Train and test errors by epochs . . . . . . . . . . . . . . . . . . . . . . . . . . . 83
6.2.2 Truncation and Generalization . . . . . . . . . . . . . . . . . . . . . . . . . . . 86
6.3 Evaluating the ERG Condition . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 86
6.3.1 The MLP3 model . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 87
6.3.2 State-of-the-Art (SOTA) models . . . . . . . . . . . . . . . . . . . . . . . . . . 89
6.4 Layer Qualities with Computational R-transforms . . . . . . . . . . . . . . . . . . . . 89
6.5 Inducing a Correlation Trap . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 90
6.6 Overloading and the Hysteresis Effect . . . . . . . . . . . . . . . . . . . . . . . . . . . 90
6.6.1 Baseline: Loading onto both FC1 and FC2 . . . . . . . . . . . . . . . . . . . . 91
6.6.2 Overloading FC1: Strongly Over-Parameterized Regime (n ≫ N × M) . . . 91
6.6.3 Overloading FC2: Under-Parameterized Regime (n ≪ N × M) . . . . . . . . 91
7 Conclusion and Future Directions 99
7.1 Future Directions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 101
A Appendix 110
A.1 Data Vectors, Weight Matrices, and Other Symbols . . . . . . . . . . . . . . . . . . . 110
A.2 Summary of the Statistical Mechanics of Generalization (SMOG) . . . . . . . . . . . . 113
A.2.1 Annealed Hamiltonian Han(R) when Student and Teachers are Vectors . . 113
A.2.2 Annealed Hamiltonian Han(R) for the Matrix-Generalized ST Error . . . . 115
A.3 Expressing the Layer Quality . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 119
A.4 Derivation of the ERG Condition . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 120
A.4.1 Setting up the Saddle Point Approximation (SPA) . . . . . . . . . . . . . . . 120
A.4.2 Casting the Generating Function (βΓ
IZ
Q¯2 ) as an HCIZ Integral . . . . . . . . 123
A.5 MLP3 Model Details . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 123
A.6 Tanaka’s Result . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 124
A.6.1 Setup and Outline . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 125
A.6.2 Step 1. Forming the Integral Transformation of ESD (ρ
∞
A(λ)) . . . . . . . . 127
A.6.3 Step 2: The Saddle Point Approximation (SPA): Explicitly forming the
Large Deviation Principle (LDP) . . . . . . . . . . . . . . . . . . . . . . . . . . 129
A.6.4 Expressing the Norm Generating Function (GA(λ)) as the Integrated Rtransform (R(z)) of the Correlation Matrix (A) . . . . . . . . . . . . . . . 132
A.7 Existence of the Free R–Transform for Power-Law Spectra . . . . . . . . . . . . . . . 134
A.7.1 Analyticity criterion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 134
A.7.2 Model and notation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 134
3
A.7.3 Stieltjes (Green’s) transform . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 135
A.7.4 Moments and free cumulants . . . . . . . . . . . . . . . . . . . . . . . . . . . . 135
A.7.5 R-transform for the bare tail . . . . . . . . . . . . . . . . . . . . . . . . . . . . 135
A.7.6 Truncated α = 2 power law . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 136
A.7.7 Key points and implications for SETOL . . . . . . . . . . . . . . . . . . . . . . . 136
A.7.8 Explicit R–transforms for the truncated tail . . . . . . . . . . . . . . . . . . . 136
A.8 The Inverse-MP (IMP) Model . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 138
A.8.1 The Branch Cut in the IMP Model . . . . . . . . . . . . . . . . . . . . . . . . 138
A.8.2 R(z)[IMP] is Complex Along the Branch Cut . . . . . . . . . . . . . . . . . 138
A.8.3 Calculation of G(λ)[IMP] . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 138
4
1 Introduction
Deep Neural Networks (DNNs)—models in the field of Artificial Intelligence (AI)—have driven
remarkable advances in multiple fields of science and engineering. AlphaFold has made significant
progress in solving the protein folding problem.[1] Notably, the 2024 Nobel Prize in Physics was
awarded to Hopfield and Hinton for developing early approaches to AI using techniques from
Statistical Mechanics (StatMech), and Jumper and Hassabis, along with Baker, received the
2024 Nobel Prize in Chemistry for their contributions to AlphaFold and computational protein
design.[2, 3] Self-driving cars now roam the streets of major metropolitan cities like San Francisco.
Large Language Models (LLMs) like ChatGPT have gained worldwide attention and initiated
serious conversations about the possibility of creating an Artificial General Intelligence (AGI).
Clearly, not a single area of science or engineering has ignored these remarkable advances in the
field of AI and Neural Networks (NNs).
Despite this remarkable progress in a research field spanning over 50 years, developing, training, and maintaining such complex models require staggering capital resources, limiting their
development to only the largest and best-funded organizations. While many such entities have
open-sourced some of their largest models (such as Llama and Falcon), using these models requires
assuming they have been trained optimally, without significant defects that could limit, skew, or
even invalidate their use downstream. Moreover, testing such models can be very expensive and
complex to interpret.
Because training and evaluating NNs is so hard, significant issues can manifest in many
obvious and non-obvious ways. A primary research goal is to improve the efficiency and reduce
the cost of training large NNs. A lesser-known but critical issue arises in many industrial settings,
specifically “selecting the best models to test.” This arises in industries such as ad-click prediction,
search relevance, quantitative trading, and more. Frequently, one has several seemingly equally
good models to choose from, but testing the model can be very expensive, time-consuming, and
even risky to the business. Recently, researchers and practitioners have started to fine-tune such
large open-source models using techniques such as LoRA and QLoRA. Such methods allow one
to adapt a large, open-source NN to a small dataset, and very cheaply. However, in fine-tuning,
one could unwittingly overfit the model to the small dataset, degrading its performance for its
intended use. Despite these and many other problems, theory remains well behind practice, and
there is an increasingly pressing need to develop practical predictive theory to both improve the
training of these very large NN models and to design new methods to make their use more reliable.
Before discussing these methods, however, let us explain What is a Semi-Empirical Theory
1.1 Statistical Mechanics (StatMech) vs. Statistical Learning Theory (SLT)
Historically, there have been two competing theoretical frameworks for understanding NNs: Statistical Mechanics (StatMech) [4, 5, 6, 7, 8, 9, 10]; and Statistical Learning Theory (SLT) [11].
• Statistical Mechanics (StatMech). This framework has been foundational to the early
development of NN models, such as the Hopfield Associative Memory (HAM) [12], Boltzmann Machines [13], [14], etc. StatMech has also been used to build early theories of
learning, such as the Student-Teacher model for the Perceptron Generalization Error [4, 5],
the Gardner model [6], and many others. Notably, the HAM was based on an idea by Little,
who observed that, in a simple model, long-term memories are stored in the eigenvectors
of transfer matrix [15]. (This general idea, but in a broader sense, is central to our approach below.) Moreover, StatMech predicts that NNs exhibit phase behavior. This has
recently been rediscovered as the Double Descent phenomenon [16, 17], but it was known
in StatMech long before it’s recent rediscovery [18]. However, unlike other applied physics
5
theories (e.g., Semi-Empirical methods in quantum chemistry), StatMech only offers qualitative analogies, failing to provide testable quantitative predictions about large, modern
NN models.[19]
• Statistical Learning Theory (SLT). SLT and related approaches (VC theory, PAC bounds
theory, etc.) have been developed within the context of traditional computational learning
problems [11], and they are based on analyzing the convergence of frequencies to probabilities (over problem classes, etc.). It was recognized early on, however, that they could not be
directly applied to NNs [20]. Moreover, SLT cannot even reproduce quantitative properties
of learning curves [21, 22] (whereas StatMech is very successful at this [8]). SLT also failed
to predict the “Double Descent” phenomenon [16]. More recently, it has been shown that
in practical settings SLT can give vacuous [23] or even opposite results to those actually
observed [24].
Technically, SLT focuses on obtaining bounds on a model’s worst-case behavior, while StatMech
seeks a probabilistic understanding of typical behaviors across different states or configurations.
Unfortunately, neither of these general theoretical frameworks has proven particularly useful to
NN practitioners. SETOL combines insights from both. Rather than being purely phenomenological like the HTSR approach, SETOL is derived from first-principles, and in the form of a SemiEmpirical theory. As such, SETOL offers a practical, Semi-Empirical framework that bridges
rigorous theoretical modeling and empirical observations for modern NNs.
1.2 Heavy-Tailed Self-Regularization (HTSR)
HTSR theory is an approach that combines ideas from StatMech with those of Heavy-Tailed Random Matrix Theory (RMT), providing eigenvalue-based quality metrics that correlate with model
quality (i.e., out-of-sample performance). HTSR theory posits that well-trained models have extracted subtle correlations from the training data, and that these correlations manifest themselves
in the Shape and Scale of the eigenvalues of the layer weight matrices W. In particular, if one
computes the empirical distribution of the eigenvalues, λi
, of an individual N ×M weight matrix,
W, then this density, ρ
emp(λ), which is an ESD, is Heavy-Tailed (HT) and can be well-fit to
a Power Law (PL), i.e., ρ(λ) ∼ λ
−α
, with exponent α. HTSR theory provides a phenomenology
for qualitatively-distinct phases of learning [25]. It can, however, also be used to define Layerlevel Quality metrics and Model-level Quality metrics: e.g., the Alpha (α) and AlphaHat (αˆ) PL
metrics, described below.
Not needing any training data, HTSR theory has many practical uses. It can be directly
applicable to large, open-source models where the training and test data may not be available.
Model quality metrics can be used, e.g., to predict trends in the quality of SOTA models in
computer vision (CV) [26] and natural language processing (NLP) [27, 28], both during and after
training, and without needing access to the model test or training data. Layer quality metrics can
be used to diagnose potential internal problems in a given model, or (say) to accelerate training
by providing optimal layer-wise learning rates [29] or pruning ratios [30]. Most notably, the HTSR
theory provides Universal Layer Quality metrics encapsulated in what appears to be a critical
exponent, α = 2, that is empirically associated with optimal or Ideal Learning. Moreover, as
argued below, the value α = 2 appears to define a phase boundary between a generalization and
overfitting, analogous to the phase boundaries seen in StatMech theories of NN learning.
These results both motivate the search for a first principles understanding of the HTSR theory,
and suggest a path for developing a practical predictive theory of Deep Learning. For this,
however, we need to go beyond the phenomenology provided by HTSR theory, to relate it to
some sort of (at least semi-rigorous/semi-empirical) derivations based on the StatMech theory
6
of learning, and drawing upon previous success (in Quantum Chemistry) in developing a first
principles Semi-Empirical theory.
1.3 What is a Semi-Empirical Theory?
Historically, one of the most well known Semi-Empirical methods comes from Nuclear Physics.
The Semi-Empirical Mass Formula, dating back to 1935, is based on the heuristic Liquid Drop
Model of the nucleus, and it was used to predict experimentally observed binding energies of
nucleons. This model describes nuclear fission, and it was central to its development of the
atomic bomb:
Prior to WWII, Nuclear Physics was a phenomenological science, which relied upon
experimental data and descriptive models [31].
In the Post-war era, the epistemological nature of nuclear theory changed, as it saw the development of Semi-Empirical shell models of the nucleus. These models were formulated with rigor (in
the physics sense) but also relied on heuristic assumptions and experimental data for accurate
predictions. They captured the structure of atomic nuclei and could accurately describe various
nuclear properties [32, 33, 34]. The shell models, analogous to the electronic shell structure of
atoms, represented a shift toward a more rigorous understanding of nuclear phenomena.
About this time, RMT itself was also introduced by Wigner [35] to model the statistical patterns
of the nuclear energy spectra of strongly interacting heavy nuclei. These patterns were universal,
independent of the specific nucleus, suggesting that a probabilistic approach would be fruitful.
In the following decades, RMT saw many advances, including the development of the MarchenkoPastur model [36], and numerous other applications in physics [37]. By the 1990s, RMT was
further expanded when Zee introduced the Blue Function, and reinterpreted the R-transform as
a self-energy within the framework of many-body / quantum field theory (QFT) [38]. Also, socalled HCIZ integrals, integrals over random matrices, were being used both to model disordered
electronic spectra [39], and, later, the behavior of spin glass models [40, 41].
Returning to the 1950s, and prior to the development of highly accurate, modern, computational ab initio theories of Quantum Chemistry, Theoretical chemists introduced the SemiEmpirical PPP method for conjugated polyenes [42]. The PPP model recasts the electronic
structure problem as an Effective Hamiltonian for the π-electrons. The PPP model resembles
the later developed tight-binding model of condensed matter physics[43]. For many year,s this
and related Semi-Empirical methods worked remarkably well, even better than the existing ab
initio theories [44, 45, 46, 47, 48]. Most importantly, these methods could be fit on a broad set
of empirical molecular data, and then applied to molecules not in the original training set.
Around the same time, Löwdin first formalized the concept of the Effective Hamiltonian,
which allowed the reduction of complex many-body problems to simplified Effective Potentials
that still captured the essential physics. Then, in the late 1960s, Brandow developed an Effective
Hamiltonian theory of nuclear structure, leveraging the Linked Cluster Theorem (LCT) (see [49])
and quantum mechanical many-body theory to describe the highly correlated effective interactions
in a reduced model space. 1
Like modern NNs, these Semi-Empirical methods of Quantum Chemistry worked well beyond
their apparent range of validity, generalizing very well to out-of-distribution (OOD) data. This
led to the search for a Semi-Empirical Theory to explain the remarkable performance of these
1Note also that the LCT shows that the log partition function (i.e., lnZ) can be expressed a sum of connected
diagrams, which is very similar to our result below, which expresses the log partition function here as a sum of
matrix cumulants from RMT.
7
phenomenological methods. Building on Brandow’s many-body formalism, Freed and collaborators [50, 51] developed an ab initio Effective Hamiltonian Theory of Semi-Empirical methods
to explain the remarkable success of the Semi-Empirical methods. Specifically, the values of the
PPP empirical parameters could be directly computed by way of effective interactions, including
both renormalized self-energies and higher-order terms. Somewhat later, in the 1990s, Martin et.
al. [52, 53, 54, 55] extended and applied this Effective Hamiltonian theory and demonstrated the
Universality of the Semi-Empirical PPP parameters numerically. Indeed, it is this Universality
that enabled the ‘for a time’ inexplicable OOD performance of these methods. Crucially, this
decades-long line of work established a comprehensive analytic and numerical Theory of SemiEmpirical methods. That is, a framework that confirmed the empirically observed Universality
provided theoretical justification for this, and enabled systematic improvements of the methods
using numerical techniques.
Finally, it is important to mention the Effective Hamiltonian approach provided by the Wilson Renormalization Group (RG) [56, 57]. The RG approach provides a powerful framework for
studying strongly correlated systems across different scales, enabling the construction of an Effective Hamiltonian by integrating out weakly-correlated degrees of freedom in a Scale-Invariant way.
It is particularly suited for critical points and phase boundaries – such as the phase boundary
between generalization and memorization in spin glass models of neural networks – and, importantly, predicts the existence of Universal Power Law (PL) exponents . For SETOL, we take what
amounts to a single step of the Exact Renormalization Group (ERG), leading to an new empirical
metric that defines the ’Ideal’ NN layer.
Relevance to Deep Learning In this sense, Semi-Empirical theories of Nuclear Physics and
Quantum Chemistry, (as well as the Renormalization Group approach), seem particularly appropriate for Deep Learning. DNN models are complex black boxes that defy statistical descriptions
in that they are commonly pre-trained on a large set of data; and than applied to new data sets
in new domains via transfer learning. Most recently, the inexplicable success of transfer-learning
is seen in the GPT (Generative Pre-Trained Transformer) models [58], and motivated early work
by Jumper et. al. on protein folding [59].
In contrast, these Semi-Empirical approaches differ from more recently developed theoretical
approaches to deep learning, which are typically based on SLT, rather than StatMech [60]. In
particular, there have recently appeared several theories of deep learning, formulated using ideas
from RMT. However, regarding realistic models, it has been explicitly stated that “These networks
are however too complex in general for developing a rigorous theoretical analysis on the spectral
behavior” [61]. Even in recent work applying RMT to NNs, it has been noted “that we make no
claim about trained weights, only random weights” [62]. The weight matrices of a trained NN,
however, are clearly not simply random matrices—since they encode the specific correlations from
the training data.
1.4 A Semi-Empirical Theory of Learning (SETOL)
We propose SETOL, a Semi-Empirical Theory for Deep Learning Neural Networks (NNs), as
both a theoretical foundation for HTSR phenomenology and a novel framework for predicting the
properties of complex NN models. This unified framework offers a deeper understanding of DNN
generalization through a Semi-Empirical approach inspired by many-body physics, combined
with a classic StatMech model for NN generalization. Specifically, SETOL combines theoretical
and empirical insights to evaluate Model Quality, showing that the weightwatcher layer HTSR PL
metrics (Alpha and AlphaHat) can be derived using a phenomenological Effective Hamiltonian
approach. This approach expresses the HTSR Layer Quality in terms of the RMT matrix cumulants
8
of the layer weight matrix W, and is governed by a Scale-Invariant transformation equivalent to
a single step of an Exact Renormalization Group (ERG) transformation. Here, we derive this from
first principles, requiring no previous knowledge of statistical physics.
The SETOL approach unifies the HTSR principles with a broader theoretical framework for layer
analysis. The HTSR theory identifies Universality (e.g., α = 2) as a hallmark of the best-trained
DNN layers, and, here, our SETOL introduces the closely related Exact Renormalization Group
Condition, a Scale-Invariant or Volume-Preserving transformation that reflects an underlying
Conservation Principle. Together, these principles form the theoretical foundation for deriving
HTSR Layer Quality metrics from first principles. By leveraging techniques from StatMech and
modern RMT, SETOL offers a rigorous framework to connect empirical observations with theoretical
predictions, advancing our understanding of generalization in neural networks.
• Derivation of the HTSR Layer Quality metrics Alpha and AlphaHat. The SETOL
approach takes as input the Empirical Spectral Density (ESD) of the layers of trained NN,
and derives an expression for the approximate Average Generalization Accuracy of a multilayer NN. We call this approximation the Model Quality, denoted Q¯NN . This Model Quality
is expressed as a product of individual Layer Quality terms, Q¯NN
L
, which themselves can
then be directly related to the HTSR Power Law (PL) empirical Alpha (α) and AlphaHat
(αˆ = αlog10 λmax) metrics.
In particular, the Layer Quality-Squared, Q¯2 ≈ [Q¯NN
L
]
2
, is expressed as the logarithm of an
HCIZ integral, which is the Thermal Average of an Annealed Error Potential for a matrixgeneralized form of the Linear Student-Teacher model of classical StatMech. This HCIZ
integral evaluates into the sum of integrated R-transforms from RMT, or, equivalently, as a
sum of integrated matrix cumulants. From this, the HTSR AlphaHat metric can be derived
in the special case of Ideal Learning. 2
• Discovery of a Mathematical Condition for Ideal Learning. By Ideal Learning,
we mean that the specific NN layer has optimally converged, capturing as much of the
information as possible in the training data without overfitting to any part of it. In defining
this and deriving our results, we have discovered (and are proposing) a new condition for
Ideal Learning, which is associated with the Universality of the HTSR theory:
– HTSR Condition for Ideal Learning. This HTSR theory states that a NN layer is
Ideal when the ESD can be well fit to a Power Law (PL) distribution, with PL exponent
α = 2. Importantly, this appears to be a Universal property of all well-trained NNs,
independent of the training data, model architecture, and training procedure.
– SETOL ERG Condition for Ideal Learning. The SETOL condition for Ideal Learning
states that the dominant eigencomponents associated with the ESD of layer form a
reduced-rank Effective Correlation Space (ECS) that satisfies a new kind of Conservation Principle, Scale-Invariant Volume Preserving Transformation where the largest
eigenvalues λ˜
i of the ECS satisfy the condition ln ∏ λ˜
i = ∑ ln λ˜
i = 0. This is called the
ERG Condition. The ERG Condition is equivalent to the taking a single step of the
Wilson Exact Renormalization (ERG).
The HTSR Condition has been proposed and analyzed previously [25, 26, 28]; but the ERG
Condition is new, based on our SETOL theory. When these two conditions align, we propose
the NN layer is in the Ideal state.
2The SETOL approach to the HTSR theory resembles in spirit the derivation of the Semi-Empirical PPP models
using the Effective Hamiltonian theory, where each phenomenological parameter is associated with a renormalized
effective interaction, expressed as a sum of linked diagrams or clusters.[53, 55]
9
• Experimental Validation. We present detailed experimental results on a simple model,
along with observations on large-scale pretrained NNs, to demonstrate that the HTSR conditions for ideal learning (α = 2) are experimentally aligned with the independent SETOL
condition for ideal learning (det (X˜ ) = 1). See Section. 6.3. Our primary objective here
is not to demonstrate performance improvements on SOTA NNs—this has been previously
established [29]. Instead, our aim is to validate the theoretical assumptions of SETOL,
test the predictions of the SETOL framework, and examine the new, independent
learning conditions we discovered—on a model that is sufficiently simple that we can
evaluate and stress test the theory.
• Observations on Overfit Layers (α < 2). Being a Semi-Empirical theory, SETOL can also
identify violations of it’s assumptions. For example, when empirical results show α < 2 for a
single layer, the layer’s ESD falls into the HTSR Very Heavy-Tailed (VHT) Universality class.
(See Section 6.6.) When this happens, the layer may be slightly overfit to the training data,
resulting in suboptimal performance and potentially even exhibiting hysteresis-like
effects (memory effects)—that we observe empirically. These effects indicate that overfit
layers may retain memory-like behavior, affecting learning dynamics and generalization.
10
2 Heavy-Tailed Self-Regularization (HTSR)
In this section, we provide an overview of the HTSR phenomenology. 3 HTSR has been presented
in detail previously [63, 64, 25].Here, we provide a self-contained summary, with an emphasis
on certain technical issues that will be important for our SETOL. We highlight its practical application for interpreting observed behaviors in trained weight matrices, and we distinguish the
HTSR phenomenology from the analytical methods used in the SETOL approach. In Section 2.1,
we summarize the basic HTSR setup and results; in Section 2.2, we summarize Gaussian (for RMT)
and Heavy-Tailed (for HTRMT) Universality; and in Section 2.3, we describe Shape metrics and
Scale metrics that arise from HTSR. (Here, we focus on basic methods for identifying HT correlations in the ESDs of pre-trained weight matrices; in Sections 6.1, 6.5 and 6.6, we show detailed
experiments using theoretical constructs from the HTSR phenomenology.)
2.1 The HTSR Setup
We can write the Energy Landscape function (or NN output function) for a NN with L layers as
E
out
NN ∶= hL(WL × hL−1(WL−1 × (⋯) + bL−1) + bL) (1)
with activation functions hl(⋅), and with weight matrices and biases Wl and bl
.
4 For simplicity
of exposition here (HTSR can be applied much more broadly), we ignore the structural details of
the layers (dense or not, convolutions or not, residual/skip connections, etc.). We also ignore the
biases bl (because they can be subsumed into the weight matrices), and we treat each layer as
though it contains a single weight matrix WL. We imagine training (or fine-tuning) this model on
labeled data {xµ, yµ} ∈ D, where xµ is the µ-th input vector and yµ is its corresponding label (e.g.,
for binary classification, yµ ∈ {−1, 1}). We expect to use backprop via some variant of stochastic
gradient descent (SGD) to minimize some loss functional, L (such as ℓ2, cross-entropy, etc.):
argmin
Wl
,bl
∑
µ
L[E
out
NN (xµ), yµ] + Ω, (2)
where Ω denotes some explicit regularizer (such as an ℓ1 or ℓ2 constraint on layer weight matrices) or some implicit regularization procedure (such as clipping the weight matrices or applying
dropout).
Given a real-valued N × M layer weight matrix W (dropping the subscript), let X be the
M × M layer Correlation Matrix:
X ∶=
1
N
W⊺W. (3)
The Empirical Spectral Density (ESD) of W, denoted ρ
emp(λ), is formed from the M eigenvalues
λj of X:
ρemp(λ) ∶=
M
∑
j=1
δ(λ − λj). (4)
Given a model, we can compute the ESDs of all of its layers, as well as other metrics below, with
the open-source WeightWatcher tool [67].5
3We may also refer to the HTSR phenomenology as the HTSR Theory; we use the term phenomenology to
emphasize its empirical nature, and to distinguish it from the analytical methods used in the SETOL approach.
4The Energy Landscape function E
out
NN acts on a data instance and generates a list of energies, or un-normalized
probabilities; [25]. This notation was chosen to make an analogy with Random Energy Models (REM) from spinglass and protein folding theories [65, 66].
5For practical purposes, the WeightWatcher tool computes ρ
emp(λ) by forming the Singular Value Decomposition (SVD) of the layer weight matrices W, computing the eigenvalues λ = σ
2
from the singular values σ, and
(when useful) smoothing them with a Kernel Density Estimator (KDE). For some calculations, such as the ERG
condition, we must also select the appropriate normalization of W.
11
(a) Log-Log ESD (b) Lin-Lin ESD
(c) Log-Lin ESD (d) λ0 vs KS distance
Figure 1: Fitting ESDs within HTSR. Depiction of the ESD and results of PL fits for a typical
well-trained layer of a modern NN (FC3 of VGG19), including both the actual and good PL fit
(red) and a hypothetical bad PL fit (purple). The same ESD is plotted on a Log-Log (a), Lin-Lin
(b) and Log-Lin (c) scales. (d) depicts how the start of the PL tail, λ0, varies with the quality
of the PL fit (the DKS distance). All plots are generated using the open-source WeightWatcher
tool. See the main text for details.
Based on empirical results based on thousands of pretrained models and tens of thousands of
layers [25, 26, 24, 28], it is generally observed that the best performing NNs have ESDs that are
HT, and the tails of these ESDs, ρtail(λ), can be well fit to a PL, beyond some cutoff λ ≥ λ0.
6
For a PL fit,
ρtail(λ) ∶= ρ
emp(λ ≥ λ0) ∼ λ
−α
, (5)
where λ0 is where the tail of the ESD starts (i.e., it is not the minimum eigenvalue, but the
minimum eigenvalue in the tail of the ESD). See Figure 1. As such, the tail of the ESD “starts”
at some value λ0, called xmin here, and it continues until the maximum eigenvalue λmax, called
xmax here (labeled xmax in the figure, shown by the orange line). We estimate xmin and α
jointly, using the method of [68], as implemented in the powerlaw python package [69], which is
also integrated into the open-source WeightWatcher tool [26, 67].7
6Doing a large meta-analysis like this is tricky; but see [25, 26, 24, 28]. The WeightWatcher tool provides a
systematic, reproducible way to compute a PL fit (using an MLE method of Clauset et al. [68]), as well other
model metrics, including the SpectralNorm, Rand-Distance, and AlphaHat metrics [26]. Also, the ESD ρ(λ)tail
is sometimes better fit by a Truncated Power Law (TPL), due to finite-size effects. (Again, this is important in
practice, but we ignore this complexity in this initial discussion of SETOL. )
7The authors of [70] failed to find evidence of a PL-like distribution in NN weight matrices, which is likely to
be the case when α and xmin are not estimated jointly, as can be seen in Figure 1(d).
12
HT/RMT Universality class µ range α range Best Fit
RandomLike NA NA MP
Bulk+Spikes NA NA MP+Spikes
Weakly Heavy Tailed µ > 4 α > 6 PL
Heavy (Fat) Tailed µ ∈ (2, 4) α ∈ (2, 6) PL
Very Heavy Tailed µ ∈ (0, 2) α ∈ (1, 2) (T)PL
Rank Collapse NA NA NA
Table 1: HTSR Heavy-Tailed Universality classes of RMT. See Table 1 of [25] for more details.
Fitting ESDs. Choosing the start of the tail, λ0, is important for HTSR (and it will be very
important for SETOL, as we will describe below). See Figure 1 for a depiction of how this was done
within HTSR theory. Figures 1(a)-1(c) show the results of both a “good fit” and a “bad fit” on the
same ESD, while Figure 1(d) indicates the quality of fit. For the good fit, the start of the tail is
the optimal value λ0 = xmin (in red); and for the bad fit, it is a suboptimal bad xmin (purple).
Figure 1(d) depicts how the best fit is determined; it plots xmin = λ0 versus the DKS value,
which is the Kolmogorov-Smirnov (KS) distance between the PL fit and the empirical data [68].
Notice that there are two nearly degenerate minima on Figure 1(d), corresponding to the good
fit and the bad fit. It is not uncommon to face such practical challenges, as real-world ESDs
are often slightly deformed from a perfect PL density, e.g., they may have two or more neardegenerate solutions on the KS plots (d). (They may also have anomalously large eigenvalues;
this is discussed in more detail in Section 3.3.)
When one finds a good PL fit for the ESD of a layer W, it provides information about
the Shape and Scale of the ESD of that layer. In particular: the SpectralNorm, λmax, being
a matrix norm, is a measure of the size Scale of the ESD [24]; the fitted PL exponent Alpha,
α, being the slope of the tail of the ESD on a Log-Log plot, describes the Shape of the ESD;
and the WeightWatcher AlphaHat metric combines Shape and Scale information. Also, as opposed to other applications of PL fits [68, 71], in our analysis, the start of the tail, λ0 = λ
P L
min,
plays a particularly important role because it identifies the subspace of the strongest generalizing
eigencomponents (i.e., X˜ , below) in each layer.
2.2 Gaussian and Heavy-Tailed Universality
The HTSR phenomenology uses RMT to classify of the ESD of a layer W into one of 5+1 Phases
of Training, each roughly corresponding to a (Gaussian or HT) Universality class (of RMT or
HTRMT). This is summarized in Table 1. A Universality class is a set of matrices having a common
limiting spectral distribution, regardless of the other properties of their entries. Of those, the
most familiar is the Gaussian class, characterized by the Marchenko Pastur (MP) results from
traditional RMT [72, 73]. The Gaussian Universality class, however, is particularly poorly suited
for analyzing realistic NNs—precisely because the ESDs of SOTA NNs are well-fit by HT distributions. This should not be surprising: weight matrices of realistic NNs do not have independent
(i.i.d.) entries—their entries are strongly-correlated precisely because they provide a view into
the correlated training data.
To model strongly-correlated NN layer matrices, the HTSR phenomenology characterizes NN
layer weight matrices in terms of their ESDs (when a good PL fit can be found) by postulating
that the (tail of the) eigenvalue spectrum ρ(λ) determines how each layer contributes to the
overall generalization. To do this, the HTSR approach models the strong-correlated layer weight
matrices as if they are actually i.i.d. HT random (i.e., entry-wise uncorrelated) matrices. By
13
(a) MP, varying Q =
N
M (b) Tracy Widom fluctuations
Figure 2: MP distributions for different aspect ratios Q and variance scales σ
2
, and an example
of the finite-sized TW fluctuation ∆TW .
doing this, one can associate each ρ(λ) with the corresponding HT Universality class, according
to the PL exponent α fitted from the ESD. As we will see in Section 3.3, it can be critical to
distinguish when the ESD is HTCorrelation-wise vs HT Element-wise.
To understand Table 1 better, we first review basic results.
2.2.1 Random Matrix Theory (RMT): Marchenko-Pastur (MP) Theory and TracyWidom (TW) Fluctuations
The Marchenko-Pastur (MP) distribution predicts the (limiting) Shape of an ESD, ρMP (λ), when
the layer weight matrix has elements that are i.i.d. random from the Gaussian Universality class.
In particular, the ESD will be MP when the matrix elements are drawn from a Normal distribution
Wi,j ∈ N(0, σ2
), e.g., as is typical at initialization, before NN training begins. Figure 2 (from
Figure 4 of [25]) displays the MP distribution for different aspect ratios Q =
N
M and variances σ
2
.
Notice that the Shape is characterized by a well-defined, compact envelope with sharp edges.
The MP distribution also predicts the Scale of an ESD, again when the layer weight matrix
has elements that are i.i.d. random from the Gaussian Universality class. In particular, an
MP distribution, ρMP (λ), has very crisp, well-defined lower and upper bounds λ
−
, λ+
[25], and
(importantly) the upper bound λ
+
exhibits finite-size Tracy-Widom (TW) fluctuations, ∆TW (λ),
which are on the order of O(M−2/3
). Thus, any layer eigenvalue with Scale greater than this, i.e.,
λ > [λ
+ + ∆TW (λ)], is an “outlier” or a “spike.”
According to the HTSR phenomenology, these spikes carry significant generalizing information.
(This is well-known for Bulk-Plus-Spike models [25], but the HTSR phenomenology generalizes
this concept.) Relatedly, for layer matrices W with aspect ratio Q > 1 (i.e., rectangular matrices,
where N > M), MP RMT predicts there should be no zero eigenvalues, i.e., λi > 0, for all i.
Generally speaking, for well trained NNs, for layers with Q > 1, all eigenvalues are strictly larger
than zero, i.e., well-trained layer weight matrices, with Q > 1, should have full rank and exhibit
no “rank collapse.” HTSR places random Gaussian and “Bulk-Plus-Spike” matrices into the first
two rows of Table 1. The essential feature of Gaussian random matrices is that their entries
have no correlations. When some correlations are injected, a few large spike eigenvalues form,
without otherwise disturbing the shape of the ESD. To really understand how individual NN
layers converge, we need to understand when and why their ESDs become HT.
14
2.2.2 Heavy-Tailed Random Matrix Theory (HTRMT) and Power Law (PL) fits
For very well-trained NN layers, ESDs are not MP at all. Frequently, if not always, their ESDs
are HT—and they are HT because they are strongly-correlated matrices. Importantly, they are
not HT element-wise. Instead, their entries have a scale, and they have ESDs that are HT due
to correlations learned during training. Existing theoretical approaches, including SLT and even
StatMech, cannot readily model such strongly-correlated systems.8
Such strongly-correlated systems, however, do frequently arise in other, related scientific domains, including in the StatMech of self-organizing systems [76, 77], in electronic structure theory [53, 55, 54], and in quantitative finance [78, 79, 73]. In these (and other) domains, correlated
systems frequently exhibit characteristic PL signatures; and it is common practice to model correlated systems as random (uncorrelated) systems by using HT statistics (e.g., Levy distributions
or PL random matrices), fully understanding that such systems are by no means actually i.i.d.
random. The HTSR phenomenology builds upon this longstanding practice by delimiting families
of HT NN weight matrices based on the corresponding Universality classes of Pareto matrices.
We explain briefly how to interpret Table 1 with respect to HTRMT. The 5+1 Phases of Training
can be identified by fitting ESDs to MP or PL distributions, whichever gives the best fit, as shown
in the last column. In case the PL distribution is a better fit, HTSR phenomenology treats the
layer weight matrix as equivalent to an i.i.d. random matrix W(µ), whose elements have been
drawn from a Pareto distribution with exponent µ.
Heavy-Tailed Universality Classes of Random Pareto Matrices For such an elementwise HT matrix, the theoretical limiting ESD of a Pareto matrix is also PL, which allows us to
related the fitted PL α with exponent α = aµ+b, to the Pareto exponent µ. Ideally, for an infinite
width matrix , a =
1
2
and b = 1, but due to finite-size effects, however, we have found we must
take a ≥
1
2
and b ≥ 1, giving
Wi,j(µ) ∼
C
x
µ+1
, ρ(λ) ∼ λ
−(aµ+b)
. (6)
According to the above relation, we can use either the fitted PL exponent α, or the Pareto
exponent µ, to index the HT Universality classes, Note, however, that the finite-size effects
strongly depend on the and aspect ratio Q = N/M, at least when applied to i.i.d random Pareto
matrices, and the (Clauset MLE) PL fit may overestimate the α of the ESD. Table 1 delimits the
HT matrices into sub-categories (as shown in the bottom four rows) based on the behaviors of α
as a function of µ.
Figure 3 illustrates how the fitted PL exponent α corresponds to the actual Pareto exponent
µ for different aspect ratios Q = M/M. Figure 3(a) displays the ESDs of three different i.i.d.
1000 × 1000 HT random matrices, with µ = 1, 3, 5, on a Log-Log scale. Notice that smaller µ,
and therefore smaller α, corresponds to heavier (i.e., larger) tails. Figure 3(b) shows how the
empirically fit PL exponent α can vary with the theoretical µ for an associated W(µ). For µ < 2
and Q = 1, the fitted α follows the linear relation α =
1
2
µ + 1, albeit with some error. In contrast,
for the more relevant µ ∈ (2, 4) regime, the relation now depends far more strongly on the aspect
ratio Q, and α ∈ [2, 6]. For µ > 4, the fitted α saturates for each specific value of Q.
We emphasize that we only model the ESDs of the NN layer weight matrices using the same
Universality class to that associated with the ESD of a random, i.i.d, HT Pareto matrix. In fact,
the elements Wi,j do not at all appear as if they have been drawn from an HT Pareto distribution,
8For example, such theoretical approaches typically deal better with Scale information (such as λmax) than with
Shape information (such as α), e.g., by characterizing an “eigen-gap” separating large eigenvalues from “noise” [74]
according to a noise plus low-rank perturbation model [75].
15
and, in contrast, are almost always well fit to a Laplacian distribution. Also, despite these strong
finite-size effects, empirically one finds that the ESDs arising large, well trained, modern NNs can
frequently be well fit to a PL (or TPL), and that the fitted α ∈ [2, 6] for 80 − 90% of NN layers.
Notably, we rarely find α < 2 in the best performing, open source, pretrained DNNs.
As there is no ground truth whatsoever as to the limiting spectral density of a stronglycorrelated NN weight matrix (especially without HT elements) the HTSR phenomenology uses
Pareto matrices as a guide. However, as we will see in Section 3.3, this analogy should be treated
with caution because there are cases where it breaks down.
No matter why a matrix ESD is HT, it can be difficult to reliably estimate the α parameter
when the true α is large. For Pareto matrices of the size investigated here, an observed α above
6 is uninformative — the tail will decay very rapidly indeed, leaving very little of it to study. In
this sense, the HT Universality classes are larger than the set of only strongly-correlated matrices
or Pareto random matrices.
(a) Heavy Tailed ESDs (b) PL α vs HT µ exponent
Figure 3: Comparison of ESDs and Power Law (PL) exponents α from Heavy-Tailed (Pareto)
weight matrices W(µ). Subfigure (a) depicts 3 typical ESDs with Pareto exponent µ = 1, 3, 5,
each decreasing in Shape and Scale. Subfigure (b) shows how the exponent α of the PL fit varies
with µ, with significant finite-size effects emerging for µ > 2 and α > 2.
There is a particularly important boundary between Universality classes where α = 2. Recall
that one of the properties of power law distributions ρ(λ) ∼ λ
−α
is that if α < 2, than the variance
of ρ(λ) is infinite. In such cases, the variance cannot be estimated empirically, making ρ(λ)
in some sense atypical. This implies that the NN will have substantially greater difficulty in
applying any further load to such a weight matrix. Thus, the value of α = 2 is a critical value.
(See Figure 27 in Section 6.6 for an empirical study of this effect in a small MLP.)
Smaller PL exponent α values correspond to heavier tails, ρtail(λ); and the HTSR phenomenology observes that smaller PL exponents α (at least for α ∈ (2, 6)) tend to correspond to better
models. This is the key idea of the HTSR: the generalizing components of a layer matrix W concentrate in larger singular vectors associated with the tail, and so that better models have more
slowly-decaying (i.e., larger) ESD tails. This differs significantly than simply taking a general
low-rank approximation to W, where the rank is chosen without insight from the HTSR phenomenology. The SETOL theory formalizes this observation as a key assumption. We will revisit
these model selection questions in Section 3.1 below.
16
2.3 Data-Free Shape and Scale Quality Metrics
The HTSR phenomenology provides quality metrics for both individual layers and (by averaging
layers) for an entire NN model.
Layer-wise Quality Metrics. Using the HTSR phenomenology, we can define several other
Shape and/or Scale based layer (quality) metrics. These are available in the WeightWatcher tool,
and they work very well in practice.
• Alpha (α): ρtail(λ) ∼ λ
−α
. A Shape-based quality metric.
• LogSpectralNorm: log10 λmax. A Scale-based quality metric.
• AlphaHat (αˆ): αlog10 λmax. A Scale-adjusted Shape-based quality metric.
• Rand-Distance: JSD[ρ
emp∣(ρ
emp
rand)]. A Shape-based, non-parametric quality metric, suitable for highly-accurate, epoch-by-epoch analysis.9
• PL KS: DKS. The KS-distance, or quality-of-fit, of the PL fits. For transformers, foundation
models, and large, complex, modern NNs, this is frequently an even better model quality
metric than the α of the PL fit itself.
• MP SoftRank: RMP . The MP-SoftRank, defined in [25], can be used to identify problems
such as when there is significant label or data noise that causes spuriously small α, and also
when it is difficult to fit a PL law.10
Each of these quality metrics provide a simple characterization of the Shape and/or Scale of the
tail of the ESD of a given layer W. These metrics are related to each other, and they have various
trade-offs in practice [26, 24, 28]. Of particular interest here in our development of SETOL are the
PL-based WeightWatcher Alpha and AlphaHat metrics.
From Layer-wise Quality Metrics to Layer-Averaged Model Quality Metrics. One
can use the HTSR phenomenology to go beyond individual Layer Quality metrics, to construct
model quality metrics by averaging Layer Quality metrics (over all layers that are not very
small). Existing HTSR model quality metrics implicitly require that all layers are statistically
independent, so that the average model quality is just the average of the contributions from each
weight matrix W.
11 Given a Layer Quality metric, Q¯NN
L
(W), one can define the Model Quality
Q¯NN metric for an entire model as
Q¯NN ∶= ∏
L
Q¯NN
L (W), (7)
a product of each independent Layer Quality Q¯NN
L
, and then consider the layer average as the
log Layer Quality,
log Q¯NN =
1
NL
∑
L
log Q¯NN
L = ⟨log Q¯NN
L ⟩L¯ (8)
9
JSD is the Jensen-Shannon Divergence between the original ESD and the ESD of the layer weight matrix,
randomized element-wise.
10The WeightWatcher tool also implements the WW-SoftRank, which is like the MP-SoftRank, but replaces
λ
+
bulk with λ
max
rand; these are mostly equivalent for large matrices, but they can be different for very small matrices.
11This independence assumption, clearly a mathematical convenience, gets us closer to a workable theory. One
could go beyond a “single layer theory” by adding in intra-layer correlations empirically. The WeightWatcher tool
does support this, but doing so is outside the scope of this work.
17
where ⟨ ⋯ ⟩L¯ denotes the layer average.
In particular, prior work has used the following metrics:
• The layer-averaged model quality metric Alpha, log Q¯NN = ⟨α⟩L¯, describes the Shape of the
ESDs. One can use the averaged Alpha when studying a single model, and only varying
the regularization hyperparameters, although Alpha also works very well as a model quality
metric when comparing different transformer models [80].
• The layer-averaged model quality metric LogSpectralNorm, log Q¯NN = ⟨log λmax⟩L¯, describes the Scale of the ESDs. The averaged LogSpectralNorm does work as a model
quality metric, but not as well as Alpha (or AlphaHat). Notably, SLT predicts that smaller,
not larger, LogSpectralNorm should be correlated with model quality; the opposite is observed in practice! This is because a smaller layer α generally, but not always, corresponds
to a larger λmax.
12
• The layer-averaged model quality metric AlphaHat, log Q¯NN = ⟨αlog λmax⟩L¯ = ⟨αˆ⟩L¯, incorporates both Shape and Scale information. This can compensate for anomalies that can
arise when (say) comparing models of different sizes or model qualities [24] or when other
issues cause unusually large λmax. See Section 3.3.1).
The layer-averaged AlphaHat model quality metric has been applied in a large meta-analysis of
hundreds of SOTA pre-trained publicly-available NN models in CV and NLP [26, 27, 28, 81].
Generally speaking, HTSR shape-based metrics, when used appropriately, outperform all other
metrics studied (including those from SLT, and with access to the training/testing data,) for
predicting the quality of SOTA pre-trained publicly available NN models. The HTSR theory
predicts that the best-performing NN models have layers with Alpha ∈ [2, 6], and with α = 2
indicating optimal performance. Moreover, prior empirical results show that the Alpha and
AlphaHat metrics can predict trends in the Quality (i.e., the Generalization Accuracy), of SOTA
NN models—even without access to any training or testing data [26].
12The LogSpectralNorm can exhibit a Simpson’s paradox when segmenting models by quality) [24]. Nevertheless,
this metric may be useful when a PL fit can not be obtained, say, when N ≫ M and M is very small, as with
LSTMs, U-Net architectures, etc.
18
3 A Semi-Empirical Theory of (Deep) Learning (SETOL)
Based on prior empirical results, and the success of the Alpha and AlphaHat metrics that are
based on the HTSR phenomenology, this leads to the deeper question:
Why do the Alpha and AlphaHat metrics work so well as NN model quality metrics
for SOTA NN models?
That is, why do NN models with heavier-tailed layer Empirical Spectral Distributions (ESDs)
tend to generalize better when compared to related models, and how can single-layer metrics
predict model quality so well ? Relatedly, can we derive these metrics from first principles? (If
so, then under what conditions do they hold, and under what conditions do they fail?)
To answer these questions, we will derive a general expression for the Layer Quality, Q¯, of an NN.
Although many modern NNs have many layers, we adopt a single-layer viewpoint (like a matrixgeneralized Student–Teacher) because in Statistical Mechanics of Generalization (SMOG) theory [8,
82] the multi-layer generalization can be factorized or approximated. For this, we will obtain by
simple averaging our model quality metrics, under effectively a single layer approximation, that
correspond to Alpha and AlphaHat.
In deriving these quantities, we will introduce to NN theory a new Semi-Empirical approach
that combines techniques from StatMech and RMT in a novel way.must depend on the spectral
density The Layer Quality Q¯ will estimate the contribution that an individual NN layer makes to
the overall quality of a trained NN model. In deriving Q¯, we have discovered a new Layer Quality
metric, called the ERG condition, which indicates that the generalizing components of the layer
must concentrate into a low-rank subspace which we term the Effective Correlation Space, or ECS.
That is, the better this condition is met, the more the generalizing components concentrate in
this ECS, and this tendency provides a new layer quality metric, derived in a totally independent
way from Alpha and AlphaHat. We will provide strong empirical justification that both metrics
are acceptable Quality metrics by showing that they converge as model quality increases.
Importantly, we have conducted detailed experiments to show that the empirical estimates of
the SETOL ERG condition align remarkably well with predictions from the HTSR theory under Ideal
conditions (see Sections 6.1) and, then, that the key assumptions of our SETOL theory are valid
(see Sections 6.2 and 6.3). In Section 6.4, we demonstrate how to apply theory directly using
explicit calculations of the RMT layer cumulants. We next examine how the HTSR predictions (i.e.,
the HT PL exponent α) behave under non-Ideal conditions (see Sections 6.5 and 6.6). In the
following, we will outline key conceptual aspects of SETOL. In Section 3.1, we give an overview
of SETOL; In Section 3.2, we describe the conditions of Ideal learning under SETOL and how they
differ from those of HTSR; and In Section 3.3 we describe conditions that deviate from this.
3.1 SETOL Overview
Our SETOL formulates a parametric expression for the Layer Quality Q¯ using a matrix-generalization
of the classic Student-Teacher (ST) model from the SMOG theory of the 1990s [8, 82], but with a
Semi-Empirical twist in that the Teacher is now an actual, trained NN that is input to theory.
Recent advances in the evaluation of so-called HCIZ random matrix integrals [73, 83, 84], such
that the final expression for Q¯ to be written in terms of empirically measured statistical properties
of the layer ESD. We summarize our basic approach here; see Section 5 for a detailed derivation,
and see Section 6 for a detailed empirical analysis. For reference here and later, Figure 4 displays
a flowchart of the conceptual development of SETOL.
As is done in the Student-Teacher (ST) model [8], we first formulate the Generalization Error
(E¯ST
gen) of the linear Perceptron (in the Annealed Approximation, and in the High-Temperature
19
Classic SMOG
Student-Teacher Perceptron
but with
Fixed Teacher (input)
ST Accuracy (Quality) Q¯ST
Thermal Average (AA, high-T)
Overlap R = s
⊺t
Q¯ST = ⟨R⟩
β
s
Matrix Generalized Quality Q¯2
HCIZ Integral
R ∶=
1
N S
⊺T
Q¯2
∶= ⟨ Tr[R⊺R]⟩β
S
Wilson Exact RG Condition (ERG)
Effective Correlation Space (ECS)
AN =
1
N SST
, A˜ = Pˆ ECSAN
dµ(S) → dµ(A) → dµ(A˜ )
∑ ln λ˜ = 0 , λ˜ ≥ λ˜min
Tanaka Result for HCIZ
Large-N in N limit
βΓ
IZ
Q¯2 =
1
N
ln ∫ dµ(A˜ )exp[nβN Tr [
1
N T⊺ANT]]
Q¯2
∶=
∂
∂n
1
β βΓ
IZ
Q¯2,N≫1
Integrated R-transforms
Q¯2 = ∑λ˜ ∫
λ˜
λ˜min RA˜ (z)
Student ESD A ∼ Teacher ESD X
RA˜ (z) = RX˜ (z)
R-transform choices
Bulk+Spikes (BS)
Free Cauchy (FC)
Inverse MP (IMP)
Lévy Wigner (LW)
⋯
Derive HTSR metrics
Extend SETOL to non-Ideal cases
α and αˆ
Experimental
verification
Figure 4: Flowchart of the theoretical concepts used to construct SETOL.
limit; see Section 4), and we then generalize this to the case of a NN (E¯ST
gen → E¯NN
gen ), so that
we can analyze the Quality of each layer. For the Perceptron, the Generalization Error is an
Energy, given as E¯ST
gen ∶= ⟨1 − R⟩
β
s
, where R is the ST vector overlap, and ⟨⋯⟩
β
s
is a Thermal
Average (defined in Section 4.2), a Boltzmann-weighted average. In this case, the model Quality,
Q¯ST is exactly the AA, high-T Average Generalization Accuracy Q¯ST ∶= 1 − E¯ST
gen = ⟨R⟩
β
s
. For an
MLP or general NN, each layer’s Energy is (negatively) associated with a Layer Quality Q¯, which
we identify as the average contribution an individual layer contributes to the overall generalized
accuracy, (i.e 1 − E¯NN
gen ) for a multilayer perceptron (MLP).
Importantly, we deviate from the traditional approach in that we take the Teacher
as a fixed, empirical input to the theory. The Teacher contribution is then formulated in
terms of the actual the layer eigenvalues, or Empirical Spectral Density (ESD). In this way, the
theory becomes Semi-Empirical.
For technical reasons (below), we will seek the Layer Quality (Squared) Q¯2
, which is defined
as the Thermal Average of the matrix-generalized overlap ( Tr [R2
]),
Q¯2
∶= ⟨ Tr [R
2
]⟩β
S
(9)
where R2
can be thought of as a Hamiltonian for the Quality-Squared (HQ¯2 = R⊺R).
In Eqn. 9, the so-called Teacher (T) is the NN model under consideration, and R ∶=
1
N
S
⊺T
denotes the ST overlap operator between the Teacher layer weight matrix T and a similar Student
(S) layer weight matrix S. The notation ⟨⋯⟩
β
S
denotes a Thermal Average over all Student weight
matrices S that resemble the Teacher weight matrix T. By “resemble”, the SETOL approach
assumes that the ESD of S has the same limiting form as T, placing them in the same HTSR
Universality class. This is made more precise below.
20
The Overlap R, and the Inner and Outer forms of A, AM and AN, resp. Let us now
express the average matrix-matrix overlap R in squared form using:
Tr [R
2
] ∶= Tr [R
⊺R] (10)
=
1
N2
Tr [T
⊺SS⊺T] =
1
N
Tr [T
⊺ANT]
where AN is called the Outer Student correlation matrix, and is the N × N form of the Student
correlation matrix, AN ∶=
1
N
SS⊺
. We will also define the M × M Inner Student correlation
matrix AM =
1
N
S
⊺S, which is used later. Note that AM also has a 1
N
factor, so that its non-zero
eigenvalues will be the same as AN, but it has dimensions M × M. We will denote the Outer AN
and Inner AM forms when the context demands it, and will simply use the general form A when
they can be interchanged.
The Quality-Squared Generating Function βΓ
IZ
Q¯2
. As explained in Section 4.2, this QualitySquared is more readily obtained as the derivative of the Layer Quality-Squared Generating
Function, βΓ
IZ
Q¯2
, defined as
Q¯2
∶=
1
β
∂
∂n lim
N≫1
βΓ
IZ
Q¯2 (11)
where n is the number of training examples used to train the model.
βΓ
IZ
Q¯2
is essentially (β times) a negative Free Energy for the (approximate) Layer QualitySquared (see Section 4, and the Appendix, Section A.3). For more details, see Section 5, and the
Appendix, Section A.2).
We can write βΓ
IZ
Q¯2 as an HCIZ Integral,
βΓ
IZ
Q¯2 =
1
N
ln∫ dµ(S) exp (nβN Tr [
1
N T
⊺ANT]) , (12)
where n is the number of training examples used to train the models.
The SETOL approach then seeks to express βΓ
IZ
Q¯2
in Eqn. 12 as an HCIZ integral (and in the
Wide Layer Large-N limit in N) [73, 83, 84]. We evaluate this at large-N in N, and write
βΓ
IZ
Q¯2,N≫1
∶= lim
N≫1
βΓ
IZ
Q¯2 (13)
The result is effectively expressed in the limit of fixed layer load n/N, analogous to a renormalized mean-field (i.e., Semi-Empirical) theory over N interacting (feature) vectors of length M.
In other words, in taking the Large-N in N limit, we assume implicitly that βΓ
IZ
Q¯2
is in the
Thermodynamic limit (and at high-T).
With these definitions in place, moving forward, the following key assumptions, which can be
tested empirically, must hold:
• The Effective Correlation Space (ECS) Condition. The generalizing components of
the Student (and Teacher) layer weight matrices concentrate into a lower rank subspace—
the ECS—spanned by the eigenvectors associated with the (heavy) tail of the layer ESD
ρtail(λ), such that the test error can be reproduced with only these components. We write
A˜ to denote the projection of the correlation matrix A˜ ∶= PecsA, onto this subspace, now
with rank M˜ ≪ M. This restricts the measure dµ(A) to the ECS, (dµ(A) → dµ(A˜ )). This
assumption will be empirically examined using real-world Teacher weight matrices T = W
in Section 6.2.
21
• The ERG Condition. The Effective Student correlation matrix A˜ satisfies the ERG condition
that Tr [ln A˜ ] = ln det (A˜ ) = 0, so that the change of measure dµ(S) → dµ(A˜ ) is Volume
Preserving. This condition is derived explicitly in terms of Inner form, A˜ M, and will also
hold for the Outer form, A˜
N Practically, this implies that the M˜ eigenvalues λ˜ of the tail
of the ESD must satisfy ∑
M˜
i=1
ln λ˜
i ≈ 0. Experiments will test this assumption explicitly in
Section 6.3.
Remarkably, both conditions hold best empirically when the HTSR PL quality metric α ≳ 2 is
Ideal. Motivated from these empirical observations, we have:
• βΓ
IZ
Q¯2,N≫1
is expressed as an HCIZ integral, at large-N. We have
βΓ
IZ
Q¯2,N≫1
= lim
N≫1
1
N
ln∫ dµ(A˜ ) exp (nβ Tr [T
⊺A˜
NT]) (14)
where measure dµ(A˜ ) lets us average over all (Outer) Student Correlation matrices A˜
N
explicitly, which lie in the ECS space and which “resemble” the Teacher. By “resemble” we
mean that they share the same functional form in their limiting ESDs, ρ
∞
A˜
(λ) ∼ ρ
∞
X˜
(λ).
• The Layer Quality (Squared) Q¯2
is a Norm Generating Function. The final expression for Q¯2
can be written as the derivative of βΓ
IZ
Q¯2,N≫1
as
Q¯2
=
1
β
∂
∂nβΓ
IZ
Q¯2,N≫1
=
M˜
∑
i=1
G(λi) (15)
where G(λi) is, albeit imprecisely, a Norm Generating Function, and is defined as the
integrated R-transform R(z) of the Teacher layer ESD (where z ∈ C), such that G(λ) ∶=
∫
λ
λ˜min
R(z)dz and λ˜min encapsulates the ECS (and, for some choices of R(z), selects the
desired branch-cut of R(z) so that it is both analytic and well-behaved) .
To apply the theory, one must choose an R-transform R(z) for the Teacher that models the
tail of the ESD ρ
emp
T
(λ), and that can be parameterized by some measurable property. This may
include the number of Spikes λ
spike, the fitted PL exponent α, the maximum eigenvalue λmax,
or even the entire tail ρ
tail
T
(λ). This may be a formal expression, a computational procedure, or
some combination.
To integrate R(z), however, and to have a physically meaningful result, one must ensure that
R(z) is both analytic and single-valued on the domain of interest, namely, the ECS (and therefore
the (PL) tail of the ESD), R[z] ≥ λ˜min.
13 Because the ESD is frequently Heavy-Tailed (HT), this
R-transform R(z) may have a branch-cut, and it is expected that this will occur at R[z] ≈ λ˜min,
corresponding to a point roughly at or before the start of the ECS (i.e., at the peak of the ESD on
a log scale). Selecting the branch-cut R(z) provides an additional physical meaning to the ECS.
To complete the theory, we will also show that the HTSR PL Layer Quality metrics Alpha
(α) and AlphaHat (αˆ) can be formally derived directly from the SETOL Layer Quality Q¯ by
selecting the appropriate R-transform R(z) and making empirically motivated approximations.
In Section 5.4 we provide several possible models of R(z) and the resulting Layer Quality Q¯.
Renormalization Group Effective Hamiltonian The formulation of SETOL closely parallels the construction of an Effective Hamiltonian HECS
Q¯2 via the Wilson Exact Renormalization
13R[z] is the Real part of z.
22
Group (ERG) approach. Consider a bare Hamiltonian HQ¯2 for the Layer Quality-Squared, defined as HQ¯2 ∶= R⊺R. We can express Eqn. 12 in terms of this bare Hamiltonian HQ¯2 , and
rewrite Eqn. 14 in terms of a Renormalized Effective Hamiltonian HECS
Q¯2
that spans the Effective
Correlation Space (ECS). Formally, we have:
1
N
ln∫ dµ(S)e
nβN Tr[HQ¯2 ] ERG
ÐÐÐ→ lim
N≫1
1
N
ln∫ dµ(A˜ )e
nβN Tr[HECS
Q¯2
]
(16)
where the ERG transformation is defined by the Scale-Invariant change of measure, applied in
the Wide Layer Large-N limit in N, and where HECS
Q¯2
is defined implicitly through the result
for Q¯2
(Eqn. 15). The result is, formally, a sum of the integrated R-transforms G(λi). In a
sense, this result resembles (a non-perturbative form of) the Linked Cluster Theorem in that
the log Partition Function is expressed as a sum of integrated matrix-generalized cumulants.
And in analogy with Semi-Empirical theories of Quantum Chemistry, the HTSR Alpha (α) and
AlphaHat αˆ) enter as renormalized empirical parameters. Most importantly, the Scale-Invariant
ERG condition can be verified empirically (See Section 6.) Importantly, in analogy with the Wilson
Exact Renormalization Group theory, the HTSR α = 2 resembles in spirit an ERG Universal Critical
Exponent at a phase boundary being between the Heavy-Tailed (HT) and the Very Heavy-Tailed
(VHT) phase of learning of the HTSR theory. This observation strengthens our argument that the
HTSR HT and VHT phases are analogous to the generalizing and overfitting phases, respectively,
of the classical SMOG theories of NN learning.
3.2 Comparing SETOL with HTSR: Conditions for Ideal Learning
The SETOL approach establishes a starting point for developing a first-principles theory for modern
NNs. Among other things, by connecting with the HTSR phenomenology, it lets us identify
conditions for an Ideal state of learning for an individual NN layer, under the Single Layer
Approximation. By Ideal, we mean that the layer is being used most effectively i.e., in some
sense it is at its optimal data load, and thus it is conjectured to result in the best model quality.
The Ideal State of Learning is conjectured to be characterized by the following three
conditions:
1. the tail of ESD, ρ
emp
tail (λ), can be well fit to a PL of α ≈ 2: ρ
emp
tail (λ) ∼ λ
−2
;
2. the eigenvalues in the tail, λi
, satisfy the ERG (i.e., Trace-Log) condition: ∑i
ln λi = 0; and
3. the generalizing components of the layer concentrate in the singular vectors associated with
the tail of the ESD, (whose span we call the Effective Correlation Space).
In Section 6, we will test and justify this conjecture by showing that the induced measures of each
condition converge to one another at the same rate, and reach convergence at the exact point
beyond which accuracy begins to degrade.
These claims are fundamentally about NN learning itself. They are motivated by our formulation of the SETOL approach in our search for a practical predictive theory behind the HTSR
phenomenology. When (1) and (2) conditions hold for any layer, we conjecture that (3) holds as
well. Moreover, when (1−3) hold for all layers, we conjecture the NN has the lowest Generalization
Error (and highest Model Quality) possible for given model architecture and dataset.
Previous results have shown that the HTSR quality metrics (Alpha, AlphaHat, etc.) correlate very well with reported test accuracies, as well as model quality on an epoch-by-epoch
basis [25, 85]. These results hold because, as indicated by the HTSR theory, the PL exponent α
characterizes both the quality of the layer and provides an after-the-fact measure of the amount of
23
regularization.14 However, the HTSR approach says nothing about the SETOL ERG condition; and
neither does the SETOL approach require a minimum of α = 2 to obtain the best model quality,
as observed by the HTSR phenomenology. Remarkably, we can show that (1) and (2) do hold
simultaneously, both in carefully designed experiments on a small model, as well as for many
pre-trained, high quality open-source models (such as VGG, ResNet, Llama, Falcon, etc).
HTSR, however, has been developed as a phenomenology describing the best-trained, most
accurate open-source models available. As such, it may be biased towards such models, and it
may not describe less optimal learning scenarios. The key goal of this work is to derive independent
conditions, both theoretical and experimental, that can identify the conditions for Ideal Learning,
and to stress-test these conditions in carefully designed, reproducible experiments.
3.3 Detecting Non-Ideal Learning Conditions
The HTSR phenomenology posits that SGD training reduces the Training Error by accumulating
correlations into the large eigenvalues in NN layer weight matrices W such that they self-organize
into a HT with a PL signature, and that this successful self-organization leads to good model
quality. Conversely, it also posits that when training has gone awry in some way, the resulting
ESD, ρ
emp(λ), will be deformed in some way. In many practical situations, there can be other,
competing factors that give rise to large eigenvalues that do not contribute to the generalization
capacity of the model, and, consequently, can affect the Scale (i.e., the largest eigenvalue(s) λmax)
and Shape (i.e., the PL exponent α, or goodness of fit DKS) of the layer ESDs. These large λ
could be Dragon Kings [86], Correlation Traps, or some other anomaly.
To apply the HTSR phenomenology most effectively, one must be able to identify various
spurious factors and distinguish real correlations from any other large eigenvalues, including the
effects of both extreme eigenvalues λ, individual matrix elements Wi,j , and rank-1 perturbations
in W. In one case the ESD is HT primarily due to correlations that help the model generalize,
whereas in another when the ESD may be more HT than expected due to suboptimal training,
mis-labeled data, etc. In extreme cases, spurious eigenvalues can push the weight matrix into the
Very Heavy-Tailed Universality class (i.e., α < 2. See Table 1, Section 2), or disrupt the formation
of a HT, resulting in a poor PL fit, undermining the core proposition of the HTSR approach.
When training a model with SGD, one may only achieve a sub-optimal result when using overly
large learning rates / small batch sizes, (see Section 6), from poor hyper-parameter settings, or
simply because direct regularization fails. In such cases, the HTSR approach allows one to detect
potential problems by looking for large eigenvalues not resulting from correlations [87].
Importantly, in the context of the SETOL theory, we can now identify such empirical anomalies due to atypical layer weight matrices W, a key factor when models break down. The SETOL
approach formalizes the empirical HTSR phenomenology, but, in doing so, assumes that the underlying layer effective correlation matrix X˜ , is typical, meaning that it can describe out-of-sample
/ test data. Conversely, if the underlying weight matrix W is atypical, then it is in some sense
overfit to the training data and can not fully represent out-of-sample / test data. Consequently,
when W is atypical, we argue that we can observe this, either in the ESD ρ
emp(λ) directly (i.e.,
when α < 2), and/or having 1 or more unusually large matrix elements Wij .
We conjecture such sub-optimal results, and particularly those occurring from overfitting,
actually arise when the underlying layer weight matrix W is atypical in some way, in analogy to
14By “after-the-fact”, we mean that it provides a measure of the regularization in a layer, along the lines of the
self-regularization interpretation of HTSR Theory [25]. However, we do not recommend that it be used as an explicit
regularization parameter. Informally, this is since the “easiest way to obtain HT ESDs is to make weight matrices
HT element-wise; but this is not what is observed in practice, and thus this is precisely not what HTSR Theory and
our new SETOL approach are designed to model.
24
the results from the classic SMOG theory (see Section 4.1), and, importantly, that we can use the
SETOL approach to detect when W is atypical and therefore a layer is overfit in some way.
Here, we identify two specific cases of atypical weight matrices—Correlation Traps and OverRegularization.
15
3.3.1 Correlation Traps. W is atypical in that W exhibits an anomalously large mean (W¯ ).
We can observe these by randomizing the layer weight matrix, W → Wrand, and then
looking for eigenvalues that extend significantly beyond the MP edge of the random bulk
(i.e., Spikes). We call such random spikes Correlation Traps, denoted as λtrap, because
they appear, in some extreme cases, to be associated with distorted ESDs and, importantly,
lower test accuracies. Examples of Correlation Traps are shown in Section 6.5.
3.3.2 Over-Regularization. W is atypical in that W exhibits an anomalously large variance
(σ
2
(W)). We can observe this when the layer α < 2. Also, since Alpha a measure of
implicit regularization, we say the layer with α < 2 is Over-Regularized. In particular, when
one layer is undertrained, having α > 6, it appears that other layers may become overtrained
to compensate, and this can be seen with having α < 2. These effects are studied in Section 6.
Additionally, we also observe that when evaluating the SETOL ERG condition, when α < 2,
then ∆λmin < 0 (see Section 6.3).
3.3.1 Correlation Traps
The first way we identify W as atypical is when it has an anomalously large mean (W¯ ); detecting
this in general, however, requires more than just examining which elements Wi,j are anomalously
large but, insteading, looking for what can be multi-element rank-1 perturbations in W. Here,
we can apply elementary RMT, as in the HTSR approach.
The HTSR phenomenology states that NNs generalize better when their layers ESDs are more
HT—precisely because the tail eigenvalues arise from correlations in the weight matrices. So one
way is to identify atypicality is to look for large eigenvalues that do not arise from correlations in X,
but, rather, from one or a few spuriously large matrix elements Wi,j and/or rank-1 perturbations
in W. We call these eigenvalues Correlation Traps, denoted by λtrap (i.e., see Section 6.5).
Indeed, if we randomize W element-wise, i.e W → Wrand, we expect the Wrand
i,j matrix
elements to be i.i.d and with a small mean (unless something odd happens during SGD training).
Likewise, we expect the singular values of Wrand to follow the MP distribution, to within finitesize / TW fluctuations. If we observe an eigenvalue λtrap extending beyond the MP bulk region,
λtrap > λ
+
bulk, then the mean Wi,j matrix element will also be anomalously large, and we can
identify W as atypical. We must be careful, however, as we do not fully understand the origin of
these atypicalities and do not claim that every one is associated with suboptimal generalization.
By a Correlation Trap, we mean that some anomaly in the training of W resulted in one or
more spuriously large eigenvalues λtrap in Wrand, and that whatever caused them also may, in
some pronounced cases, tend to “trap the correlations in X itself, preventing them from coalescing
into a well defined PL Heavy Tail, or otherwise distorting the ESD. Whether they are a signature
of training gone wrong, or whether they distort the dynamics of the tail correlations simply by
being there, Correlation Traps can be expected to alter the shape of the ESD, reducing the quality
of the PL fits, and sometimes producing spurious α values.
Why would such anomalies occur in a NN? It is conceivable that SGD will, when it fails
to find usable correlations, instead produce spurious correlations in the form of large elements
15Later, in Section 6, we will show that we can systematically induce both phenomena and observe their effects
on the HTSR HT PL metric α and the SETOL ERG condition.
25
and/or rank-1 perturbations. Also, the matrix itself may simply undergo an innocuous meanshift because the mean is not explicitly controlled during training. Here, mean-recentering may
be beneficial. 16
We will see, below in Section 6, that we can induce a Correlation Trap both by shrinking the
batch size, or, equivalently, increasing the learning rate, and that this is associated with degraded
model performance and small α. We seek to identify specific ways of identifying such traps
because we reason that the presence of foreign large eigenvalues may disrupt the self-organization
of correlations – or that failed learning may produce them as a by-product.
Detecting Correlation Traps with RMT. RMT suggests that when a matrix W has unusually
large elements Wij , then the ESD will have one or more large eigenvalues λtrap lying outside the
bulk edge λ
+
bulk of the ESD, as predicted by MP theory. One can detect these so-called Correlation
Traps in a weight matrix W by performing the following:
1. randomize W element-wise to obtain Wrand;
2. compute the ESD for Wrand; and
3. look for large eigenvalues λtrap ≫ λ
+
bulk.
WeightWatcher looks for Correlation Traps (λtrap) in the ESD of the randomized Wrand, that
are larger than λtrap > λ
+
bulk + ∆TW , where λ
+
bulk is the MP bulk edge ∆TW are the associated
finite-size Tracy Widom (TW) fluctuations. This procedure detects any anomaly in the matrix
weights that produce spuriously large eigenvalues. It is implemented in WeightWatcher (using
the randomize option), which was used to generate the plots in Figure 5.
(a) Well-formed ESD (b) ESD with Correlation Trap
Figure 5: Comparison of a well-formed, Heavy-Tailed ESD (a) to one with a Correlation Trap
(b), in the VGG16 model (FC2 layer)
See Figure 5(a), which displays the (log)-ESD of a typical SOTA NN layer X (green), i.e., on
a log-linear scale, along with the (log)-ESD of that layer after randomizing it element-wise (red).
16Similarly, when training NNs, frequently the weight matrices [88] or activations [89] may need to be clipped
during training to ensure good results.
26
The two ESDs differ substantially: the ESD of the original weight matrix W (green) is very HT,
whereas the ESD of the randomized weight matrix Wrand (red) is an MP (and as predicted by
the MP RMT). The orange line corresponds to the maximum eigenvalue of the randomized ESD.
Note that it is at the MP bulk edge of the red plot, indicating that this ESD is not affected by
unusually large elements or other weight anomalies. Here, we say that the ESD of X is HT, and
that W is not HT element-wise. HTSR says this layer is well trained.
Contrast this with Figure 5(b), which displays the (log)-ESD of a NN layer with a Correlation
Trap. The ESD of X (green) is weakly HT, but it looks nothing like the ESD in Figure 5(a).
In fact, it looks very much like the ESD of the randomized weight matrix Wrand (red), except
for a small shelf on the right. The orange line again corresponds to the maximum eigenvalue of
the randomized ESD, and this is just past this shelf. Relative to the randomized ESD, this line
depicts (an) unusually large element(s)—or, equivalently, a rank-1 perturbation of Wrand. By a
Correlation Trap, we mean that some anomaly in the elements of W tends to “trap” the ESD
of X, concentrating the correlations in X into the small shelf of density around the orange line.
HTSR says this layer is not well-trained because it does not have a good PL fit.
3.3.2 Over-Regularization
The second way we identify W as atypical is when it has an anomalously large variance (σ
2
(W)).
The SETOL theory – a single-layer theory of learning – casts the training of a NN layer in
terms of how the correlations concentrate into the layer Effective Correlation Space (ECS), and
becomes exact when the ERG condition is satisfied. Analogously, the HTSR theory – a single layer
phenomenology of learning – casts training of an N layer by fitting its ESD to a PL, and noting
that the PL exponent α measures the amount of implicit regularization in the layer. Comparing
the two approaches, we see that smaller α corresponds to the correlations concentrating into
a low-rank ECS. In general, and likewise, the more the weight matrix correlations concentrate
into a low-rank ECS, the better the layer has been regularized. A natural question arises then,
namely, can a layer be Over-Regularized and can we detect this? and in large, Empirically, we
do indeed observe that over the course of training, α decreases, (See Figure 27 (a), Section 6.6,)
and that the models predictions are concentrated into the ECS, (See Section 6.2). Thus, we
also interpret α and ECS concentration to be measures of learning itself, meaning that NNs are
self-regularizing [25].
Importantly, however, the HTSR phenomenology indicates that Alpha usually lies in the FatTailed Universality class, such that α ∈ [2, 6]. When α < 2, the ESD is Very Heavy Tailed (VHT),
and, also, this indicates that W has an anomalously large variance. That is, W is atypical.
Occasionally, but very infrequently, we do observe α < 2, and in large, production quality models
(like Llama). Interestingly, we also observe that, frequently, when the HTSR α < 2, the SETOL ERG
condition holds fairly well. This is further explored in Section 6
We have applied the WeightWatcher tool to have examined dozens of modern, very large NNs;
of particular interest are the so-called Large Language Models (LLMs) that have revolutionized
the field of AI. To that end, in Figure. 6, we present the WeightWatcher layer Alpha metrics for
the Falcon-40b and the Llama-65b LLMs. 17
For the Falcon-40b model, all of the layer Alpha range between α ∈ [2, 6], and therefore lie in
the Fat-Tailed Universality class (in Table 1) and are well-fit. In contrast, looking at the Llama40b layer Alpha, very many have α > 6, indicating these layers are under-fit, and while a few have
α < 2, suggesting these are over-fit. Finally, there are more layers with α ∼ 2 in Llama-65 vs
Falcon-40b.
17Similar results are found for the larger, more modern Llama models, and can be found on the WeightWatcher
website[67]
27
Figure 6: Falcon vs Llama
The observations on Llama-2 suggest that the layers with α ≤ 2 are compensating for the
layers with α > 6, and yielding suboptimal performance for the Llama-65b architecture. Based
on these observations, we hypothesize that, in a multi-layer-perceptron (MLP), when one layer
does not or can not learn, then other layers will have to compensate, and will be overloaded with
the training data, leading to α < 2, and even the ERG condition ∆λmin < 0.
In Section 6.6, we will test this hypothesis. By reducing the trainable parameters in a small
MLP, we can simulate the situation seen above in the Llama-65b model, and observe the formation
of a Very Heavy Tailed (VHT) ESD in the dominating layer weight matrix. Overloading results
from having too few parameters for the complexity of the task. Adding more data increases the
load up to the total complexity of the task itself.
Moreover, we will also argue that in our experiments, the model enters a kind of glassy metastable phase, similar to the kinds of phases predicted by classic StatMech theories of learning [8]
(described below). Section 6.6 will explore how far we can push the analogy of glassy systems in
our experiments to stress test the SETOL approach. In particular, we will see effects such as the
slowing down of its dynamics, leading to a kind of hysteresis, specific to the under-parameterized
regime.
28
4 Statistical Mechanics of Generalization (SMOG)
In this section, we review the StatMech approach to learning: both to understand how it is
usually applied in Statistical Mechanics of Generalization (SMOG) theory; and to understand how
our Semi-Empirical approach in SETOL is similar to and different from the traditional approach.
We will also obtain an expression for the Generalization Accuracy (or Model Quality Q¯ST ) for
the classic Student-Teacher (ST) model of the Linear Perceptron (in the AA, and at high-T), as
described in [7, 8]. In Section 5, we will generalize this to a Layer Quality metric, Q¯, for a layer
in a general Multi-Layer Perceptron (MLP), i.e., Q¯ST → Q¯, so that Q¯ can then be expressed in
terms of the ESD of the NN layer.
Outline. Here is an outline of this section.
• Approaches to the SMOG. In Section 4.1, we explain the mapping from the StatMech
theory of disordered systems to the StatMech theory of NN learning (SMOG); and how our
Semi-Empirical approach (SETOL) is similar to and different from the traditional approach.
• Mathematical Preliminaries. In Section 4.2, we review the mathematical details of
StatMech, providing definitions and detailed derivations of quantities and expressions necessary later.
• Student-Teacher Model. In Section 4.3, we discuss the setup of the Student-Teacher
(ST) model as a general means to estimate the Average Generalization Error empirically.
First, in subsection 4.3.1, we describe the ST setup with an operational analogy. Then, in
subsection 4.3.2, we derive the (new) result for the ST Model Quality, Q¯ST , using the setup
of the classic (ST) model for the Generalization Error (and accuracy) of the Perceptron (in
the AA, and at high-T).
Additional information can be found in the Appendix.
• Symbols and Equations. In Section A.1, we summarize the important symbols and
key results, including the dimensions of the vectors and matrices, different notations for
energies, and key equations.
• Summary of the SMOG. In Section A.2, we provide a more detailed analysis of the results
we derive in Section 4.3.2. In particular, in Section A.2.1, we repeat the derivations of
the ST Generalization Error E¯ST
gen and related quantities (from Section 4.2), using more
concrete notation to align with [7, 8]; and in Section A.2.2, we use this to derive the matrixgeneralization of the ST Annealed Error Potential ϵ(R) (as well as the normalization for
the weight matrices, necessary for later).
4.1 StatMech: the SMOG approach and the SETOL approach
In this subsection, we review the basic StatMech setup necessary to understand SMOG theory as
well as SETOL. This theory was developed in the 1980s and 1990s [7, 8, 90, 6, 91, 5].
Traditional SMOG theory. In traditional SMOG theory, one maps the learning process of a NN
to the states and energies of a physical system. The mapping is given in Table 2. SMOG theory
models the SGD training of a Perceptron on the data, x
n
, to learn the optimal weights, w, as
a Langevin process.18 The power of the StatMech approach comes from the fact that the core
18Typically, we have no guarantee of the true equilibrium in a high-dim nonconvex landscape; so, when the
Thermodynamic limit exists, the Langevin process converges or relaxes to the thermodynamic equilibrium.
29
Statistical Physics Neural Network Learning
Gaussian field variables Gaussian i.i.d (idealized) data ξ
n
∈ D
State Configuration Trained / Learned weights w
State Energy Difference Training and Generalization Errors E¯
train, E¯
gen
Temperature Amount of regularization present during training T
Annealed Approximation Average over data ξ
n first, then weights w.
Thermal Average Expectation w.r.t. the distribution of trained models
Free Energy Generating function for the error(s) F
Table 2: Mapping between states and energies of a physical system and parameters of the learning
process of a neural network.
SETOL Terminology Explanation
Model Quality Q¯ Generalization accuracy,
in the AA and at high-T
Layer Quality Q¯
Layer contribution to the accuracy,
in the AA and at high-T
Layer Quality-Squared Q¯2 Layer Quality squared, used for technical reasons
Quality Generating Function ΓQ¯,ΓQ¯2 Generating function for Quality
Annealed Hamiltonian Han Energy function,
for errors or accuracies
Effective Hamiltonian Hef f Exact energy function,
but restricted to a low-rank subspace
Table 3: Additional terminology introduced for the SETOL. Notice that the Quality Generating
Function Γ is simply one minus the Free Energy, Γ ∶= 1 − F, but it is introduced because sign
convention for the Free Energy is always decreasing with the error. In contrast, we define the
Hamiltonian in terms of the model error or accuracy, depending on the context.
concept of Thermal average corresponds to taking the expectation of a given quantity only over
the set of trained models, as opposed to uniformly over all possible models (or in a worst-case
sense, over all possible models in a model class). This capability is particularly compelling in
light of the StatMech capacity to characterize disordered systems with complex non-convex energy
landscape (which can even be glassy, characterized by a highly non-convex landscape [8, 82, 91],
and recognized classically by a slowing down of the training dynamics [92]). Thus, concepts such
as training and Generalization Error arise naturally from integrals that are familiar to StatMech;
and theoretical quantities such as Load, Temperature, and Free Energy also map onto useful
and relevant concepts [93]. The Free Energy is of particular interest because it can be used as
a generating function to obtain the desired Generalization Error and/or accuracy. We wish to
understand how to compute the associated thermodynamic quantities such as the expected value
of the various forms of the Average Generalization Error (E¯
gen), Partition Function (Z), and the
Free Energy (F) and other Generating Functions (Γ).
The Student-Teacher model. We seek to compute and/or estimate the Average Generalization Accuracy for a fixed Teacher Perceptron T by averaging over an ensemble of Student S
Perceptrons, in the Annealed Approximation (AA), and at High-Temperature (high-T); we call
this ST Model Quality, and denote it Q¯ST . In Section 5, we generalize Q¯ST to an arbitrary
30
layer in a Multi-Layer Perceptron, giving a Layer Quality, i.e., Q¯ST → Q¯. This formulation of
the ST problem differs from the classic approach [8, 91] in that we treat the Teacher as input
to the theory rather than a random vector t and that Teacher provides the trained weights w
explicitly as opposed to the labels y
T
µ
. In the simpler Perceptron formulation, there are only a
few degrees of freedom (i.e., the ST overlap R, the feature load, the inverse Temperature β, and
the number of training examples n), whereas in the matrix generalization, the structure of the
empirical Teacher weight matrix T = W provides N × M additional degrees of freedom, allowing
for a much richer theoretical analysis. This is one of many ways that distinguishes the current
approach from previous work. Because of this, we present both a pedagogic derivation of Q¯ST
(for a general NN in Section 4.2, and for the ST model specifically in the Appendix, Section A.2),
whereas in Section 4.3.2 we provide a simple derivation, assuming the Annealed Approximation
and High-Temperature at all times.
Towards a Semi-Empirical Theory. In the SETOL approach to StatMech, we want a matrix
generalization of the Student-Teacher Model Quality, Q¯ST , for a single Layer Quality Q¯ ∼ Q¯NN
L
in an arbitrary Multi-Layer Perceptron (MLP). This matrix generalization is a relatively straightforward extension of the classical (i.e., for a vector Teacher) SMOG ST Model Quality (but our
SETOL approach will use it in a conceptually new way).
In our matrix generalization, the Teacher vector t is replaced by a Teacher matrix T (i.e.,
t → T); and, in our SETOL approach, T represents actual (pre-)trained NN weight matrix (i.e.,
T ≃ W) that has been trained on real-world, strongly correlated data D. As such, for our SETOL
theory, we seek an expression that can be parameterized by the Teacher, and in particular by
the ESD of the Teacher. This is what makes the basic method Semi-Empirical: we make a
modeling assumption that the Teacher has the same limiting spectral distribution as the Student,
and hence the same PL fit parameter α. This is all done with the understanding that later we
will augment (and hopefully “correct”) our mathematical formulations with phenomenological
parameters fit from experimental data. To make the Semi-Empirical method a Semi-Empirical
Theory, we not only seek to parameterize our model; but we also try to understand how to
derive HTSR empirical metrics, such as Alpha and AlphaHat, how they arise from this formalism,
how they are related to the correlations in the data, and why they are transferable and exhibit
Universality. Importantly, we do not just seek a method with adjustable parameters, but rather
formulate the theory using techniques similar to those to explain the origins of Quantum Chemisty
Semi-Empirical methods, resulting in formal expressions that resemble a renormalized Self-Energy
and/or Effective Hamiltonian from such approaches [94, 54, 52, 95, 50, 51, 57]. This gives what
we call a Semi-Empirical Theory.
4.2 Mathematical Preliminaries of Statistical Mechanics
SubSection Roadmap Briefly, in the following subsection, we start by defining an arbitrary
NN model, with weights (w). Then, we explain the difference between using real-world (x) and
random data (ξ). This lets us define an energy error function, E
n
L
(w), the error the NN makes
on the data. We then explain how to take different kinds of Thermodynamic averages of the
data, including Sample and Thermal average and the implications, and the difference between
computing errors and accuracy. Next, we define the Free Energy (F) for the error(s), and the
Generating Function (Γ) for the accuracy and/or quality. From here, we explain the Annealed
Approximation (AA) and how to define the Annealed Hamiltonian, Han(w), a crucial expression
that will be the starting point later for our matrix model. In the AA, Han(w) simplies to
Han
hT = ϵ(w), where ϵ(w) is an Annealed Error Potential that depends only on the weights w.
Likewise, we can define the Self-Overlap, η(w) ∶= 1 − ϵ(w), which is useful for obtaining the
31
Quality. We show how to obtain the Average Training and Generalization Errors E¯
train, E¯
gen using
the StatMech formalism, which defines them in terms of partial derivatives of the Free Energy (F).
Doing this, we show that in the AA and at high-T they are equivalent, [E¯ST
train]
an,hT = [E¯ST
gen]
an,hT
,
and can both be expressed as a Thermal Average over all Students, as a function of the Teacher,
as [E¯ST
gen]
an,hT = ⟨Han
hT (R)⟩
β
s
= ⟨ϵ(R)⟩β
s
. Note that these averages are obtained by using the Free
Energy as a Generating Function. We then explain how to obtain the Model Quality as partial
derivatives of a Generating Function (ΓQ¯). We then discuss the more advanced techniques,
the Large-N Approximation and the Saddle Point Approximation (SPA), which will be used
extensively later. Finally, we introduce HCIZ integrals, which will be necessary to evaluate the
matrix-generalized form of ΓQ¯ to obtain the final result.
In this subsection, we will compare and contrast several types of averages, energies and other
Thermodynamic concepts we will encounter.
4.2.1 Setup. In Section 4.2.1, we will start by describing the basic setup of the problem, including
the distinction between the actual training process and how we model the training process.
4.2.2 BraKets, Expected Values, and Thermal Averages. In Section 4.2.2, we will describe
our use of physics BraKet notation for both Sample Averages (over the data x) and Thermal
Averages (over the weights w) and —in particular, under the Annealed Approximation (AA)
and in the High-Temperature (High-T) limit—showing how they relate to each other and
to the notion of Replica Averages.
4.2.3 Free Energies and Generating Functions. In Section 4.2.3, we will make a connection
between these different averaging notions and Free Energies and Generating Functions,
showing how they relate to each other.
4.2.4 The Annealed Approximation (AA) and the High-Temperature Approximation
(high-T). In Section 4.2.4, we explain the Annealed Approximation, the High-Temperature
approximation, and the Thermodynamic Large-N limit and the Saddle Point Approximation (SPA). We also introduce the Quality Generating Function ΓQ¯
4.2.5 Average Training and Generalization Errors and their Generating Functions. In
Section 4.2.5, we will show how to compute the Average Training and Test/Generalization
Errors E¯
train, E¯
gen using the Free Energy as a Generating Function, and how these errors
are related to each other in the AA and High-T limit.
4.2.6 The Thermodynamic limit. In Section 4.2.7, we discuss the Large-N Thermodynamic
limit and the Saddle Point Approximation (SPA), along with the concepts of Self-Averaging
and Wick rotations.
4.2.7 From the ST Perceptron to a Matrix Model. Finally, in Section 4.2.8, we introduce
the physical concepts necessary to understand the SETOL matrix model, including the Wide
Layer Large-N limit and the notion of Size-Consistency. We then describe how to obtain
the Layer Quality Q¯ from a matrix-generalized Thermal Average over random matrices,
called an HCIZ integral.
The various symbols and other important results are summarized in the Appendix A.1
4.2.1 Setup
In formulating SETOL, we want a methodology to estimate quantities such as the generalization
error and/or accuracy that does not rely on traditional methods that split the data into training
32
Real Data
xµ, yµ ∈ D, µ = 1, 2, . . . , n
where
yµ ∈ {−1, 1}
xµ is real world, correlated data
xµ ∈ RM, ∥xµ∥
2
F = 1
Idealized Data
ξµ, yµ ∈ D, µ = 1, 2, . . . ,∞
where
yµ ∈ {−1, 1}
ξµ ∼ N (0,
1
M I)
ξµ ∈ RM, ∥ξµ∥
2
F = 1
Figure 7: Mapping from a fixed set of n real-world, correlated data instances [x, y] ∈ D to an
uncorrelated, random model of idealized data [ξ, y] ∈ D, drawn from a Gaussian i.i.d. distribution.
and test sets because SETOL makes data independent predictions. To accomplish this, we idealize
the empirical data distribution as Gaussian fields, and we will use StatMech to construct quantities
(basically, free energies or generating functions) so that we can compute training/testing errors
by taking appropriate derivatives of these quantities.
In more detail, we imagine training a NN on n training data instances, xµ, which are mdimensional vectors, with labels yµ, chosen from a large but finite-size training data set D. The
goal of training a perceptron (or, later, a NN) is to learn the m weights of the vector w (or, later,
a weight matrix W) by running a form of stochastic gradient descent (SGD) to minimizing a
loss function L (ℓ2, cross-entropy, etc.). We want to approximate the actual network’s learning
dynamics by an analytically tractable ensemble so that we can then obtain analytic expressions for
the Free Energy and Generating Function we then use to compute Thermodynamic averages such
as the Average Generalization Error (E¯
gen) and Model Quality (Q¯) (which is our approximation
to the Average Generalization Accuracy).
Counting Samples and Features. We let the number of training samples be n and the
dimension (i.e., number of features) for each sample be m. When we move to SETOL matrixmodel, there will still be n training examples, but treated implicitly. The m-dimensional weight
vector w will become an N × M weight matrix, W, (N ≥ M) described as having N number of
M-dimensional (input) feature vectors. Table 4 summarizes these conventions.
Definition Vector Matrix
Total number of data samples used in training. n n
Number of features per training sample (Input dimension). m M
Dimension of layer output (Output dimension) 1 N
Number of free parameters (in R) 1 M(M − 1)/2
Energy scaling n n × N × M
Table 4: In the ST Perceptron vector model, lowercase m is the dimension of the weight vector
(total parameters), which is also the number of features per sample. In the vector case, there is
one free parameter – the overlap R (or angle θ) between student and teacher. In the SETOL matrix
model, uppercase N and M are the input and output dimensions of the weight matrix, and the
matrix the overlap R, being an M ×M symmetric matrix, has M(M −1)/2 free parameters. The
Energy scales as n in the ST Perceptron model, and as n × N × M in the SETOL matrix model.
Actual and Idealized Data and Energies. Consider a large set n of actual, real-world data,
(xµ, yµ) ∈ D, µ = 1,⋯, n, (17)
33
where xµ ∈ R
m is an m-dimensional real vector, yµ is a binary label taking values {−1, 1}, and
D denotes the finite-size dataset. WLOG, we assume that xµ is normalized such that the norm
squared is unity:
∥xµ∥
2
∶=
m
∑
i=1
x
2
µ,i = 1 (18)
We call x
n
an n-sized sample (of the training data instances x) from D.
We define model errors as an energy EL, the difference squared between the Student’s and
Teacher’s output. Smaller energies correspond to smaller errors and therefore better models. For
example, for the mean-squared-error (MSE) loss, one has
EL(w,xµ, yµ) ∶= (yµ − E
out
NN (w,xµ))2
, (19)
where E
out
NN (w,xµ) is output prediction of the NN, as in Eqn. 1.
19
We assume that there is a real-world training process that generates a Teacher model (T),
trained on a particular dataset D, and we seek a theory for the Quality (or generalization accuracy) of this model. To estimate quantities such as the generalization error or generalization
accuracy, we will adopt an approach that starts off by assuming idealized Gaussian data D, but
ends with the NN with a parametric model that we will fit with a Semi-Empirical procedure
(described later). To that end, in the theoretical setup, the replacement scheme is,
D → D, xµ → ξµ, yµ → yµ, (20)
where we denote the model training and/or test data instances as (ξ, y) such that
(ξµ, yµ) ∼ D, µ = 1,⋯,∞. (21)
Here, ξµ ∈ R
m is a random vector (i.e., an m-dimensional random variable), sampled from an i.i.d
m-dimensional joint distribution D of both the features ξµ, which are Gaussian, and the labels
yµ, which are binary NN outputs.
4.2.2 BraKets, Expected Values, and Thermal Averages
Given the setup from Section 4.2.1, we will treat the total energy E
n
L
(w,x
n
) as a sum over some
n-size data set x
n
. We can write the Total Data Sample Error, using an overloaded operator
notation, as
E
n
L(w,x
n
, y
n
) ∶=
n
∑
µ=1
EL(w,xµ, yµ), (22)
where the superscript n in E
n
L
(w,x
n
, y
n
) indicates this is a sum over the entire set of n pairs
[x
n
, y
n
]. We should keep in mind that this depends on the specific set of n data pairs [(xµ, yµ) ∈
D ∣ µ = 1,⋯, n]. Here, we treat the labels y as implicit in E
n
L
(w,x
n
). We will therefore drop the
yµ and y
n
symbols, and simply write this total error / energy difference as
E
n
L(w,x
n
) ∶=
n
∑
µ=1
EL(w,xµ), (23)
19Note that the Energy Landscape function E
out
NN is nonlinearly related to EL. We treat E
out
NN and EL as being
the energy states of two separate, but interacting, thermodynamic systems, and the loss function L is a transfer
function that relates them.
34
which is now a function of the entire set of n vectors [x
n
].
20 This operator notation will prove
useful later in Section 4.3.2 (see Eqn. 86) and in Appendix A.2.
We will not, however, work directly with samples and sample averages. Instead, we will model
them. To that end, we need to estimate them with a theoretical approach. For example, we can
write the Total Data Sample Error in terms of our random data variables ξ
n
, written formally as
E
n
L(w, ξ
n
) ∶=
n
∑
µ=1
EL(w, ξµ), (24)
but to evaluate this we need to take an integral and/or Expected Value over the data sample ξ
n
.
Expected Values. We need to compute various sums and integrals, sampling from a idealized
D for the actual (i.e, real-world) data distribution D, over n-sized data samples (or data sets), and
also over distributions of weights (w, s) and weight matrices (W,S, A,⋯). This will frequently
(but not always) be defined as more familiar Expected Values. We will denote Expected Values
using physics Bra-Ket notion. Importantly, we use the term Expected Value in the physics
sense, and BraKets will denote an un-normalized sum or integral; when the quantity is to be
normalized, we will denote the normalization explicitly. For example, given a function f(ξ), we
write the BraKet integral as:
⟨f(ξ)⟩ξ ∶= ∫ dµ(ξ)f(ξ). (25)
We would express an n-sized sample average over f() as
⟨f(ξ
n
)⟩
ξ
n ∶=
1
n
∫ dµ(ξ
n
)f(ξ
n
)
=
1
n
⎡
⎢
⎢
⎢
⎢
⎣
n
∏
µ=1
∫ dξµP(ξµ)
⎤
⎥
⎥
⎥
⎥
⎦
⎡
⎢
⎢
⎢
⎢
⎣
n
∑
µ=1
f(ξµ)
⎤
⎥
⎥
⎥
⎥
⎦
. (26)
The BraKet ⟨⋯⟩
ξ
n denotes an integral over an n-sized sample of idealized Gaussian-field data
ξ
n
, with the convention that summation over n points and normalization 1
n
appears inside the
Bra-Ket implicitly.
For example, we treat an Expected Value of the Data Sample Error, which is correlated
when using real-world data D, using the uncorrelated idealized data D; this is specified with the
following mapping:
1
n
E
n
L(w,x
n
)
Expected Value
ÐÐÐÐÐÐÐÐ→ ⟨E
n
L(w, ξ
n
)⟩
ξ
n , (27)
where the BraKet ⟨⋯⟩
ξ
n denotes the integral over the idealized dataset ξ
n
of a normalized average
over the sample. In this case, we obtain:
⟨E
n
L(w, ξ
n
)⟩
ξ
n ∶=
1
n
∫ dµ(ξ
n
)E
n
L(w, ξ
n
)
=
1
n
∫
⎡
⎢
⎢
⎢
⎢
⎣
n
∏
µ=1
dξµP(ξµ)
⎤
⎥
⎥
⎥
⎥
⎦
⎡
⎢
⎢
⎢
⎢
⎣
n
∑
µ=1
EL(w, ξµ)
⎤
⎥
⎥
⎥
⎥
⎦
,
=
1
n
n
∑
µ=1
∫ dξµP(ξµ)EL(w, ξµ), (28)
20In the classic Student Teacher model, the labels y
n
represent the Teacher outputs and are effectively treated
as either uniform random variables to be averaged over later, or as the outputs of an optimal Teacher. In this work,
the Teacher weights w, not labels, are input (i.e., a fixed ESD, later) so we can drop the labels.
35
where P(ξ
n
) is a product of n i.i.d. m-dimensional Gaussian distributions. The subscript ξ
n
indicates this is an Expected Value of an average of an n-size sample of ideal data, where the
Expected Value is taken over datasets, and the average is over data points within each sample.
The third line follows because the n samples are i.i.d. (This is used in both Sections 4 and 5.)
(The normalization 1
n
ensures the Bra-Ket is a proper Expected Value of a sample average.) The
measure dµ(ξ
n
) denotes the probability measure associated with a single realization of a random
data sample of size n drawn from an m-dimensional idealized Gaussian distribution. A subscript
ξ on the Ket as ⟨⋯⟩ξ indicates that the integral is over potential data points, not an average of
a data sample (i.e., there would be no 1/n prefactor).
Size-Extensivity and Size-Intensivity. A key requirement for the Thermodynamic limit in
StatMech is Size-Extensivity: that physically meaningful quantities (i.e, total energies and free
energies) scale linearly with the system size or number of parameters, i.e. m (or N × M, below).
Extensive quantities scale with system size, intensive ones do not. Along with this, Thermodynamic average quantities should be Size-Intensive, meaning that they remain independent of n (or
N × M, below) as the system size increases. In our setting, Size-Extensivity and Size-Intensivity
underpin the so-called Large-N limits we employ, ensuring that macroscopic observables become
independent of microscopic fluctuations so that the system approaches equilibrium as expected.
In the theory of the Statistical Mechanics of Generalization (SMOG), however, one also holds
the (feature) load = n/m fixed when taking the Thermodynamic limit, and we will do the same [90,
8, 91, 93]. In fact, we will take load to be unity, and not treat it explicitly. 21 Moreover, we
will use n as the primary variable below as we are interested in how the Free Energy and other
quantities change with n. That is, we use the notational convention in [10] (as opposed to [8]).
Consequently, for our purposes here, this means that the Thermodynamic limit and
the ‘Large-N limit in n’ are effectively the same.
As an example of Size-Extensivity and Size-Intensivity, we write the Expected Value (i.e., the
data-average) of Data Sample Error E
n
L
(w, ξ
n
) (Eqn. 28) in the Large-N limit in n as
lim
n≫1
⟨E
n
L(w, ξ
n
)⟩
ξ
n = lim
n≫1
1
n
∫
⎡
⎢
⎢
⎢
⎢
⎣
n
∏
µ=1
dξµP(ξ
n
)
⎤
⎥
⎥
⎥
⎥
⎦
⎡
⎢
⎢
⎢
⎢
⎣
n
∑
µ=1
EL(w, ξµ)
⎤
⎥
⎥
⎥
⎥
⎦
. (29)
Here, the notation (n ≫ 1) means n grows arbitrarily large, but is not necessarily at the limit
point (n = ∞). And, again, the load is fixed, so n ≫ 1 ↔ m ≫ 1. The Total Data Sample Error
E
n
L
(w, ξ
n
) is Size-Extensive, whereas the average ⟨E
n
L
(w, ξ
n
)⟩
ξ
n is Size-Intensive. This limit will
be implicit later when taking a Saddle Point Approximation (see below). 22
The data-averaged error ⟨E
n
L
(w, ξ
n
)⟩
ξ
n will appear frequently below. For convenience and for
compatibility with [8], we denote it using the symbol ϵ(w):
ϵ(w) ∶= lim
n≫1
⟨E
n
L(w, ξ
n
)⟩
ξ
n (Size-Intensive). (30)
where, by our normalization here, ϵ(w) ∈ [0, 1]. The symbol ϵ(w) is our theoretical estimate of
the sample average E
n
L
(w, ξ
n
) (Eqn. 29), well-defined for any n. We also call ϵ(w) the Annealed
Error Potential, which will be made clear below.
21For our purposes, we only require that n/m is some fixed quantity as we take this limit. We note that n/m = 1
is the Double Descent Worst Case, however assuming that n/m is some other constant addresses this without
changing any of the results.
22As we are working within a “physics-level of rigor”, we take some liberties in evaluating these Large-N limits;
and we leave the formal proofs for future work.
36
It is also convenient to write Total Annealed Error Potential as an Energy,
E(w) ∶= nϵ(w) (Size-Extensive). (31)
This will only be useful when the Thermodynamic limit exists, and this can be reasonably expected
for the Annealed Approximation (AA), which is the regime in which SETOL will be developed.23
From Errors to Accuracies: The Average Generalization Accuracy, the Quality, and
the Self-Overlap. We have been discussing various forms of errors. In SETOL, we will, however, primarily be concerned with approximating the Average Generalization Accuracy, or, more
generally, the Quality of a NN model and/or its layers.24 The average accuracy is simply one
minus the error. To represent this, we introduce the Self-Overlap η(w), which is defined generally
as
η(w) ∶= 1 − ϵ(w), η(w) ∈ [0, 1], (32)
and which describes the “overlap” between the true and the predicted labels. Unlike here, however,
in later sections (4.3.2, 5.1, and Appendix A.3) we will first define a data-dependent Self-Overlap,
so that we may obtain η(w) ∶= ⟨η(w, ξ)⟩
ξ
n directly.
Braket Notation. We will use physics Bra-Ket notation, ⟨⋯⟩, to denote different kinds of sums
and integrals, with superscripts and subscripts, and for Expected Values (estimated theoretical
averages). We use superscripts to denote the kind of integral or average:
Thermal ⟨⋯⟩
β
, Annealed ⟨⋯⟩
an, high-T ⟨⋯⟩
hT , HCIZ ⟨⋯⟩
IZ, etc.
We use subscripts to emphasize the dependent variables:
weights ⟨⋯⟩w, ⟨⋯⟩s, ⟨⋯⟩S
data ⟨⋯⟩ξ,⟨⋯⟩ξn ,⟨⋯⟩
ξ
n
When averaging over whole datasets ξ
n
, the subscript will appear with a bar (i.e. ξ
n), but when
just integrating over a particular dataset, no bar will appear (i.e., ξ
n
). We also reuse these
symbols for other quantities, such as the Z
an,hT
n , E¯an,hT
gen , Han(w), etc, but may mix-and-match
subscripts and superscripts for visual clarity.
Sign Conventions. Finally, we discuss the sign conventions used. Since errors decrease with
better models, Energies (EL(w, ξ), E(w), ϵ(w),⋯) and Free Energies (F) are minimized to obtain
better models. Likewise, since accuracies increase with better models, Qualities (Q¯, Q¯2
,⋯), SelfOverlap (η), and Quality Generating Function (Γ) would be maximized to obtain better models.
An exception will be Hamiltonians (H,H), where the sign convention will depend on context.
23We should note that, while our model training and generalization errors are always expressed as energies, an
energy is not necessarily a model error.
24Technically, the Quality will estimate the average Precision rather than the Accuracy. This will distinction
will be clarified in the Section 4.3.
37
Thermal Averages (over weights). To evaluate the expectation value of some equilibrium
quantity that depends on the weights w (say E
β
w[f(w)]), one uses a Thermal Average. By this,
we mean a Boltzmann-weighted average: given a function f(w), we define the Thermal Average
over w as
⟨f(w)⟩β
w ∶=
1
Zn
∫ dµ(w)f(w)e
−βE(w)
, (33)
where the superscript β denotes Thermal Average, β =
1
T
is an inverse temperature, and Zn is
the normalization term (or Partition function), defined as
Zn ∶= ∫ dµ(w)e
−βE(w)
, (34)
defined for the n-size Data Sample ξ
n
. In particular, when we want to compute the Thermal
Average of the Total Energy difference or Error E(w) over w, we could write
⟨E(w)⟩β
w ∶=
1
Zn
∫ dµ(w)E(w)e
−βE(w)
. (35)
Importantly, we will never calculate the average errors directly like this. Instead, we will calculate
them from partial derivatives of the Free Energy Fn (as shown below). Also, we may use ⟨⋯⟩
β
ξn
to denote what looks like a Thermal Average over the data; this is not essential and only used
once below and can be ignored for this section.
Other Notation: Overbars, Superscripts and Subscripts. As above, we may also occasionally denote averages using the common notation for expected values, E[⋯]. See Table 11
and 12 in Appendix A for a list of these and other notational conventions and symbols we use.
When discussing quantities such as the Free Energy (F), training and test errors/eneries (E),
the Layer Quality (Q), etc., we will place a bar over the symbol (i.e., F¯, E¯, Q¯, etc.) when referring
to an average over the data n. Otherwise, we will refer to these quantities as the total (averaged)
energy, error, quality, etc.
Finally, when referring to the model (i.e., theoretical) training and generalization errors, we
will use the superscript ST for the average Student-Teacher training and generalization errors,
E¯ST
train and E¯ST
gen, respectively, and the superscript NN for the matrix-generalized NN layer average
training and generalization errors, E¯NN
train and E¯NN
gen , respectively. When referring to empirical
errors, we denoted these as E¯emp
train and E¯emp
gen , respectively.
4.2.3 Free Energies and Generating Functions
If one needs an average energy (or error), it is often easier to calculate the associated Free Energy
and take corresponding partial derivatives than it is to compute that quantity directly via an
expected value or Thermal Average. Generally speaking, a Free Energy, Fn, is defined in terms
of a partition function Zn as
βFn ∶= −lnZn. (36)
Keep in mind that Zn may actually be a function of the idealized data ξ (or some other variables),
i.e., Z(ξ), but we usually don’t write this explicitly. Likewise, while both Fn and Zn depend
explicitly on the system size n, we will only include these subscripts when emphasizing this. Also,
Fn has units of Energy or Temperature, so βFn = −lnZn is a “unitless” Free Energy. Each model
(in single-layer models) and/or layer (in multi-layer models) will have its own Partition Function
38
and associated Generating Functions. We call Fn and Zn Generating Functions because they can
be used to generate the model errors. That is, given an Fn and/or Zn, we can “generate” the
training and generalization errors with the appropriate partial derivatives w/r.t β and n [9, 10].
From this generating function perspective, i.e., when using a generating function to compute
quantities of interest, we can work with other transformations of Fn. Most notably, we will
consider
Γn = n − Fn. (37)
where we note that these Energies scale as n for the ST vector model, but for a matrix model
they scale as n × N × M; see Appendix A.2.1). The quantity Γn decreases as the error increases
(as opposed to Fn, which increases), i.e., it increases as the accuracy of quality of the model
increases. Thus, we will use it as a generating function for the model quality/accuracy. For
average Qualities, one has
Γ¯
n ∶= 1 − F¯
n (38)
for the model or layer under consideration (see below).
4.2.4 The Annealed Approximation (AA) and the High-Temperature Approximation (high-T)
In the traditional SMOG approach, one models the (typical) generalization behavior of a NN by
defining and computing the Expected Value of the Free Energy of the model. The full expected
value of the Free Energy, βFn = −lnZn, with respect to the (idealized) data ξ
n
, is:
Eξn [βFn] = βF¯
n ∶= −⟨lnZn⟩
ξ
n , (39)
where ⟨lnZn⟩
ξ
n means where we average over the n samples of the data (ξ
n
, of size n). This is
also called the Quenched Free Energy. This is, however, frequently too difficult to analyze, and
doing so typically requires a so-called Replica calculation.
The Annealed Approximation (AA) is a way of taking the data-average first and greatly
simplifies the model under study and its analysis. The standard way to move forward is to follow
the methods used in disordered systems theory [8, 5]. The mapping is:
Average over the Data ↔ Annealed Approximation ↔ Disorder Average
Learning the Weights ↔ NN Optimization Algorithm ↔ Thermal Average.
through the NN optimization algorithm, and then evaluates the training or test accuracy by
averaging over the (actual) data x
n
. In the theoretical analysis, however, the steps are reversed.
One first averages over the (model) data ξ
n
(i.e., a multi-variate Gaussian distribution) so that
the disorder (variability in the data) is averaged out. Then, a Thermal Average is used to model
the final state of NN learning process, the learned weights.
The Annealed Approximation (AA). Formally, the AA makes the substitution
−⟨lnZn⟩
ξ
n ≈ −ln⟨Zn⟩
ξ
n . (40)
Here, we are averaging over the disorder. We may associate:
−β⟨lnZn(ξ
n
)⟩
ξ
n ∶ the Quenched Free Energy
−β ln⟨Zn(ξ
n
)⟩
ξ
n ∶ the Annealed Free Energy.
39
Applying the AA amounts to applying Jensens inequality as an equality, and it allows will let us
interchange integrals and logarithms when computing the data average:
1
n ∫ dµ(ξ
n
) ln(⋯)
AA
ÐÐ→ ln 1
n ∫ dµ(ξ
n
)(⋯) (41)
This will allow us to switch the order of the data and the thermal averages, i.e.,
⟨⟨⋯⟩
β
w⟩
ξ
n
AA
ÐÐ→ ⟨⟨⋯⟩
ξ
n ⟩
β
w
, (42)
greatly simplifying the analysis.
The use of the AA is common in StatMech, as it simplifies computations considerably; and it
is chosen when it holds exactly (if, say, x is a typical sample from D and Zw(ξ) has a well-defined
mean). In contrast, there are situations in StatMech when the average is atypical, and then it
one can get different results for the Quenched versus Annealed cases. In a practical sense, one
imagines this may occur when the data is very noisy and/or mislabeled, and this requires a special
treatment [8].
Annealed Hamiltonian Han(w) and Annealed Parition Function Z
an
. When we apply
the AA (as in Eqn. 41), we average over the data ξ
n first. Doing this will allow us to develop a
theory in terms a (Temperature dependent) Annealed Error Potential.
Following [8] (see their Eqn.(2.30)), we will call this average the Annealed Hamiltonian,
Han(w), which we define as
βHan(w) ∶= −
1
n
ln∫ dµ(ξ
n
)e
−βE
n
L
(w,ξ
n)
. (43)
The Annealed Hamiltonian is a simple “mean-field-like” Hamiltonian for the problem. This can
be seen by noting that we can express Eqn. 43 as an integral over a single data example ξ:
βHan(w) = −
1
n
ln∫ dµ(ξ
n
)e
−β ∑
n
µ=1 EL(w,ξµ)
= −ln [∫ dµ(ξ
n
)e
−β ∑
n
µ=1 EL(w,ξµ)
]
1
n
= −ln
⎡
⎢
⎢
⎢
⎢
⎣
n
∏
µ=1
∫ dµ(ξµ)e
−βEL(w,ξµ)
⎤
⎥
⎥
⎥
⎥
⎦
1
n
(44)
= −ln∫ dµ(ξ)e
−βEL(w,ξ)
(45)
This will be a critical piece needed to generalize the vector-based ST Perceptron model to the
matrix-generalized ST MLP model. In BraKet notation, Eqn. 45 can be expressed as
βHan(w) ∶= −
1
n
ln ⟨e
−βE
n
L
(w,ξ
n)
⟩
ξn
= −ln ⟨e
−βEL(w,ξ)
⟩
ξ
.
Using Han(w), we can define the Annealed Partition Function, Z
an
n
, as
Z
an
n
∶= ∫ dµ(w)e
−nβHan(w)
= ∫ dµ(w) exp [−n(−
1
n
ln∫ dµ(ξ
n
)e
−βE
n
L
(w,ξ
n)
)]
= ∫ dµ(w) exp [ln∫ dµ(ξ
n
)e
−βE
n
L
(w,ξ
n)
] (46)
= ∫ dµ(w)∫ dµ(ξ
n
)e
−βE
n
L
(w,ξ
n)
. (47)
40
where the lines after the first line follow by substituting Eqn. 43 into Eqn. 46. Note also that the
order of the integrals in Eqn. 47 is exactly what we expect using the AA, as in Eqn. 42. Also,
analogously to Eqn. 45, we can write Z
an
n as the product of the n = 1 case, Z
an
n = [Z
an
1
]
n
. Finally,
we will only need the high-T version, Han
hT , of Han(w), and this will take a very simple form. 25
The High-Temperature (High-T) Annealed Hamiltonian (Han
hT (w) = ϵ(w)) and Partition Function (Z
an,hT
n ). In addition to the AA, we will be evaluating our models at at high-T.
Notably, the Annealed Hamiltonian Han(w) in Eqn. 43 is a non-linear function of β; by taking
the high-T approximation, we can remove this dependence and obtain the simpler expression that
Han(w) = ϵ(w). This greatly simplifes both the Partition Function, i.e., Z
an,hT
n , and subsequent
results (below).
To obtain the high-T result, we can write the Taylor expansion for e
βE
n
L
(w,ξ
n)
and keep the
first two terms:
e
−nβE
n
L
(w,ξ
n)
≃ 1 − βE
n
L(w, ξ
n
)
´ ¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¸ ¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¶
high−T
+(βE
n
L(w, ξ
n
))2 + ⋯. (48)
Let us now express Han(w) directly in terms of ϵ(w) = ⟨E
n
L
(w, ξ
n
)⟩
ξ
n (see Eqn. 30) as a
Thermal Average at high-T. To do so, let’s take a high-T expansion of Eqn. 43 by expanding the
exponential to first order in β, to obtain
βHan
hT (w) = −
1
n
ln∫ dµ(ξ
n
)[1 − βE
n
L(w, ξ
n
)]
≈ −
1
n
∫ dµ(ξ
n
)βE
n
L(w, ξ
n
)
= ⟨βE
n
L(w, ξ
n
)⟩
ξ
n
=
1
n
β(E(w))
= βϵ(w). (49)
Here, we have used the AA, and the property that ln(1 + y) ≈ y, for ∣y∣ ≪ 1, and the fact that
ϵ(w) takes the form given in Eqn. 30. This gives Han
hT (w) ∶= ϵ(w), which is no longer a non-linear
function of β. Moving forward, we will assume we are taking the high-T limit like this.
Given Eqn. 49, we can now express the Annealed Partition Function at high-T directly in
terms of the Annelaed (i.e., data-averaged) error ϵ(w):
Z
an,hT
n
∶=∫ dµ(w)e
−nβHan
hT (w)
=∫ dµ(w)e
−nβϵ(w)
(50)
If we assume that at high-T we can make this approximation, since we only care about the small
β results. This will prove very useful later when working with HCIZ integrals.
4.2.5 Average Training and Generalization Errors and their Generating Functions
Here, we show how to derive the Average Training Error E¯
train and the Average Generalization
Error E¯
gen, in the Annealed Approximation (AA), and at high-Temperature (high-T), using the
25We will derive expressions for ϵ(R) and E¯ST
gen(R) in Eqn. 97 and Eqn. 98, respectively, using relatively simple arguments. In Appendix A.2.1 we present a more detailed derivation of H
an(R) and H
an
hT (R) = ϵ(R) in
Appendix A.2.2 we show that these derivations generalize to the matrix-generalized case, H
an(R).
41
Free Energy Fn and/or the Partition Function Zn as a generating function. In particular, we
show that, in the AA and at high-T, these errors are (approximately) equal and equal to the
Thermal Average of the Annealed Error Potential, E¯an,hT
train ≈ E¯an,hT
gen ≈ ⟨ϵ(w)⟩β
w.
Generating Functions for the Errors: the StatMech way. In our theory, after applying
the AA, we obtain expressions where the random model data ξ
n has been integrated out. This
leaves formal quantities that depend only on the weights w, which are the variables being learned
during training. Since we are left with a distribution over w, we define the training error not
explicitly as an average over the training data itself, but instead in terms of how the Free Energy,
βF ∶= −lnZn, varies with β, i.e., the amount of stochasticity in the model weights.
Following [9, 10], we define the Average Training Error, in the AA, by differentiating lnZ
an
n
with respect to β:
E¯an
train ∶= −
1
n
∂(lnZ
an
n
)
∂β = −
1
n
1
Zn
∂Zan
n
∂β . (51)
This error captures how the model predictions will vary with changes in the learned weights w,
which implicitly describes how the changes will vary with the training data ξ
n
. Similarly, we
define the Average Generalization Error, in the AA, by differentiating lnZ
an
n with respect to n,
the number of data points. Following
E¯an
gen ∶= −
1
β
∂(lnZ
an
n
)
∂n
+
1
β
ln z(β) = −
1
β
1
Zn
∂Zan
n
∂n
+
1
β
ln z(β), (52)
where z(β) is a constant normalization term that depends only on β (which, moving forward, we
ignore, as it only shifts the scale). This error captures how the model’s predictions will change
as more data points are introduced.
In the Thermodynamic limit (n ≫ 1), these two definitions of the error become equivalent at
High-T, and they are equal to the Thermal Average of the Annealed Error Potential:
E¯an,hT
train = E¯an,hT
gen = ⟨ϵ(w)⟩β
w , n ≫ 1. (53)
To see this, substitute Eqn. 50 into Eqn. 51, and take the partial derivative w.r.t β, to obtain
E¯an,hT
gen ∶=
1
n
∂(−lnZ
an,hT
n )
∂β
= −
1
n
∂
∂β ln∫ dµ(w)e
−nβϵ(w)
(54)
=
1
n ∫ dµ(w)nϵ(w)e
−nβϵ(w)
∫ dµ(w)e
−nβϵ(w)
=⟨ϵ(w)⟩β
w. (55)
Likeswise, if we substitute Eqn. 50 into Eqn. 52, and take the partial derivative w.r.t. n, we
obtain
E¯an,hT
gen ∶=
1
β
∂(−lnZ
an,hT
n )
∂n
= −
1
n
∂
∂β ln∫ dµ(w)e
−nβϵ(w)
(56)
=
1
β ∫ dµ(w)nϵ(w)e
−nβϵ(w)
∫ dµ(w)e
−nβϵ(w)
=⟨ϵ(w)⟩β
w. (57)
42
Notice that both of these results arise because of the simple expression that appears in the
exponent of Z
an,hT
n , namely because −nβHan
hT (w) = nβϵ(w).
This equivalence reflects the fact that when the system is large enough, adding a new data
example to the training distribution is formally equivalent to adding noise, making the two errors
indistinguishable. This approach allows us to define both training and generalization errors in
terms of fundamental thermodynamic quantities, providing a simplified formal framework suitable
for empirical adjustment later.
Also, note that the model data variables ξ do not enter the calculation because we integrated
them out before the calculation of Thermal Average. (This illustrates the difference between
taking an annealed versus a quenched average.) Also, our sign convention is consistent with a
model of NN training that minimizes the total loss (L) and/or ST error, and, therefore minimizes
Free Energies as well.
More generally, we see that we can use the Partition Function, Zn, and/or the Free Energy,
βFn ∶= −lnZn, as a Generating Function to obtain any desired Thermodynamic average by taking
the appropriate partial derivative of the corresponding form of lnZ. In Appendix 4.2 we show
how to obtain E¯an
train and E¯an
gen obtain explicitly in this way using Z
an
.
4.2.6 The Quality (Q¯) and its Generating Function (ΓQ¯)
Here, we explain how to define what we call the Quality Q¯, which is defined as the Thermal
Average of the Self-Overlap, Q¯ ∶= ⟨η(w)⟩β
w, and which can be obtained from the associated
Quality Generating Function ΓQ¯. See Table 5 for a quick summary.
Symbol Definition & Formula Eq. #
ϵ Annealed Error Potential (Large-N , high-T) ϵ(w) = ⟨E
n
L
(w, ξ
n
)⟩
ξ
n 30
η Self-Overlap (average accuracy): η(w) = 1 − ϵ(w) 32
F¯
n Average Free Energy: β F¯
n = −⟨lnZn⟩
ξ
n 39
ΓQ¯ Quality-Generating Function: ΓQ¯ = 1 − F¯
n 38
Q¯ Model Quality (average self-overlap): Q¯ = ⟨η(w)⟩β
w 60
Table 5: Summary of the main intensive (average, per-parameter) quantities. Here n is the
number of free parameters. The average model Quality Q¯ is the model’s average accuracy (one
minus the error), and the Quality-Generating function ΓQ¯ plays the same role as the free energy
F¯ but with an opposite sign convention.
For our purposes below, we define the Model Quality (as in Eqn. 7) as our approximation to
the Average Generalization Accuracy for our model. We denote the Model Quality for the ST
Perceptron model, Q¯ST , and for a general NN, Q¯ST , such that
Q¯ST ∶= 1 − E¯ST
gen (58)
Q¯NN ∶= 1 − E¯NN
gen (59)
In this work, however, the Quality will always be defined at high-T, and so we may write
Q¯ST
= 1 − [E¯ST
train]
an,hT
= 1 − [E¯ST
gen]
an,hT (60)
Q¯NN = 1 − [E¯NN
train]
an,hT
= 1 − [E¯NN
gen ]
an,hT (61)
43
We also define a Layer Quality, simply denoted Q¯, which will describe the contributions an
individual layer makes to the overall Model Quality Q¯NN . To obtain the Layer Quality, we define
an accuracy–or Quality–Generating Function, denoted ΓQ¯, which is analogous to a layer Free
Energy, but with the opposite sign convention.
Generally speaking, the Quality Generating Function ΓQ¯ is defined in the AA, and at high-T
and is given as
βΓQ¯ ∶= ln∫ dµ(w)e
nβη(w)
(62)
For example, for the single-layer ST Perceptron, ΓQ¯ST ∶= n−F
ST
n
(in units of energy or error).
The term ΓQ¯ST is directly related to the Total Free Energy F
ST
n
, which can be seen by substituting
Eqn. 32 for η(w) in Eqn. 62:
βΓQ¯ST = ln∫ dµ(w)e
nβ(1−ϵ(w))
= ln∫ dµ(w)e
nβe
−nβϵ(w)
= ln (∫ dµ(w)e
nβ) + ln (∫ dµ(w)e
−nβϵ(w)
)
= ln e
nβ + ln (∫ dµ(w)e
−nβϵ(w)
)
= nβ − βF ST
n
(63)
Dividing by n, we can also recover the more general relation for the Average Free Energy, Γ¯
Q¯ =
1 − F¯
n. (Eqn. 38). For the matrix case we do something similar; see Appendix A.3.
Likewise, one can show that the Quality Q¯ (again, always in the AA and at high-T) can be
identified as the Thermal Average of the (data-averaged or Annealed) Self-Overlap,
Q¯ = ⟨⟨η(w, ξ)⟩
ξ
n ⟩
β
w
= ⟨η(w)⟩β
w (64)
We can then obtain Q¯ by taking the appropriate partial derivative of its Generating Function,
ΓQ.
For technical reasons, however, we will actually define and use a Generating Function for the
Average Layer Quality-Squared Q¯2
, denoted βΓ
IZ
Q¯2
. In analogy with Eqns. 51 and 52, and at
high-T and Large-N (explained below), we can obtain Q¯2
(see Section 5 Eqn. 128) as
Q¯2
∶=
1
β
∂
∂nβΓ
IZ
Q¯2,N≫1
≈
high-T
1
n
∂
∂β βΓ
IZ
Q¯2,N≫1
(65)
See Section 5 and Appendix A.3 for more details.
4.2.7 The Thermodynamic limit
In the SMOG, the Thermodynamic limit is expressed as the limit of the number of free parameters
m grows arbitrarily large, m ≫ 1, but also holding the feature load n/m fixed (i.e., see [8]). In
these older approaches, one is interested in how the thermodynamic properties depend on the
feature load. Here, however, we are interested in estimating the generalization accuracy, so we
are interested how the thermodynamic properties vary with n, the number of training examples.
For that reason, we will express the Thermodynamic limit as the Large-N limit in n. i.e., while
n grows arbitrarily large, n ≫ 1, while implicitly fixing the corresponding feature load (n/m for
the ST Percepton, and n/N for our ST matrix model, below).
44
To express the average Free Energy F¯
n in the Large-N limit in n, we can write
−βF¯
n = lim
n≫1
1
n
ln∫ dµ(w)e
−nβϵ(w)
. (66)
When this Large-N approximation is well behaved, then the total energy E(w) is extensive, i.e.,
when E(w) = nϵ(w); and, consequently, the total Free Energy is also extensive, i.e., Fn = nF¯
n.
This property is a cornerstone of statistical mechanics, as it allows for meaningful macroscopic
predictions from microscopic interactions.
Self-Averaging. The existence of the limit signifies that the system is Self-Averaging, meaning
that the macroscopic properties are independent of fluctuations, etc. This also implies that the
relevant averages (i.e., training and generalization errors) are the same for almost all samples, or
realizations of the disorder. Additionally, the Annealed and the Quenched averages, ln⟨Zn⟩
ξ
n and
⟨lnZn⟩
ξ
n , respectively, become sharply peaked, and
⟨lnZn⟩
ξ
n ≈ ln⟨Zn⟩
ξ
n , as n ≫ 1 (grows arbitrarily large). (67)
For a NN, Self-Averaging implies that the weights w are typical of the distribution, and therefore
the NN can generalize to similar but unseen test examples.
The Saddle Point Approximation (SPA). In StatMech, one is frequently faced with intractable integrals, and one common solution is to apply Saddle Point Approximation (SPA).
The SPA is a Large-N approximation and is used to describe the system interactions at LargeN , forming the average or mean interaction over the data. For example, when considering the
Large-N in n case, we could write
∫ dµ(w)e
−nβϵ(w)
≈
√ 2π
nϵ(w∗)
e
−nβϵ(w∗
)
, as n ≫ 1 (68)
where w∗
satisfies the saddle point equations:
ϵ(w
∗
) ∶=
∂
∂w
ϵ(w)∣w=w∗ = 0 (69)
ϵ(w
∗
) ∶=
∂
2
∂
2w
ϵ(w)∣w=w∗ > 0. (70)
These equations guarantee that the solution ϵ(w∗
) is at a (local) minimum in w. The SPA can
be applied to many Large-N integrals, as long as the integrand decays exponentially,
Wick Rotation. When working with complex oscillatory integrands, i.e., eiN f(x)
, the SPA is
frequently combined with a Wick rotation, which is used to convert an oscillatory integral to
an exponentially decaying one. The Wick rotation is an analytic continuation that rotates the
integration variable by 90 degrees, i.e., x → ±ix, deforming the integration contour to pass through
a saddle point of the integrand:
x → ix, eiN f(x) → e
−N f(ix)
, (71)
This is typically used in a formal sense; that is, one assumes the resulting complex integrand
is both analytic and well-behaved. One would then apply the SPA in the Large-N in N case.
Below, we will do just this, and use the SPA with a Wick Rotation in both Appendices A.4 and
A.6.
45
When the Thermodynamic limit fails: atypical behavior. When the Thermodynamic
limit fails to exist, the Free Energy will contain additional, non-extensive terms, i.e., F = nF¯
ex +
n
1+xF¯
non−ex, x > 0.
26 In this case, the AA may fail, the SPA may not apply, and the system
may fail to be Self-Averaging. This causes the system to behave in an atypical way, possibly
converging to a meta-stable and/or glassy phase. Indeed, when the weights w are atypical, they
may describe the training data well, but they would fail to describe the test data well; in this
sense, we say the model is overfit to the training data. We will not explicitly consider a model
that is non-extensive; however, we will present empirical results where we suspect the model is
overfit (in Section 6.5) and, additionally, where we observe glassy behavior, which we refer to as
a Hysteresis Effect (in Section 6.6).
4.2.8 From the ST Perceptron to a Matrix Model.
In Section 5, we will move from a single m-dimensional Perceptron vector w to a matrix model
of N × M weights, which can be thought of as N interacting (feature) vectors of dimension
M. Moreover, in the Student-Teacher (ST) model used below, in the vector case, the m free
parameters (o.e., degrees of freedom) of the feature vectors are integrated out, whereas for the
matrix case, we will retain a reduced set of the M degrees of freedom, M˜ . To do this, we need to
ensure the Free Energies and related quantities like the Layer Quality-Squared scale properly.
SETOL uses two Large-N approximations, the Thermodynamic limit, which is a Large-N
limit in n, and the Wide Layer limit, which is a Large-N limit in N:
• Thermodynamic limit: m ≫ 1 (the number of parameters or features, fixed load: n/m = 1)
• Large-N limit in n: n ≫ 1 (the number of training examples, fixed load: n/m = 1)
• Wide Layer limit: N ≫ 1 (the number of feature vectors, n/N fixed)
Note that by “limit” we mean n or N simply grows arbitrarily large. In taking the Wide Layer
Large-N limit in N, we assume implicitly that the layer load, n/N, the number data points per
feature vector or output dimension N, remains constant. 27 In doing this, we expect the matrixgeneralized total Energies (F, Han
,⋯) to be Size-Extensive in n and N, the average quantities to
be Size-Intensive in both, and, as discussed below, Size-Consistent in M˜ .
Size-Consistency. The notion of Size-Consistency is a notion commonly used Quantum Chemistry to describe interacting, correlated systems. It is often introduced through the Linked Cluster
Theorem [49, 98], which states that the average energies and/or free energies (F¯) of, in particular,
a correlated system scale with M˜ , the number of independent interacting components:
−βF¯ = ⟨lnZ⟩
ξ
n =
M˜
∑
µ=1
(Connected Components)
=
M˜
∑
µ=1
Cumulants(µ)+higher order terms (72)
For SETOL, below, these connected components will be the matrix-generalized cumulants from
RMT. In Quantum Chemistry, this is analogous to ensuring the renormalized Self-Energy and/or
Effective Hamiltonian is Size-Consistent. For NNs, Size-Consistency appears when scaling the
26When dealing with matrix integrals (below), F ∼ nNMF0+⋯, when there are n×N ×M degrees of freedom [96].
27Although we do not enforce this explicitly here, we note that the so-called “Chinchilla Scaling Law” demonstrates an explicit compute-optimal frontier [97], which we can expect practitioners will likely adhere to.
46
number of feature vectors N in our matrix model, and it ensures that our layer Free Energy, etc.,
remain well-behaved as we increase M˜ . For a simple example, see Appendix A.2 where we derive
the expression for the matrix-generalized Annealed Hamiltonian Han. Both Size-Extensivity (in
n and N) and Size-Consistency (in M˜ ) are crucial in our SETOL analysis: they justify taking the
Large-N approximation for matrix integrals, and they ensure our resulting HCIZ integral–a sum
of integrated RMT cumulants (below)– scales with the dimension of the Effective Correlation
Space (ECS), M˜ .
HCIZ Integrals To generalize the Linear ST Perceptron (in the AA, and at high-T) from
Perceptron vectors to MLP matrices, we need to generalize the thermal average over the mdimensional Perceptron weight vectors (w = s) to an integral over NN Student N × M weight
matrices (W = S). The resulting Partition Function and Free Energy will be expressed with what
is called an HCIZ integral. (See Tanaka [83, 84], Gallucio et al. [40] and/or Parisi [96], and also
Eqn. 129 in Section 5.) As in Eqn. 14, we will define a Layer Quality Generating Function, βΓ
IZ
Q¯2
,
for the Layer Quality (squared). This will take the form of an HCIZ integral,
βΓ
IZ
Q¯2,N≫1
∶= lim
N≫1
1
N
ln∫ dµ(A) exp[nβN Tr[ANXN ]]
´ ¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¸ ¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¶
HCIZ Integral
≈ nβ Tr[GA(λ)], (73)
that is evaluated in the Wide Layer Large-N limit in N (here, meaning as N ≫ 1, but not
at the limit N = ∞). Here, AN and XN are N × N Hermetian matrices, (See Appendix A.1
for notation,) dµ(A) is a measure over all random (Orthogonal) matrices (see Eqn. 256), and
GA() is called a Norm Generating Function (which are different from Γn, above, and will be
made clear in Section 5.4 ), defined in Eqn. 132 in Section 5. In applying this, we will actually
write Tr[AX] =
1
N
Tr[AWW⊺
] =
1
N
Tr[W⊺AW], where W is an N ×M layer weight matrix, and
A = AN ∶=
1
n
SS⊺
is an (Outer) layer Correlation matrix, and here, X = XN =
1
NWW⊺
). Moreover,
to evaluate this, we will need to restrict A (and X) to the lower-rank Effective Correlation Space
(ECS), i.e., A → A˜ . (X is already restricted to the span of X˜ by ECS assumption.)
Notice that this looks similar to a Saddle Point Approximation (SPA), but where the more
complicated function GA() now appears. Also, GA() here depends only on the limiting form of
the ESD of A, ρ
∞
A
(λ), and depends on the M normalized eigenvalues λi of X.
To evaluate this, we will form the Large-N limit in N, using a result from Tanaka [83, 84],
but slightly extended (in Appendix A.6) to include the term nβ explicitly. We can write
βΓ
IZ
Q¯2,N≫1
∶= lim
N≫1
βΓ
IZ
Q¯2
, (74)
with the final result
βΓ
IZ
Q¯2,N≫1
= nβ
M˜
∑
i=1
∫
λ˜
i
λ˜min
dzR(z), (75)
where R(z) is a complex function, the R-transform of the ESD ρ(λ) of the Teacher, and λ˜
i are
the eigenvalues of Teacher Correlation Matrix X˜ , restricted to the ECS. For more details, see
Section 5 and Appendices A.4 and A.6.
Branch Cuts and Phase Behavior. Free energies (F,Γ) often exhibit branch cuts when
expressed as analytic functions of complex parameters (i.e., temperature, coupling constants, or
eigenvalue cutoffs), and arise from singularities in the underlying integral representations of the
47
partition function Zn, When a branch cut occurs, this demarcates non-analytic behavior and
this indicates the onset of a phase transition where macroscopic observables such as correlation
lengths and/or variance-like quantities may diverge or change character abruptly.
In the context of our HCIZ-based construction, integrating a complex function like the R(z)-
transform of a Heavy-Tailed ESD can produce precisely this phenomenon. For example, if R(z)
has a square-root term, i.e., (
√
z − c), then it will have a branch cut at z = c, and the Generating
Function (i.e., negative Effective Free Energy) βΓ
IZ
Q¯2,N≫1
will be non-analytic and we must choose
the appropriate, physically meaningful branch, i.e., (R[z] > λ˜min), corresponding to the ECS
(where R[z] is the Real-part of z) We argue that this cut signifies a phase boundary—an abrupt
change in the system’s correlation structure and corresponds to an emerging singularity in the
Layer Quality. Even though we perform only a single exact RG step, (rather than fully iterating a
renormalization flow), the appearance of a branch cut in βΓ
IZ
Q¯2,N≫1
will encode nontrivial phaselike behavior in the SETOL Heavy-Tailed matrix model.
4.3 Student-Teacher Perceptron
In this subsection, we present a unified view of the Student-Teacher (ST) Perceptron model
from both a practical and a theoretical perspective. The common sense modern understanding
of Student-Teacher learning typically involves knowledge distillation: a larger, pretrained neural
network (the Teacher) is used to train a smaller network (the Student), which learns to imitate the
Teacher’s outputs y
T
µ
. In practical terms, this often helps the smaller Student achieve comparable
performance more efficiently [99]. And it remains an open question as to why this works so
well. In contrast, classical theoretical models from the Statistical Mechanics of Generalization
(SMOG) interpret the Student-Teacher relationship differently. Here, the Teacher is an idealized
theoretical model T, a Perceptron (vector) t, that explicitly generates (binary) labels y
T
µ
from
some idealized data ξ, and the Students S are Perceptrons (vectors) of the same size as the
Teacher, ∥s∥ = ∥t∥, selected from a Boltzmann distribution. The ST theory seeks simple analytic
derivations of the generalization errors E¯ST
gen under different simplifying assumptions (AA, high-T,
linear activations, etc.), in order to understand phenomena such as what is now called Double
Descent[100], NN storage capacity, learning with noisy labels, memorization vs. generalization,
and other fundamental learning phenomena[18, 101, 4, 102, 7, 8]. A key result is that E¯ST
gen
is a simple analytic function of the ST Overlap, R = s
⊺
t. Our SETOL approach is motivated
by both these perspectives, and introduces a novel, Semi-Empirical twist. We seek a matrixgeneralization of the classical vector ST model. And while inspired by the ST Perceptron, rather
than an idealized theoretical Teacher T, we take the Teacher as an empirical input—a layer weight
matrix, T = W, taken from a real model–a model you might train yourself, download, or acquire
elsewhere. We aim to estimate the layer accuracy (or precision), which we call Layer Quality Q¯,
using an ensembles of similar Students, S ∼ T, i.e. N × M matrices, S, T. Q¯ is also a function
of the ST Overlap, now a matrix, R =
1
N
S
⊺T, but now also critically depends on the ESD of
T. And while our Students are the same size as the Teacher, we will find later that the optimal
Teacher is actually much smaller than the one we started out with.
Practical framing. Imagine being a real-world practitioner who has trained a large-scale neural network (NN), or downloaded it from the internet, or received it by other means — an
increasingly common scenario. We call this NN the Teacher, because it is the primary empirical
input to the theory. The term “Teacher” means that we consider this object to be the original
source of supervision – this trained model replicates the training labels y
T
µ
, often to perfection
48
(interpolation) or nearly so.28 You also hold a dataset D that approximates the distribution on
which T was trained, yet you lack a reliable yardstick for evaluating T’s outputs: perhaps D is
contaminated with test data, or T is an LLM with highly arbitrary outputs, or you just don’t
have access to test data. In any case, you can escape this bind by applying a Student–Teacher
protocol. You retrain one or more Students S1, S2, . . . on D using ordinary tools (SGD), that
resemble the Teacher T (same size, amount of regularization, etc), but now slightly varying the
training procedures (i.e., initial conditions, optimization hyperparameters, etc.)– tasking them
solely with imitating T’s predictions. Averaging over many such Students, you can obtain an
indirect but quantitative estimate of T’s performance.
Why this works. We view T as an inevitable outcome of the training pipeline: if you reran
the same recipe many times, you would likely converge on a narrow family of similar models. The
tighter that distribution, the more Precise it is, and—assuming the ground-truth concepts y
T
µ are
at least nearly realizable— the closer any single draw (your Teacher) must sit to the truth. By
comparing Student and Teacher outputs, you can approximate the Teacher’s generalization performance even without an explicit hold-out set. Notably, if the Teacher perfectly interpolates its
training data, the Student’s error directly estimates the Teacher’s true Generalization Accuracy.
Otherwise, it captures the Teacher’s Precision in reproducing its own noisy or biased labels.
An extended theory. We seek a succinct, analytic formulation of the ST Average Generalization Error, denoted E¯ST
gen. We work in the Annealed Approximation (AA) – a simplification often
valid when networks are sufficiently large and can nearly interpolate their training data. Under
this approximation, one obtains closed-form expressions for the Student-Teacher Overlap (R) and
thus for the Teacher overall error or accuracy. Take note also that we do not assume that the
labels y
T
µ are realizable by the Teacher, but, rather, take the Teacher as empirical input to the
theory. This point is both subtle – since the derivations proceed closely at times to those found
in [91, 102, 7, 8] – and also radical, because our derived quality metrics Q¯ now depend directly
on empirical properties of the Teacher. These results lay the foundation for our Semi-Empirical
approach, in which we supplement this theoretical form with empirical measurements (e.g., from
the trained weight matrices) to account for real-world correlations in the data and the model’s
internal structure. As far as we know, this is completely new Semi-Empirical approach
for modeling NNs.
4.3.1 Operational Setup. In subsection 4.3.1 we explain how to set up our formulationStudentTeacher model in an operational manner. In particular, we emphasize the difference between
true accuracy (vs. ground-truth labels) and Precision (vs. the Teacher’s own labels). We
also the discuss the difference between our Quality metrics and the Generalization Gap.
4.3.2 Theoretical Student-Teacher Average Generalization Error (E¯ST
gen). In subsection 4.3.2 we outline how to derive E¯ST
gen using the AA. We introduce the key expressions
that will serve as the starting point for our extended Semi-Empirical theory in subsequent
subsubsection.
4.3.1 Operational Setup
Here, describe the basic setup of the classic Student-Teacher model, taking an operational view
from the perspective of a practitioner training real-world Student and Teacher models. Specifically, we present the Annealed Approximation (AA) in a practical light, and use it to explain the
28If nearly so, then we consider this label noise to be the contribution of the Teacher, which SETOL can characterize.
49
ξ1
ξ2
ξ3
ξ4
y
T
µ
y
S
µ
S1
S2
S3
S4
T1
T2
T3
T4
Figure 8: Pictorial representation Student and Teacher Perceptrons.
difference between computing the Empirical Generalization Error, E¯emp
gen , for the True Accuracy
and for the Precision of a Teacher model. Table 6 summarizes some of the key notation we will
be using in this section, and also the next two sections for reference.
Symbol Meaning
T Teacher model (fixed network)
S Student model (trained to mimic T)
D True dataset of (xµ, y
true
µ
) pairs
D Gaussian-field idealization of D
E¯emp
gen Average empirical test-set error estimate
E¯T
gen Average, true generalization error of T (unknown)
E¯ST
gen Student–Teacher average generalization error
s, t Student and Teacher weight vectors
y
S
µ
, y
T
µ Student and Teacher labels (+1∣ − 1) for example µ
R Student-Teacher Overlap R = s
⊺
t
ϵ(R) Annealed Error Potential (i.e., 1 − R)
η(s, t) Self-Overlap (i.e., R)
Q¯ST Student-Teacher Perceptron Quality
Table 6: Key notation for our formulation of the Student–Teacher model.
Test Error of the Teacher. We start by describing how to obtain a simple formal expression
for the empirical test errors of the Teacher T. Let us say we have a model, called Teacher (T),
which maps some actual (i.e., correlated, meaning the features are not i.i.d.,) data xµ ∈ D to some
known or true labels (y
true
µ
) (where, y
true
µ
is, say, an N-dimensional vector of binary labels). We
might say that y
true
µ
represents the Ground Truth for the problem. Operationally, we train the
Teacher T to reproduce or at least approximate the true labels y
true
µ
.
T ∶ xµ → y
T
µ ≈ y
true
µ
. (76)
If T reproduces the true labels exactly, we might say that the Teacher has been trained to
Interpolation, and therefore, y
T
µ = y
true
µ
. Indeed, most models today are trained to Interpolation,
and we don’t need to necessarily worry about the difference between the true and the predicted
50
Teacher labels. Formally, however, and to better understand the AA, it is beneficial to discuss
the implications of this distinction.
Following Eqn. 1, let’s say the Teacher outputs are specified by the NN output (or inference
energy) function E
out
NN
29
y
T
µ = E
out
NN (T,xµ) (77)
so that we may write the Empirical Average Training Error E¯emp
train as
E¯emp
train ∶=
1
Ntrain
Ntrain
∑
µ=1
L[y
train
µ
, Eout
NN (T,x
train
µ
)]. (78)
Ideally, we seek the True Average Generalization Error of the Teacher, denoted E¯T
gen. Of course,
this is unknowable, but in practice, we estimate E¯T
gen by measuring the error of the Teacher
predictions on some test (or hold-out) set (x
test
µ
, y
test). We call this the Empirical Average Generalization Error, E¯emp
gen , and write
E¯T
gen ≈ E¯emp
gen ∶=
1
Ntest
Ntest
∑
µ=1
L[y
test
µ
, Eout
NN (T,x
test
µ
)]. (79)
To measure the error, the loss function L may be an L1 (ℓ1) or L2 (ℓ2) loss; whereas for training
a NN model, it is usually something like a cross-entropy loss. 30
If we don’t have a hold-out set, however, we can still estimate E¯T
gen using the Student-Teacher
approach.
Estimating the Student-Teacher Error: Accuracy vs. Precision. Imagine we train a
Student model S (with the same architecture as the Teacher T) on the real-world dataset D,
using T’s outputs as targets:
S ∶ D → y
S
µ ≈ y
T
µ
,
where y
T
µ = E
out
NN (T, xµ) are the Teacher’s predicted labels or energies.
Value
Accuracy
Teacher output y
T
µ Ground truth y
true
µ
Student outputs y
S
µ
Precision
Bullseye Example
Figure 9: Precision vs. Accuracy
Figure 9 illustrates the following two scenarios side by side:
29We refer to outputs of E
out
NN (t,xµ), when applied to a data point xµ, as energies because they are effectively
un-normalized probabilities for the class outputs (for labels yµ = 1 or −1).
30Note that E¯T
gen will be treated as an energy function, as is E
out
NN , yet they are related nonlinearly. This is
because they are energies in two different systems – training, and inference – and L is a transfer function that
bridges them.
51
• Accuracy (Interpolation) regime: If T perfectly interpolates the true labels (y
T
µ = y
true
µ
),
then ∥y
S
µ − y
T
µ
∥ = ∥y
S
µ − y
true
µ
∥ measures how well the student recovers the Ground Truth
labels—i.e., the Student’s training accuracy.
• Precision (Noisy teacher) regime: If T itself makes mistakes (y
T
µ ≠ y
true
µ
), then ∥y
S
µ −y
T
µ
∥
estimates how faithfully S reproduces T (its precision), rather than true accuracy.
By analyzing the Student–Teacher overlap R = ∑µ s
⊺
t, one can show that the average generalization error of T depends simply on 1 − R under the annealed approximation—-even when
T was trained on noisy labels. In practice, we exploit this fact by training a large ensemble of
students to estimate R and hence recover the teacher’s true error.
The Student-Teacher model also explains why NNs can generalize even when trained to Interpolation on noisy data (which has been a source of confusion [103]). In this model, the
Generalization Error E¯T
gen is a simple function of the overlap R between the Teacher T and the
Students S, i.e., E¯T
gen ∼ ⟨1 − ϵ(R)⟩β
s
. So even if the Teacher T is trained on noisy data, as long
as there are Students S with significant overlap R with the Teacher, the Teacher Generalization
Error E¯T
gen can be considerably small. For more details, see [104].
Learning the Student. Moving forward, we will always assume the Teacher is trained to
Interpolation because this actually corresponds to the Annealed Approximation, whereas if the
Teacher makes errors, we may need to consider Quenched averages, explained below.
Imagine now that to estimate the empirical Average Generalization Error, E¯emp
gen , by training
a very large number of Students, and computing the average ST error on some test set. Let us
break the data set into training x
train
µ and test x
test
µ
examples, train models on the training data
(that is, find the optimal model weights), and evaluate the S and T models on the test data.
The Student learning task can be written as in Eqn. (2) as the following optimization problem
over the training data:
argmin
{S′}
Ntrain
∑
µ=1
L[E
out
NN (S
′
,x
train
µ
), Eout
NN (T,x
train
µ
)], (80)
If the Teacher is trained to Interpolation, then the NN optimization problem training a Student
to reproduce the Ground Truth labels, so that y
S
µ ∼ y
true
µ
for both the training and test sets.
argmin
{S′}
Ntrain
∑
µ=1
L[E
out
NN (S
′
,x
train
µ
), y
true
µ
], (81)
If not, then the Student is reproducing the possibly incorrect Teacher labels, and, importantly,
the Student S now depends explicitly on how the Teacher was trained. That is, we should denote
that the learned Student explicitly depending on T, i.e. S → S[T]. This will be important below.
The Thermal Average. In StatMech, the average over Student weights ⟨⋯⟩s is a Thermal
Average ⟨⋯⟩
β
s
. This means we should restrict the choice of S to the same Boltzmann distribution
we assume T is drawn from. In a practical sense, we would expect our Students S to be trained
with the roughly same amount of regularization (same weight decay, etc.). In principle, we should
be able to admit many kinds of S, including those arising from different optimization paths which
lead to the same equilibrium sample. 31 While the parameter β is like a fixed regularization set31See [105], which shows that for NNs trained with SGD, nearly all learning rates, if they are not too large,
converge to the same minima given enough time. Additionally, [106] shows that one can effectively train with
any batch size and learning rate—as long as you schedule them correctly (i.e., growing them)—and still guarantee
convergence to the same solution.
52
ting, it actually modulates the trade-off between energy and entropy— sometimes called entropic
regularization—and therefore how strongly the data constrains the model, and not specifically
how the model is trained. Moreover, in the high-T (small-β) limit the measure becomes broader,
so we can admit more Student weights that fluctuate around the mean. In practice, however, we
find that different optimizer settings (batch sizes, learning rates, stopping criteria, etc.) can give
very different results. Therefore, we must be careful not to admit any arbitrary Student into the
average because this would amount to averaging over several Boltzmann distributions and this
should be treated explicitly.
The Average Generalization Error. We may seek to estimate the Empirical Average Generalization Error by replacing the test predictions in Eqn. 79 with the student predictions y
test
µ → y
S
µ
,
and then averaging directly over the test data x
test
µ
for all possible or available test examples.
If we have a very large number of suitable Students (say, drawn from some random distribution), then we can try to estimate the Average Generalization Error of the Teacher, i.e.,
E
T
gen ≈ E¯emp
gen . E¯emp
gen is given by an average loss, the average is over all possible Students NS, and
then over all Ntest test data points x
test
µ
∈ D
E¯emp
gen =
1
Ntest
Ntest
∑
µ=1
1
NS
∑
S
L[E
out
NN (S,x
test
µ
), Eout
NN (T,x
test
µ
)] (82)
=
1
NS
∑
S
1
Ntest
Ntest
∑
µ=1
L[E
out
NN (S,x
test
µ
), Eout
NN (T,x
test
µ
)],
where (ideally) Ntest is extremely large.
In Bra-Ket notation, we may also write
E¯emp
gen = ⟨⟨E
n
L(S, T,x)⟩S⟩xtest (83)
= ⟨⟨E
n
L(S, T,x)⟩xtest ⟩S
where E
n
L
(S, T,x) ∶= L[E
out
NN (S,x
test
µ
), Eout
NN (T,x
test
µ
)]. For the empirical estimate, it does not
matter what order we take the sums in, but we are not estimating the the True Average Generalization Error of the Teacher, E¯T
gen, unless T is trained to Interpolation. For the theoretical
estimate, however, the order can be important, and this also depends on if T is trained to Interpolation or not. 32
Annealed vs. Quenched Averages. Recall that in the StatMech approach to computing
errors, we do not break the data into training and test, but, instead, to obtain the Average
Generalization Error, E¯
gen, use the Generating Function approach. In doing this, we need to
compute both the Thermal Average over the model weights (S, T), and also take the data average
over the entire available idealized data set D. And the order can matter.
In the case where the Teacher is trained to Interpolation, may train the Student independently
of when the Teacher. But if Teacher is not trained to Interpolation, then formally we must train
the Teacher first to obtain target predictions for the Student. That is, the Student formally
depends on the Teacher, S[T]. The empirical errors in T would then formally depend on the
specific instantiation of the data ξ
n
∈ D, and therefore, conceptually imagine that we must first
average over the data before averaging over the weights. Training to Interpolation corresponds
32This approach can be likened to the Bootstrap method [107] used for error estimation. However, the Bootstrap
method predominantly emphasizes variations in the input data x
n
∈ D, while in this context, we are essentially
bootstrapping over the students S.
53
conceptually to working with a model in the Annealed Approximation, whereas not doing so
corresponds to Quenched case.
Practically, when the Teacher is not trained to Interpolation, we may need to resample the
training data and training an ensemble of models to compensate for anomalies in training data
(bad labels, noise, etc.) that may cause the underlying model to overfit to the training data.
Theoretically, within SMOG, this is equivalent to quenching the system to the data (a term analogous to quickly cooling a physical system, frezing in any defects). In contrast, when one anneals
a physical system, one heats up and cools it down slowly, and repeatedly, thereby removing any
defects (of data anomalies for NNs, or material defects in a physical system).
In StatMech, one can perform a so-called quenched average using a replica calculation, effectively removing the dependence on test and/or training data from the final estimate for E¯emp
gen .
However, the theoretical quenched result may differ significantly from the annealed case when the
underlying model is unrealizable [8]. This may occur when the training data is very noisy and/or
the model architecture is such that it can not correctly predict all the training labels. In such
cases, the model will always have some finite, non-zero Average Training Error, E¯
train > 0, even
in the Large-N limit of infinite data n = ∞. In such a case, this indicates a highly complex error
landscape with many local minima separated by extremely high barriers, and a slowing down of
the dynamics.33
While it is commonplace to train ensembles and/or use cross-validation when training small
models (as the above discussion assumes), this could be extremely expensive and impractical in
modern ML, e.g., for very large models like Large Language Models (LLMs). For such massive
NNs, one needs a theory that can detect anomalies in training directly from observations during
and/or after training. This is a hallmark of the SETOL approach, and it distinguishes SETOL from
the classic StatMech approach.
Generalization Gap vs. Model Quality. We should distinguish between what is typically
done in the SLT literature versus the StatMech approaches. In SLT, one is typically interested in
modeling the Generalization Gap. The Generalization Gap quantifies the difference between a
models performance on training data versus unseen test data:
E
emp
gap ∶= Etrain[x
train] − Egen[x
test] (84)
In contrast, in StatMech approaches, one considers the Model Generalization Accuracy directly,
which is sometimes called the Model Quality in the SLT literature. Model quality is an indication
of the models accuracy, precision, recall, or any other relevant metric based on the task at hand.
While related, in developing an analytic theory, the Generalization Gap and the Model Quality (or Model Generalization Accuracy) require conceptually different approaches. This is because
the Generalization Gap depends on a specific realization of the training data, whereas our Model
Generalization Accuracy will be formulated on a random training data set (and then corrected
later with empirical data). In this sense, any theory of the Generalization Gap requires a formalism where the predicted model error is Quenched to the training data, which is not what we
want. In contrast, the Model Generalization Accuracy will be formulated using the Annealed
Approximation (AA), and is therefore both conceptually and mathematically simpler. Notably,
the HTSR model Quality metric trend well when compared to the Model Generalization Accuracy,
whereas methods from the SLT literature tend to only describe the Generalization Gap well.[28]
33In modern ML parlance, one might say the model can not be evaluated at interpolation, although in practice
such a model might have a zero empirical Training Error since it may overfit the specific training data.
54
What about Grokking? There could be a scenario where the Teacher T is trained to intepolation, achieving perfect training accuracy, and, yet, the test accuracy remains low; This scenario
happens in Grokking, in both the pre-Grokking and anti-Grokking phases, and has been analyzed
using the HTSR theory.[108] In the pre-Grokking case, the Teacher has not properly converged,
and has one or more layer α ≫ 2, whereas in the anti-Grokking phase, the layers have may
be overfit or overcorrecting, with α ≪ 2. In either case, we might argue that Teacher has not
achieved Thermodynamic equilibrium, and a naive ST protocol may fail! This and other complex
situations motivate why it is necessary to have a robust theory that can characterize the internal
states of the NN layers, and without needing access to test or training data.
4.3.2 Theoretical Student-Teacher Average Generalization Error (E¯ST
gen)
Here, we seek a simple, formal expression for the Student-Teacher Average Generalization Error,
E¯ST
gen, that can be used as the starting point for our extended Semi-Empirical theory.
The Idealized Data. To develop a Semi-Empirical theory of the Teacher Generalization Error,
E
T
gen, instead of training and evaluating a NN model using real data (x), we seek a simple,
analytical expression with parameters that can be fit to empirical measurements. So in addition
to using a model for our NN, we must specify a idealized model for the data. In a real NN, the
data x is correlated, and, in fact, very strongly correlated; and this is reflected in the layer weight
matrices. However, to be tractable, our starting theoretical expressions use uncorrelated (i.i.d)
data. Formally, we must replace the correlated data with some uncorrelated, random model of
the data, i.e., x → ξ. As described in Figure 7, our Data Model is a standard Gaussian N(0, σ2
I)
model for the input data
xµ → ξµ, ξµ ∈ N(0, σ
2
Im), (85)
where N(0, σ2
Im) denotes a Gaussian distribution with zero mean and variance σ
2 =
1
m
, ξµ is
normalized such that ∥ξµ∥
2
∶= ∑
m
i=1
(ξµ)
2
i
= 1 for all n data vectors. The full idealized sample is
denoted ξ
n
, and is an n × m matrix, or, equivalently, a n-dimensional vector of m-dimensional
vectors ξµ.
We make this distinction between actual D and idealized data D to emphasize that, later, we
will use our so-called Semi-Empirical procedure to account for the real correlations in the actual
data phenomenologically by taking some analytical parameter of the theory and fitting it to the
real world observations, here, on the ESD of the NN weight matrices.
The ST Error Model and the Annealed Potential ϵ(s, t). We now model Teacher error
E¯T
gen with the Average ST Generalization Error E¯ST
gen, which is obtained by first computing the
ST error function E
n
L
(s, t, ξ
n
) over the set of all possible n input examples ξ. Define the datadependent ST test error function–or Energy–as
E
n
L(s, t, ξ
n
) ∶=
n
∑
µ=1
L[E
out
NN (s, ξµ), Eout
NN (t, ξµ)]. (86)
where L(s, t, ξµ) is simply the ℓ2 loss. This measures the error between the Student and the
Teacher; it is zero when their predictions are identical, (y
S
µ = y
T
µ
), and is nonzero otherwise.
We aim to derive a simple expression for the Average ST Generalization Error, E¯ST
gen, and to
do this, we define the Annealed Error Potential for the data-averaged ST Generalization Error
ϵ(s, t), as in Eqn. 30, as:
ϵ(s, t) = ⟨E
n
L(s, t, ξ
n
)⟩
ξ
n ∶=
1
n
∫ dµ(ξ
n
)E
n
L(s, t, ξ
n
) (87)
55
The measure dµ(ξ
n
) will end up being a Gaussian measure over n samples (see Appendix A.2),
and the intent is to evaluate it in the Large-N limit in n, thereby sampling all possible inputs in
the idealized data space, ξ
n
∈ D.
As in Section 4.2, by applying the AA, we can rewrite the Average ST Generalization Error,
E¯ST
gen: first, a simple average over all the possible inputs ξ
n
; and, second, then as a Thermal
average over all Students S, in the AA, and at high-T
E¯ST
gen ∶= ⟨ϵ(s, t)⟩β
s
. (88)
(Recall that in this regime, E¯ST
gen = E¯an,hT
gen .)
In the classic StatMech approach, the average ⟨⋯⟩
β
s
is a Thermal Average in the canonical
ensemble with β fixed, as explained in Section 4.2. Here, we will do something similar, as the
Student average ⟨⋯⟩
β
s will be computed from the associated generating function βΓ
IZ
Q¯2
for the
matrix-generalized case (an HCIZ integral defined over all students, and in both the Large-N
Thermodynamic limit in n, and the Wide Layer Large-N limit in N).
Recall that above, the empirical estimate for E¯emp
gen depended on a specific instantiation of the
model for the training data x
train
µ
, i.e E¯ST
gen is Quenched to the training data. For that reason,
for the final result, we needed to take a second, quenched average over all possible data sets.
Here, we do not need to consider this and always work in the Annealed Approximation (AA).
This is because we incorporate the specific effects of the real-world training data (x
n
) after
we derive our formal expressions by fitting the model parameters to empirical data. The final
expression for E¯ST
gen, derived below, will be generalized to E¯NN
gen , matrix-generalization of the classic
StatMech formula for the Linear Perceptron, in the Annealed and High-T approximations. (see
Appendix A.2).
The Annealed Potential as a function of the overlap (ϵ(R)). We want an expression
for the data average of the ST test error, from Eqn. 87, generalized from Perceptron vectors to
NN layer weight matrices. For the Perceptron, one obtains different expressions for the ST error
function, depending on the type of activation function h(x) in Eqn. 1; The simplest are the Linear
and Boolean Perceptrons, and for both (and with ℓ2 loss), ϵ(s, t) is simply a function of the ST
overlap R [8]. This gives ϵ(s, t) → ϵ(R), where
R = s
⊺
t =
m
∑
i=1
siti
, R ∈ [0, 1]. (89)
which is simply the dot product between the m-dimensional Student s and Teacher t weight
vectors. Notice that the data vectors are normalized such that ∥s∥
2 = ∥t∥
2 = 1 and we assume
there are no inversely-correlated Students–Students that predict more than half the Teacher labels
incorrectly. And, importantly, the number of free parameters becomes 1.
For a Linear Perceptron [8],34 with activation function h(x) = x, we can obtain the error
function as
ϵ(R) = 1 − R, R ∈ [0, 1]. (90)
34In the classic approach for the ST model, the theory examined different expressions ϵ(R). For example, one can
consider the Boolean Perceptron [8, 109], with activation function h(x) = sgn(x), i.e., the Heaviside step function.
Then, the error is ϵ(R) = 1 −
1
π
arccos(R). In both cases, perfect learning occurs when R = 1 [8].
56
s
t
θ
R = s
⊺
t
S
T
θ
R =
1
N
Tr(S
⊺T)
Figure 10: Comparison of 2D and 3D representations of the vector and matrix Student–Teacher
overlap R. Left: R = s
⊺
t (and normalized s.t. ∥s∥
2 = ∥t∥
2 = 1). Right: R =
1
N
Tr(S
⊺T) with
conic sections on the sphere (red S, blue T), plus a purple wedge for the angle. Averaged over
matrix dimension N (and implicitly normalized over to 1/M). Note that R ∈ [0, 1], where R = 1
indicates Student perfect learning (and R ≥ 0 because there no Students that predict more than
half the Teacher labels incorrectly).
Derivation of the ST error (ϵ(R)) for the Linear Perceptron. To derive Eqn. 90, define
the data-dependent ST error (Eqn. 86) in terms of an ℓ2 loss function.
Write the data-averaged ST error as
E
n
ℓ2
(s, t, ξ
n
) =
1
2
n
∑
µ=1
(y
S
µ − y
T
µ
)
⊺
(y
S
µ − y
T
µ
) (91)
Define the n label vectors y
S
∶= [y
S
µ=1
, y
S
µ=2
,⋯, y
S
µ=n
] and y
T
∶= [y
T
µ=1
, y
T
µ=2
,⋯, y
T
µ=n
]. This lets us
write the total Energy as
E
n
ℓ2
(s, t, ξ
n
) =
1
2
Tr [(y
S − y
T
)
⊺
(y
S − y
T
)]
=
1
2
Tr [(y
S
)
⊺y
S − 2(y
S
)
⊺y
T + (y
T
)
⊺y
T
]
=n − Tr [(y
S
)
⊺
(y
T
)]
=n − Tr[η(s, t, ξ
n
)], (92)
where we define the data-dependent Self-Overlap:
η(s, t, ξ
n
) ∶= (y
S
)
⊺
(y
T
) (93)
The expression η(s, t, ξ
n
) is analogous to the ST overlap R, but before the data has been
integrated out. It is convenient to work directly with the Self-Overlap η(s, t, ξ
n
) because it
will appear later in Eqn. 107 (in Section 5), when formulating the matrix-generalized overlap
operator R.
In defining η(s, t, ξ
n
), we replace the individual labels (y
S
µ
, y
T
µ
) with the Energy functions
E
out
NN that generate them, giving an expression in terms of the weights (s, t) and the Gaussian
data variables (ξ). We will then integrate out the data variables, leaving an expression just in
terms of the weights. Using the E
out
NN Energy generating or output function (Eqn. 1, 4.3.1, 77),
we can write the labels as
y
S
µ = s
⊺
ξµ, y
T
µ = t
⊺
ξµ. (94)
57
This now gives the data-dependent Self-Overlap explicitly as
η(s, t, ξ
n
) =
n
∑
µ=1
(s
⊺
ξµ)
⊺
(t
⊺
ξµ) =
n
∑
µ=1
ξ
⊺
µ
(s
⊺
t)ξµ (95)
After integrating over the data, we have the data-independent Self-Overlap, η(s, t):
η(s, t) ∶= ⟨η(s, t, ξ
n
)⟩
ξ
n =
1
n
∫ dµ(ξ
n
)η(s, t, ξ
n
)
=
1
n
∫ dµ(ξ
n
)
n
∑
µ=1
ξ
⊺
µ
s
⊺
tξµ
=
1
n
∫ dµ(ξ
n
)
n
∑
µ=1
ξ
⊺
µRξµ
=R
1
n
n
∑
µ=1
∫ dµ(ξµ)ξ
⊺
µξµ
=R. (96)
where the third equality holds because R is a scalar constant, and the fourth holds because
the elements of ξ are i.i.d. and normalized to unit variance. (See Section A.2.1) .
We can now obtain the Annealed Error Potential ϵ(R) for the data-averaged ST test error
(see Eqn. 30), as in Eqn. 90,
ϵ(R) = ⟨E
n
ℓ2
(s, t, ξ
n
)⟩
ξ
n = 1 − R. (97)
In traditional StatMech (e.g., [8]), one is interested in how the Total Generalization Error
Egen(R) depends on R. With these simple error functions, Eqn. 88 reduces to a function over R,
and the Average ST Generalization Error E
ST
gen(R) is then obtained by taking a Thermal Average
over the Students
E¯ST
gen(R) = ⟨ϵ(R)⟩β
s
= ⟨1 − ⟨η(s, t, ξ
n
)⟩
ξ
n ⟩
β
s
= ⟨1 − η(s, t)⟩β
s
= ⟨1 − s
⊺
t⟩
β
s
= ⟨(1 − R)⟩β
s
, (98)
where ⟨⋯⟩
β
s
is a Thermal Average over the Student weight vector s. Recall that even at high-T,
this restricts the Students to ones trained with a similar amount of regularization as the Teacher
(i.e. the measure dµ(s) = ds is independent of β).
The average Quality for the ST Perceptron, Q¯ST , is just the Average Generalization Accuracy,
so we can write
Q¯ST ∶= 1 − E¯ST
gen(R) = ⟨1 − ϵ(R)⟩β
s
= ⟨⟨η(s, t, ξ
n
)⟩
ξ
n ⟩
β
s
= ⟨η(s, t)⟩β
s
= ⟨s
⊺
t⟩
β
s
= ⟨R⟩
β
s
. (99)
Eqn. 99 is the starting point for deriving a SemiEmpirical theory for the WeightWatcher quality
metrics (Alpha,AlphaHat); see Section 5.1. To generalize this expression, we will start with the
Self-Overlap η(S, T, ξ
n
) for a Multi-Layer Perceptron (MLP3) in Section 5.
Before doing this, however, we note that we can obtain this expression for E¯ST
gen by defining
the Annealed Hamiltonian Han
hT (R), at high-Temperature, as in Section 4.2, Eqn. 49. Indeed, it
is really Han
hT (R) = ϵ(R) that we must generalize to the matrix case in the next section, which we
do (using a technique similar to a Replica calculation, but still in the AA).
58
Quantity Traditional SMOG
Linear Perceptron
in Traditional SMOG
Matrix Generalization
for SETOL
Total (Idealized) Data Error E
n
L
(w, ξ
n
) (31) E
n
L
(s, t, ξ
n
) (91) E
n
ℓ2
(S, T, ξ
n
) (102)
Annealed Hamiltonian Han
hT = ϵ(w) (30) Han
hT (R) = ϵ(s, t) = 1 − R (97) Han
hT (R) = N(IM − R) (206)
(Data-Averaged Error) (AA, at high-T) (and at Large-N ) (only for a layer)
Self-Overlap η(w) = 1 − ϵ(w) (32) η(s, t) = s
⊺
t (96) η(S, T) =
1
N
S
⊺T (107)
Model Quality Q¯ ∶= 1 − E¯
gen Q¯ST ∶= 1 − E¯ST
gen (60) Q¯NN ∶= 1 − E¯NN
gen (60)
in terms of Layer Quality Q¯NN ∶= ∏L Q¯NN
L
Table 7: Summary of key quantities compared across traditional SMOG models, the StudentTeacher (ST) Linear Perceptron–in the Annealed Approximation (AA) and at high-Temperature
(high-T) and at Large-N in n, and the matrix-generalized forms as the starting point to frame
SETOL. The total ST Error of Energy, E
n
L
, represents the difference (squared) between the model
and its labels for the ST model between the Student and Teacher predictions. The Annealed
Hamiltonian is the Energy function for this Error after it is averaged over the model for the
training data (an n-dimensional i.i.d. idealized Gaussian dataset, ξ
n
). In the AA, the Annealed
Hamiltonian is equal to the Annealed Error Potential. For the ST model, this is one minus the
average overlap, Han
hT (R) = (1 − R); for the SETOL, this is the (M-dimensional) identity minus
the overlap operator/matrix, Han
hT (R) = N(IM − R). The Self-Overlap η(⋯) is used to describe
the Accuracy (as opposed to the Error) for both the ST model and its matrix-generalized form.
Finally, the different forms of the Quality are defined. Generally speaking, the Quality Q¯ is an
approximation to some measure of 1 minus the Average Generalization Error, Q¯ ∶= 1 − E¯
gen (in
the AA, at high-T, at Large-N , and with whatever else approximations are applied). For the ST
model, having just 1 layer, the Model Quality and the Layer Quality are the same, and denoted
Q¯ST . For SETOL, the Model Quality Q¯NN is a product of individual Layer Qualities Q¯NN
L
. (Note
that the final SETOL Layer Quality Q¯ is defined in terms of the Layer Quality-Squared Q¯2
, and
the starting point for this is expressed with the Layer Quality-Squared Hamiltonian HQ¯2 = R⊺R.
Final Remarks. Additional results are provided in Appendix A.2. In particular, In Appendix A.2.1, we use the ideas from this section and the previous one to derive the full non-linear
vector form of the Annealed Hamiltonian Han(w) (Eqn. 43) for the Linear Perceptron, in the AA.
Then, in Appendix A.2.1, we derive the matrix generalization Han(R) → Han(R) of these quantities using a Large-N in n expansion (as opposed to a full replica calculation). This derivation
explains how to obtain the implicit 1/M normalization for the weight matrices W that is necessary for the final results in Section 5. (Notice this is similar to the implicit normalization 1/m
on the ST vectors) Then, taking the case N = 1, we can recover the original result for Han(R).
We can then express the Annealed Hamiltonian at High-T, i.e., Han
hT (R) = ϵ(R) (Eqn. 49). This
shows that our new derivation is consistent with the full and the High-T results for classic ST
Perceptron model, for a Linear Perceptron in the AA. [8]
59
5 Semi-Empirical Theory of the HTSR Phenomenology
In this section, we present the main technical elements of our Semi-Empirical Theory of Deep
Learning (SETOL). Our goal is to explain and, where possible, derive the HTSR PL metrics Alpha
(α) and AlphaHat αˆ from first principles, and, in doing so, also present the ERG condition and
newly proposed WeightWatcher DetX metric. To do this, we introduce a Matrix Generalization
of the Student-Teacher model for a Linear Perceptron (See Section 4.3.2), adapted here for a
(3-layer) Multi-Layer Perceptron (MLP3). We seek a theory for the Layer Quality Q¯ = Q¯NN
L
of
a NN, where this Layer Quality now corresponds to the (approximate) contribution each layer
makes to the total generalization accuracy, or total Quality Q¯NN . For technical reasons, we
actually seek a formal expression(s) for the Layer Quality-Squared, Q¯2 ≈ (Q¯NN
L
)
2
. We say that
the SETOL is Semi-Empirical because the final result Q¯2
is expressed directly in terms of the
empirically observable spectral properties of the Teacher layer weight matrix T = W.
5.1 Matrix Generalization of the ST Model. Section 5.1 generalizes classical StatMech
vector-based ST model of Section 4.3 to obtain a Layer Quality for a single layer in an NN.
It starts by first formulating the learning problem for the NN generalization accuracy or
quality, Q¯NN , of a 3-layer MLP (MLP3). We then replace vectors with N × M matrices
s, t → S, T, and obtain and expression for the NN Self-Overlap η(S, T, ξ
n
),which then
gives a matrix-generalized overlap operator R ∶= ⟨η(S, T, ξ
n
)⟩
ξ
n =
1
N
S
⊺T. This can be
related to a single-layer matrix-generalization of the ST Annealed Hamiltonian, presented
in Appendix A.2, Han
hT (R) ∶= N(IM − R), where, importantly, the scalar overlap R is now
a matrix R of M × M adjustable parameters. Importantly, being empirical quantities, the
weight matrices are implicitly normalized by 1/n, and, like the ST vectors, also implicitly
normalized by 1/M. (See Appendix A.2.2).
5.2 The Layer Quality-Squared Q¯2
. Section 5.2 presents the expression for NN Layer
Quality-Squared Q¯2
. Following the ST analogy, we define a Thermal Average over possible Student weight matrices S for the matrix overlap, giving Q¯NN
L
∶= ⟨Han
hT ⟩
β
S
= ⟨R⟩
β
S
.
For technical reasons, however, we actually seek the (approximate) Layer Quality-Squared,
Q¯2 ≈ (Q¯NN
L
)
2
, defined as Q¯2
∶= ⟨R⊺R⟩
β
S
. To evaluate Q¯2
, rather than sampling all random
Student matrices S directly, we switch measures to the (Outer) Student Correlation matrices35 AN =
1
N
SS⊺
along with their Inner counterparts AM =
1
N
S
⊺S. Importantly, we argue
that the measures dµ(AM) ↔ dµ(AN), can be interchanged for our purposes, making them
effectively equivalent. This reparameterization leads us to an integral of the HCIZ type (as
in Eqn. 73) which, as shown by Tanaka [83, 84], is expressed in terms of the R-transform
RAM(z), defined for the tail of the limiting form of the ESD of AM, ρ
∞
AM
(λ).
Then, we introduce the Effective Correlation Space (ECS), and two key approximations, the
Independent Fluctuation Approximation (IFA) and the Exact Renormalization Group (ERG)
Condition. We impose the IFA (described below) because it is necessary for the final result.
The ERG condition states that the determinant of the (effective) Student correlation matrix
is unity, det (A˜ ) = 1. Critically, this condition can be tested empirically by assuming the
(effective) Teacher correlation matrix also follows the testable ERG condition, det (X˜ ) = 1.
The empirically testable ERG condition is a key result of this work.
5.3 The Wide Layer Large-N limit in N. Section 5.3 presents the core result, (as in
Eqn. 15), closed-form or semi-analytic expression(s) for the Layer Quality-Squared Q¯2
35We adopt the convention “Inner” for smaller, M × M full rank Student correlation matrices, and “Outer” for
larger, N × N rank-deficient Student Correlation matrices. Note that both are scaled as 1/N, meaning that their
nonzero eigenvalues are the same.
60
formed in the Wide Layer Large-N limit in N. Restricted to the ECS, and under the
ERG condition and the IFA, our HCIZ integral for Q¯2 becomes tractable at large-N, giving
an expression that can be parameterized in terms of M˜ eigenvalues λ˜ of the Teacher correlation matrix restricted to the ECS X˜ . In doing this, the M˜ Teacher eigenvalues are treated
as experimental observables, and become the effective Semi-Empirical parameters (i.e., α,
λmax) of the SETOL approach.
5.4 Selecting the Heavy-Tailed R-transform. Section 5.4 presents several models of different R-transforms. Evaluating Q¯2
requires evaluating selecting an R-transform R(z) for the
Teacher Empirical Spectral Density (ESD), and also ensure that it is analytic and singlevalued on the domain of interest– the ECS and/or tail of the ESD. We examine four possible
models for R(z): (i) the Bulk+Spikes (BS), (ii) the Free Cauchy (FC) model, (iii) the Inverse Marchenko-Pastur (IMP) model, and (iv) the Levy-Wigner (LW) model. First, as a
trivial case, the tail of ESD can be treated as a collection of spikes, and the ESD is simply a
sum of Dirac delta functions; in this case, Q¯2 becomes a “Tail Norm”, the Frobenius Norm
of the PL tail. When the layer is Ideal, i.e., α ∼ 2 and det (X˜ ) ∼ 1, one can use Free Cauchy
(FC) model. and the resulting Layer Quality Q¯ is approximately the Spectral Norm λmax.
Since λmax increases with decreasing α, the FC model yields the HTSR Alpha metric. One
can also use Inverse Marchenko-Pastur (IMP) model. Being a model for the full Teacher
ESD (not just the tail) the IMP R-transform contains a branch cut in the complex plane
which aligns with the start of the ECS Power Law tail. Finally, Using the Levy-Wigner
(LW) model, one can model cases where α ≤ 2 and derive the HTSR AlphaHat metric.
These core elements form a bridge between well-established empirical properties of large-scale
NNs and a tractable ST-based theory. In the subsequent sections, we formalize the key steps:
(i) setting up the matrix-based ST problem, (ii) defining our HCIZ integrals over restricted
correlation matrices (ECS), and (iii) analyzing the resulting Layer Quality (or Quality-Squared)
expressions in the Large-N limit in N.
5.1 Multi-Layer Setup: MLP3
In this section, we describe the matrix generalization of the ST model of the Linear Perceptron;
and, in particular, a matrix-generalized version of the key quantities we derived in Section 4.3.2.
A simple model. Consider a simple NN with three layers (two hidden and an output), i.e., a
three-layer Multi-Layer Perceptron, denoted as the MLP3 model. (This is a very simple model
of a modern NN with hundreds of layers and complex internal structure.)
Ignoring the bias terms, Without Loss of Generality, (WLOG), the NN outputs E1µ, E2µ, E3µ
for each layer, as defined in Eqn. 1, are given by:
E
out
1µ
∶=
1
√
N1
h(W⊺
1
ξµ),
E
out
2µ
∶=
1
√
N2
h(W⊺
2E
out
1µ
),
yµ ∶= E
out
3µ =
1
√
N3
h(W⊺
3E
out
2µ
), (100)
where E
out
#µ
denotes a vector of energies or NN outputs for that layer, and h is a general function
or functional, denoting either a non-linear activation or a more complex layer structure, such as
61
a CNN or an RNN. Note that for binary classification, E
out
3µ = 1∣ − 1 is just a binary number. We
can consider h(⋅) to be an (unspecified) activation function, but here we simply take h(x) = x.
As in Eqn. 91, let us specify the ST error, or total Energy, specifically in terms of the ℓ2 or
MSE loss:
E
n
ℓ2
(S, T, ξ
n
) =
1
2
n
∑
µ=1
(y
S
µ − y
T
µ
)
2
. (101)
We now develop the matrix generalized form of the self-overlap η:
5.1.1 Data-Dependent Multi-Layer ST Self-Overlap (η(S, T))
Following the same approach in Section 4.3.2, it is convenient to rewrite the total Energy E
n
L
in
Eqn. 101 as:
E
n
ℓ2
(S, T, ξ
n
) ∶=
1
2
Tr [(y
S − y
T
)
⊺
(y
S − y
T
)] = n − Tr [(y
S
)
⊺ y
T
] = n − η([Sl
, Tl], ξ
n
) (102)
where the Self-Overlap η([Sl
, Tl], ξ
n
) is of the same form as the (vector) Linear Perceptron (in
Eqn. 91). Note that η([Sl
, Tl], ξ
n
) depends on the (idealized) data ξ
n because we have not
evaluated the expected value ⟨⋯⟩ξn yet.
Using the general expression from Eqn. 100 for the action of the NN on the input data ξ, we
can write the formal expression of the ST error for the simple MLP3 model as:
⟨η([Sl
, Tl], ξ
n
)⟩
ξ
n = ⟨η(S1,S2,S3, T1, T2, T3, ξ
n
)⟩
ξ
n
∶=
1
n
Tr [{ 1
√
N3
h (S
⊺
3
1
√
N2
h (S
⊺
2
1
√
N1
h (S
⊺
1
ξ
n
)))}⊺
×
1
√
N3
h (T
⊺
3
1
√
N2
h (T
⊺
2
1
√
N1
h (T
⊺
1
ξ
n
)))] (103)
So far, we have not used any particular assumption on the form of the NN or the data, other
than that the layer structure used to write the explicit expression for the form eventually needed,
η(S, T), a single layer Self-Overlap. As a next step, we show which assumptions are needed in
order to reformulate the setup as an effectively a single layer linear model for a NN.
5.1.2 A Single Layer Matrix Model
Following others in the literature [110], and for simplicity, one can restrict to the simplifying case
that the function h(x) is the identity function. To evaluate Eqn. 103, there are three possibilities.
First, we can multiply all the matrices together, and treat a multi-layer NN effectively as a single
layer. Under this assumption, Eqn.103 simplifies to
⟨η([Sl
, Tl], ξ
n
)⟩
ξ
n =
1
n
1
N3 N2 N1
Tr [(ξ
n
)
⊺S1S2S3T
⊺
3T
⊺
2T
⊺
1
(ξ
n
)] . (104)
While this is possible, it would not lead to layer-by-layer insights (as HTSR-based approaches do).
Second, we could attempt to expand Eqn. 104 into inter- and intra-layer terms, which we could
readily do if the S and T matrices were square and the same shape, and then apply Wick’s
theorem:
η([Sl
, Tl]) ≈
1
n
L
∏
l=1
1
Nl
⟨η(Sl
, Tl
, ξ
n
)⟩
ξ
n =
1
n
L
∏
l=1
1
Nl
Tr [(ξ
n
)
⊺S
⊺
l Tl(ξ
n
))] + intra-layer cross terms.
(105)
62
However, these matrices are not square, and we don’t know how to express the intra-layer cross
terms. Finally, we can simply assume that the individual layers are statistically independent, in
which case we can treat each layer independently. By ignoring the intra-layer cross-terms, let us
write the single-layer self-overlap η(Sl
, Tl
, ξ
n
) as:
η(Sl
, Tl
, ξ
n
) = ⟨η(Sl
, Tl
, ξ
n
)⟩
ξ
n → 1
n
1
N
Tr [(ξ
n
)
⊺S
⊺
l Tl(ξ
n
)] . (106)
This third approach is the one we will adopt. Moving forward, we will drop the layer subscript,
l, and we will consider a SETOL as a single-layer theory.
5.1.3 The Matrix-Generalized ST Overlap (η(S, T)).
We can now relate the Self-Overlap for the NN layer η(ξ)l
in Eqn. 106 as a matrix-generalized
form of the ST Annealed Error Potential ϵ(R) = 1 − R. Since we can interpret the Trace as an
expected value over the model data ξ
n
, this gives the desired
η(S, T) ∶=
1
N
Tr [S
⊺T] (107)
We have dropped the 1/n because (as noted above), the 1/n will be implicit in the T and S
matrices because they represent empirical observables. That is, the weights are the learned
parameters, effectively averaged over the entire real-world training data set D. This is the matrix
generalized form of Eqn. 96.
5.2 Quality Metrics of an Individual Layer as an HCIZ Integral
In this subsection, we describe how to generalize the (Thermal) average over the Students ⟨⋯⟩
β
s
to an integral over random Student matrices, ⟨⋯⟩
β
S
, called an HCIZ integral.
5.2.1 A Generating Function Approach to Average Quality-Squared of a Layer
For our matrix generalization, we need to express the Layer Quality Q¯ in terms of the dataaveraged Self-Overlap in Eqn. 107 for a individual layer.
• Student-Teacher Overlap R For the vector-based Perceptron ST model, the dataaveraged Self-Overlap appears in the expression for the Layer Quality in Eqn. 99, and
is just the average ST vector overlap R = s
⊺
t. We define
R =
1
N
S
T T. (108)
The overlap matrix R describes the average interactions between N interacting M-dimensional
layer (i.e., feature) vectors. Also, notice that we have dropped 1/n term. For the vectorbased ST model, the ST Quality Q¯ST in Eqn. 99 is expressed as the Thermal Average
Q¯ST (R) = ⟨R⟩
β
s
. We seek a similar formal expression for the individual matrx-generalized
Layer Quality Q¯NN
L
, this time in terms of R and a matrix-generalied Thermal Average.
• Model and Layer Qualities Q¯NN , Q¯NN
L We define the Model Quality Q¯NN , as explained
in Subsection 2.3, Eqn. 7, to be a product of individual NN Layer Qualities Q¯NN
L
, and, as
in Eqn. 60, approximates the total NN Average Generalization Accuracy (1 − E¯NN
gen ):
Q¯NN ∶= ∏
L
Q¯NN
L ≈ 1 − E¯NN
gen (109)
The individual Q¯NN
L
expresses the contribution that layer makes to the approximate total
NN Average Generalization Accuracy.
63
• Layer Quality-Squared Q¯2 For technical convenience, however, rather than compute the
NN Layer Quality Q¯NN
L
directly, we will work with the Average Layer Quality-Squared,
defined as
Q¯2
∶= ⟨R
⊺R⟩
β
S
(110)
where ⟨⋯⟩
β
S
is now a Thermal Average over Student weight matrices S– an HCIZ integral.
This choice means that the final Layer Quality Q¯ we use approximates what would be the
matrix-generalized NN Layer Quality Q¯NN
L
(above) as
Q¯ ∶=
√
Q¯2 =
√
⟨R⊺R⟩
β
S
(111)
≈ ⟨
√
R⊺R⟩
β
S
≈ ⟨R⟩
β
S
= Q¯NN
L
• Overlap Squared The Overlap operator (squared) Tr[R⊺R] is defined in terms of Eqn. 116
such that we can
Q¯2
∶= Tr [R
⊺R] =
1
N2 Tr [T
⊺SS⊺T] =
1
N
Tr [T
⊺ANT] . (112)
This choice, (as opposed to Tr[RR⊺
], which would correspond with Eqn. 115,) places Q¯2
in the form of the HCIZ integral, as in Eqn. 73. See Appendix A.6 for a detailed discussion
of why we define Q¯2 using the Outer Student Correlation matrix AN (defined in Section 3,
and Eqn. 116 below).
• Generating Function For the vector-based ST model, we could compute Q¯ST using a
generating function, βΓ
ST
Q¯
. For our matrix generalization, we compute Q¯ from a Layer
Quality-Squared Generating Function βΓ
IZ
Q¯2
, given as
βΓ
IZ
Q¯2
∶=
1
N
ln∫ dµ(S) exp [nβN Tr [R
⊺R]] . (113)
We normalize the βΓ
IZ
Q¯2 because we are going to take the Large-N limit in N, and we need
to keep the Layer Quality finite. This allows us to implicitly fix the effective load of the
layer, n/N, as βΓ
IZ
Q¯2
is also in the Thermodynamic limit and therefore at Large-N in n,
although we do not explicitly set the layer (feature vector) load n/N. See Appendix A.3
for the derivation of Eqn. 113 (and recall the discussion in Section 4.2).
We cannot evaluate Eqn. 113 directly; but we will be able to evaluate it if we transform it
into an HCIZ integral (as in Eqn. 73). To do this, however, requires a trick which will allow us
to work with different forms of the Student A (and Teacher X) Correlation matrices.
From weight matrices to Correlation matrices. To evaluate Q¯2
in terms of derivatives of
Eqn. 113, we need to introduce the change of measure:
dµ(S) → dµ(A), (114)
64
and then restrict dµ(A) to resemble just the generalizing eigencomponents of the Teacher correlation matrix X. We have two choices for the Student Correlation Matrix A, call them AM
(Inner) and AN (Outer), defined as:
(Inner) AM ∶=
1
N
S
⊺S (which is M × M) (115)
(Outer) AN ∶=
1
N
SS⊺
(which is N × N). (116)
Note that 1
N
is the correct scaling on each of these. Eqn. 115 is consistent with our definition
of the layer Correlation Matrix, and we use it as the starting point below to derive the VolumePreserving ERG condition (Appendix A.4). Eqn. 116 is consistent with Tanaka, which requires A
be N × N, but we still need a Duality of Measures to rederive this (Appendix A.6) [83, 84].
Duality of measures. For either form of A, the measure dµ(A) is the same because we will
restrict the measures to the ECS space of non-zero eigenvalues (λi ≫ 0). We note that AM and AN
have the same eigenvalues λi
, or ESD, up to the additional zero eigenvalues (λi = 0) in the null
space of AN. Consequently, both forms of A have the same non-zero part of the ESD (ρ(λ) ≫ 0),
and the same Trace ( Tr[AM] = Tr[AN]). In the large-N approximation, the ESD of (either form
of) A, ρ
∞
A(λ), becomes continuous (and bounded), but remains zero in the null space. (Notice
that following the physics approach, in the limit, N remains finite but we nevertheless assume a
continuous spectrum) Consequently, when integrating over the eigenvalues λ, one can interchange
AM with AN, such that
∫ dµ(A) [⋯] ↔ ∫ dλ [⋯] ρ
∞
AM
(λ) ↔ ∫ dλ [⋯] ρ
∞
AN
(λ) (117)
This equivalence will be essential both to derive the ERG condition (Section A.4), and to (re)derive
the core result by Tanaka (Section A.6). Additionally, and WLOG, we may occasionally denote
the Student Correlation matrix as A˜ instead of explicitly using AM and AN.
5.2.2 Evaluating the Average Quality (Squared) Generating Function
We can write the generating function βΓ
IZ
Q¯2
in Eqn. 113 in terms of AN, giving, as in Eqn. 12,
βΓ
IZ
Q¯2 =
1
N
ln∫ dµ(S)e
nβNT r[
1
N
TT ANT])
. (118)
To recast Eqn. 118 as an HCIZ integral, as in Eqn. 73, we must perform a change of measure,
from N × M Student weight matrices S to N × N Outer Student Correlation matrices AN, as in
Eqn. 114.
When we perform the change of measure on Eqn. 118, we obtain the following expression:
βΓ
IZ
Q¯2 ≈
1
N
ln∫ dµ(A)e
nβNT r[
1
N
T⊺ANT]
e
N
2
ln(det(AM))
=
1
N
ln ⟨e
nβNT r[
1
N
T⊺ANT]
e
N
2
ln(det(AM))⟩
A
(119)
where the latter expresses the former in Bra-Ket notation. Since the measures of AM and AN
are equivalent, this evaluates to the same quantity regardless of which is used. This expression is
derived in Appendix A.4 (see Eqn. 249): it contains the original overlap term that depends AN
as well as a new term from the transformation that depends on det (AM) that is not yet defined.
65
5.2.3 The Effective Correlation Space (ECS)
Currently, Eqn. 119 is a “formal” expression, and we have not identified and/or justified its
realm of applicability. Fortunately, we have empirical evidence to suggest that this can be made
“physically” meaningful, by “restricting” the integral to the tail of the Empirical Spectral Density
(ESD). Since the ESD is an empirical object, this is where the theory becomes Semi-Empirical.
Prior work on HTSR theory indicates that during training the generalizing parts of a layer
weight matrix concentrate into ρ
emp
tail (λ), the tail of the ESD, as the ESD becomes more Power Law
(PL), and layer PL exponent α → 2+ (from above) [63, 64, 25, 26, 28]. This suggests the integral
in Eqn. 9 should instead average over a low rank subspace spanned only by the generalizing eigencomponents of the Inner Correlation matrix, AM =
1
N
S
⊺S (or equivalently, the Outer Correlation
matrix AN). We call this subspace the Effective Correlation Space (ECS).
Let A˜ be the matrix spanned by the largest M˜ eigen-components A in either form, AM (Inner)
or AN (Outer), and using physics BraKet notation, giving
A˜ ∶= PecsA, Pecs ∶=
M˜
∑
i=1
∣λ˜
i⟩⟨λ˜
i
∣ (120)
where Pecs is the projection operator onto the subspace spanned by the eigenvector ∣λ˜
i⟩ associated
with the eigenvalue λ˜
i of AM or, equivalently, AN. We denote the corresponding projected Student
correlation matrices with a tilde, as follows:
AM → A˜ M such that dµ(AM) → dµ(A˜ M) (121)
AN → A˜
N such that dµ(AN) → dµ(A˜
N)
where now the matrices A˜ M and A˜
N are restricted to the ECS, and the measure dµ(A˜ M) is similarly
restricted. We denote the eigenvalue λ with the tilde, λ˜, when we want to emphasize it is in the
ECS–but may drop the tilde if it is clear from the context. See Appendix A.4 for more..
When an ESD is Fat-Tailed the ECS is at least as large if not larger than the PL tail of the
ESD. For an ESD with α ≥ 2, the generalizing eigen-components, i.e.∣λi⟩, will mostly concentrate
into the tail of the ESD, but some will remain in the bulk, so the ECS will be larger than the tail.
In this case that M˜ ≥ Mtail, and ρECS(λ) ⊇ ρtail(λ), as depicted in Figure 5.2.4. When α = 2,
the layer is Ideal (as in Section 3.1), in that all of the generalizing eigen-components to have
now concentrated completely into the tail, so that M˜ = Mtail, and ρECS(λ) = ρtail(λ). (When
α < 2, this suggests that the layer is overfit, and the layer may have a Correlation Trap and/or
frequently also has many near-zero eigenvalues.)
This leads to the following Model Selection Rule (MSR) for the ECS:
When transforming the measure dµ(S) → dµ(A˜ ), we invoke an eigenvalue cutoff rule
that prescribes how to replace A with a low-rank effective matrix A → A˜ , where the
cutoff λ˜ ≥ λ˜min is chosen so that the ECS at least contains the PL tail and, importantly,
such that det(A) = det(AM) = det(AN) is well defined.
Formally, this means that when we evaluate the Quality (squared) Q¯2
, Generating Function βΓ
IZ
Q¯2
or other relevant averages, we restrict the measure (i.e., integral or sum) to the eigencomponents
in the tail of the ESD of A (or X, when appropriate) starting with λ˜min. To our knowledge, this
proposed MSR is completely novel.
66
Restricted to the ECS, we now replace Eqn. 119 with:
βΓ
IZ
Q¯2 =
1
N
ln∫ dµ(A˜ )e
nβNT r[
1
N
T⊺A˜ NT]
e
N
2
ln(det(A˜ M))
=
1
N
ln ⟨e
nβNT r[
1
N
T⊺A˜ NT]
e
N
2
ln(det(A˜ M))
⟩
A˜
(122)
where we have used the formal Duality of Measures, dµ(A˜ ) = dµ(A˜ M) = dµ(A˜
N).
36
5.2.4 Two Simplifying Assumptions: the IFA and ERG Condition
It now remains how to define the cutoff for the ECS space. To accomplish this, we make the
following assumptions.
• The Independent Fluctuation Assumption (IFA). This condition states that the two
terms appearing in the exponential of Eqn. 122 are statistically independent:
βΓ
IZ
Q¯2 ≈
1
N
ln ⟨e
nβNT r[
1
N
T⊺A˜ NT]
⟩
A˜
⟨e
N
2
ln(det(A˜ M))
⟩
A˜
. (123)
• The Exact Renormalization Group Condition (ERG). This condition states that the
determinant of the Student (and Teacher) Correlation matrix is unity, such that:
det (A˜ ) = 1 or Tr [ln A˜ ] = 0, (124)
(and with A additionally normalized to M, as explained in the Appendix, Section A.2.1)
so that when we replace the measure over Student layer weight matrices dµ(S) with a
measure over all Student Correlation matrices dµ(A), restricted to the ECS, the second
term in Eqn. 123 becomes unity, ⟨exp [
N
2
Tr[ln[det AM]]]⟩A˜ = 1, and vanishes in the final
expression for βΓ
IZ
Q¯2
.
At this point, the IFA is made purely for mathematical convenience, i.e., we have not demonstrated
it empirically, but it is not implausible as a statistical modeling assumption. On the other hand,
we can test the ERG condition empirically; this is a critical part of our SETOL approach.
Notably, when taking the large-N approximation of βΓ
IZ
Q¯2
, we effectively do this independently
in two steps. First, we apply a Saddle Point Approximation (SPA) to the second term in Eqn. 123,
leading to the ERG condition (see Appendix A.4). Most importantly, we can test the ERG condition
empirically, and this is another critical part and justification of our SETOL approach. Second, when
applying the result by Tanaka (Eqn. 75), we are applying an SPA to the result, but assuming we
are restricted to the ECS.
Empirical Tests of the ERG Condition and the ECS. Since we require that the students
resemble the fixed Teacher, a reasonable estimate for the average of the Student Correlation
matrix A˜ (either A˜ M or A˜
N) (in the ECS) is the point estimate provided by the actual (known)
fixed Teacher correlation matrix X˜ ,
⟨ det A˜ M⟩
A˜
= ⟨ det A˜
N⟩
A˜
≃ det X˜ = ∏
t
λt = 1 ∀λ˜
t ∈ ρ
emp
tail (λ), (125)
for all eigenvalues λ˜
t
in ρ
emp
tail (λ) the tail of the ESD of X˜ , i.e., λ˜
t ≥ λ˜min. How should one choose
λ˜min in this expression? We already know that most NNs layers have Fat-Tailed ESDs, but if the
36Also, we could replace T → T˜ , but to simplify the notation, we do not do this.
67
Figure 11: The image depicts a typical Empirical Spectral Density (ESD) of a layer correlation
matrix X, with WeightWatcher Power-Law (PL) exponent α = 2.0.The green and red shaded
regions depict eigenvalues λ in the Effective Correlation Space (ECS) of X˜ , defined by λ > λ
ECS
min .
The x-axis displays the eigenvalues on the log scale, ln λ. The vertical red line is at the start of
the PL tail (λ
P L
min). The purple, vertical line is at the start of the ECS tail (λ
ECS
min ). The green
shaded region depicts those eigenvalues where ln λ > 1.0, whereas the red shaded region depicts
those eigenvalues where ln λ < 1.0. The ECS is defined such that the Exact Renornmalization
Group (ERG) condition is best satisfied, i.e ∑ ln λ = 0 for λ ≥ λ
ECS
min .
Teacher ESD is simply Random-Like or even Bulk+Spikes, then the expected value ⟨det (AM)⟩
itself of the determinant of a random full rank Student Correlation matrix AM might be well
defined and easy to estimate, and we might not even need to define the lower rank ECS. Indeed,
in these cases, the correlated eigencomponents, if they exist, may be buried in the bulk region of
the ESD and not readily identifiable. Because ρ
emp
tail (λ) is Fat-Tailed and Power Law (PL), this
poses some difficulty, we need to first define the Effective Correlation Space (ECS).
Because the Teacher ESD is most likely MHT and PL, if we choose λ˜min ∶= λ
ECS
min too small,
and the tail extends too far into the bulk region of the ESD, then for all practical purposes
det (X˜ ) ≪ 1. On the other hand, if we choose λ˜min too large, then we only capture very large
eigenvalues, and for all practical purposes det (X˜ ) ≫ 1. Therefore, if we set the scale of X˜
appropriately, we can choose a λ˜min such that det (X˜ ) = 1. In this case, by choosing λ˜min
appropriately, we can estimate the expected value of ⟨det (AM)⟩ with an empirical point estimate
over the Teacher Correlation matrix, which is unity.
∣ det X˜ ∣ ≃ 1; Tr [ln X˜ ] = ln ∣ det X˜ ∣ ≃ 0. (126)
This expression can now be used in a practical calculation to define a low-rank subspace
that both allows us to evaluate the HCIZ integral, and to identify, in principle, the generalizing
components of the layer. We also refer to Eqn. 126 as the ERG condition, which is technically
its empirical form. For example, Figure 5.2.4 depicts the eigenvalues in the ECS for a typical
ESD with PL α = 2.0.. Notice that the start of the PL tail, (λ
P L
min), is very close to the start of
the ECS tail. (λ˜min), i.e. ∆λmin ∶= λ˜min − λ
P L
min ≈ 0. Also, notice that while there are many large
eigenvalues, ln λ > 1.0, there are numerous small eigenvalues as well, ln λ < 1.0, such that the
∑ ln λ ≈ 0 for λ ≥ λ˜min. In other words, the red and green shaded areas have the same measure.
Additional plots like Figure 5.2.4, generated with WeightWatcher, can be found in Section 6, in
68
Figure 21 as well as plots of ∆λmin vs. the WeightWatcher PL α for several real-world examples,
in Figures 22 and 23.
5.3 Evaluating the Layer Quality (Q¯) in the Large-N Limit
To generate the Average Quality, Q¯2
, we first take the Large-N limit of βΓ
IZ
Q¯2
in N,
βΓ
IZ
Q¯2,N≫1
∶= lim
N≫1
βΓ
IZ
Q¯2 = lim
N≫1
1
N
ln ⟨exp Nβ Tr [
1
N T
⊺ A˜
N T]⟩A˜
(127)
and then take the appropriate partial derivative, analogously to how we did for E¯ST
gen; see Section A.3 for more details. This gives (as in Eqn. 65)
Q¯2
∶=
1
β
∂
∂nβΓ
IZ
Q¯2,N≫1
≈
high-T
1
n
∂
∂β βΓ
IZ
Q¯2,N≫1
(128)
Notice that since we are at high-Temperature, it doesn’t matter which partial derivative we take,
and we expect both results to be yield the same expression.
This HCIZ integral in Eqn. 127 can be evaluated (i.e in the Large-N limit in N) using a
result by Tanaka —provided the matrix A˜
N is low rank, which holds when the ERG condition is
satisfied. Thus, moving forward, we will assume an Effective Correlation Space (ECS) of rank M˜ ,
where λ˜min is the Mth-largest eigenvalue of X˜ , and defines the start of the ECS (and whatever
branch-cut is necessary to integrate R(z)).
Tanaka’s result for the ECS can be expressed as:
lim
N≫1
1
N
ln ⟨exp (nβ Tr [T
⊺A˜
NT])⟩A˜ = nβ
M˜
∑
i=1
G(λ˜
i), (129)
where the sum now only includes the eigenvalues of X˜ (in the ECS), β =
1
T
is the InverseTemperature, n is the size of the training dataset, and λ˜ is an eigenvalue of X˜ , the Teacher
Correlation matrix projected into the ECS space. A˜
N is the N × N form of the Student Correlation matrix, with N − M non-zero eigenvalues, and T is the N × M Teacher weight matrix (also
effectively projected into the ECS, i.e. T → T˜ here). G(λi) is the Norm Generating Function,
defined below. 37 This gives
βΓ
IZ
Q¯2,N≫1
= nβ
M˜
∑
i=1
G(λ˜
i), (130)
This gives a final expression for the Average Layer Quality (Squared) Q¯2
as
Q¯2
=
M˜
∑
i=1
G(λ˜
i), (131)
Note that Q¯2
is independent of N and β, and, indeed, Eqn. 128 is an equality.
The average Quality (squared) can be expressed as a sum over Generating Functions G(λ),
which depend only the statistical properties of the actual Teacher Correlation matrix X˜ (projected
into the ECS). Each term in the sum, G(λ˜
i), takes the form
G(λ) ∶=
λ
∫
0
RA(z)dz ECS
ÐÐ→
λ
∫
λ˜min
RA˜ (z)dz (132)
37We use the notation ⟨⋯⟩A˜ for expected value and placed 1
N
on the L.H.S. to help the reader compare this to
the original expressions in [83, 84]. Also, in [83, 84],β = 1∣2, but, in fact, if one inserts −β as an inverse temperature
into the final expression, it simply factors out.
69
where RA˜ (λ˜) is the R-transform from RMT, and λ˜min is the lower bound of the ECS spectrum.
Importantly, the R-transform for a Heavy-Tailed ESD may have a branchcut at or near the start
of the ECS (as explained in Section 5.4), so restricting the integrand to start at λ˜min is critical.
Since we expect the best Student matrices to resemble the actual Teacher matrices, we expect the Student correlation matrix A˜ to have similar spectral properties to our actual empirical
correlation matrices X˜ . That is, from the perspective of HTSR theory and the classification into
5+1 Phases of Training [25], we expect all the A˜ to be in the same phase as X˜ (and, in addition,
to have the same PL exponent value). That is,
We expect the R-transform of A˜ to have the same functional form as the R-transform of X˜ .
If our (Teacher) NN weight matrix exhibits a HT PL, then the tail the ESD (ρtail(λ)) of the
Student and Teacher will both take the limiting form of a PL, with the same empirical variance
σ
2
and (critically) the same PL exponent α:
ρtail[A˜ ](λ) ∼ ρtail[X˜ ](λ) ∼ λ
−α
. (133)
Up until this point, our derivation of Q¯2
only depends on the ERG condition, irrespective of
the exact functional form of R(z), therefore the SETOL approach is tested by examining how well
the ERG condition and the ECS holds for the layers in very well-performing models. We do this in
Section 6.3.
5.4 Modeling the R-Transform
To apply SETOL, the model must satisfy the ERG condition–which occurs during the case of Ideal
Learning. For most cases of NN models, the ESD are HT; and in practice, one usually would
select R(x) that reflects this. But to be more general, we formally extend the theory to allow the
practitioners to both model the ESD as just a collection of discrete spikes, and to even correct
for Correlation Traps and/or Very Heavy-Tailed (VHT) ESDs. Most importantly, we derive
expressions for both the WeightWatcher Alpha and AlphaHat metrics, valid for the case Ideal
Learning where α = 2, and formally extended for other cases.
The goal of this section is to obtain formal expressions for the Layer Quality, Q¯ which is
given in terms of what we somewhat imprecisely refer to as a Norm Generating Function. In
many cases, however, the resulting approximate expressions for Q¯ do take the form well-known
norms, including the Frobenius norm, the Spectral norm, and the Shatten norm. The models
for R(z) we use are presented in Table 8. The inal expression for the Layer Quality Q¯ extended
Semi-Empirically to the non-ideal layers that can then be used to obtain the HTSR Alpha and
AlphaHat metrics; these are given in Table 9.
5.4.1 Elementary Random Matrix Theory
We begin with some useful notions definitions from Random Matrix Theory. Using the ESD
ρ(λ), defined as
ρ(λ) ∶=
1
M
M
∑
i=1
δ(λ − λi), (134)
we can express the Greens Function (or Cauchy-Stieltjes transform C(z)) by38
G(z) = C(z) ∶= ∫ dλ
ρ(λ)
z − λ
. (135)
38Notice our naming and sign convention in Eqn. 135. We equate the Greens Function G(z) with the (positive)
Cauchy-Stieltjes transform, G(z) = C(z). Other works may use the opposite sign convention, G(z) = −C(z).
70
From G(z), we can recover the ESD, ρ(λ), using the inversion relation
ρ(λ) = lim
ϵ→0+
1
π
IM(G(λ + iϵ)), (136)
where IM is the imaginary part of G(z), and where the limϵ→0+ means to take the limit approaching from the upper half of the complex plane. The R-transform, R(z), can be defined using the
Blue function B(z)
R(z) ∶= B(z) −
1
z
, (137)
where the Blue function B(z) [38] is the functional inverse of the Greens Function G(z),
39 satisfying
B[G(z)] = z. (138)
By specifying the R(z) transform, we specify the complete ESD, ρ(λ). Here, we are actually
only interested in the tail of ρ(λ). That is, we can given R(z) = R(z)tail + R(z)bulk, we only need
R(z) ≈ R(z)tail.
Before moving forward, it is first necessary to demonstrate the R-transform R(z) exists for
the power-law ESDs that arise in SETOL; this is shown in Appendix A.7. Specifically, it is shown
that while the R(z) does not formally exist for a pure power-law (PL) tail with α = 2, it does
generally exist for Truncated power-law (TPL) tails, defined such that the domain is compact,
i.e., λ ∈ [λ˜min, λ˜max].
5.4.2 Known R-transforms and Analytic (Formal) Models
There are only a few known analytic results for the explicit R-transform R(z). The ones we need
are in Table 8. Below, we review some of them, explaining what ESD they correspond to, and
what the resulting Norm Generating Function G(λ) would be if applied as a model R(x) here.
5.4.3 Discrete Model: Bulk+Spikes, MHT, HT
Here, we consider modeling the tail ESD, ρtail(λ), as a collection of discrete spikes λspike, where
λspike ≥ λ˜min. This could be for modeling an ESD in the Bulk+Spikes (BS) HTSR Universality
class, or it could be applied to an MHT or HT ESD as a discrete distribution with no inherent
structure; its a modeling choice. Here, R-transform for the ECS is sum of Dirac delta functions.
This lets us compute the Layer Quality Q¯ in closed form in terms of the Teacher weight matrix
T = W as a Tail norm, the Frobenius norm of the tail eigenvectors.
Let the tail of the ESD have M˜ = Mtail eigenvalues that define the ECS, i.e.,
ρtail(λ) =
M˜
∑
i=1
δ(λ − λ˜
i). (139)
where the λ˜
i are normalized by 1
M .
The Greens Function G(z) is then
G(z) = ∫ dλρtail(λ)
z − λ
=
M˜
∑
i=1
∫ dλδ(λ − λ˜
i)
z − λ
=
M˜
∑
i=1
1
z − λ˜
i
, (140)
39The Blue function was first introduced by Zee [38] to model, among other things, spectral broadening in
quantum systems. Briefly, given a deterministic Hamiltonian matrix H0, with eigenvalues λ
0
i
, one can model the
spectral broadening of λ
0
i by adding a random matrix H1 to H0: H = H0 + H1. The resulting eigenvalues of H
now contain some level of randomness, σ, i.e., λ = λ
0 + σ. To model the ESD of H, one then specifies the Rtransforms for H0 and H1; the full ESD of H can then be reconstructed by adding the two R-transforms together
R(z) = R0(z) + R1(z). Zee notes that R(z) is the self-energy Σ(z) from quantum many body theory [38].
71
Model HTSR Universality Class R(z)
Discrete Bulk+Spikes, MHT, HT 1
M˜ ∑
M˜
i=1 λi
Wishart Models
Multiplicative-Wishart HT/VHT ϵϕz2
2 − ϵϕ2z
2
Inverse Marchenko-Pastur (IMP) HT/VHT κ −
√
κ(κ − 2z)
z
Lévy Wigner (LW)
Free Cauchy (FC) (αl = 1) HT α = 2 a + iγ
General Lévy (αl ≠ 1) VHT α < 2 a + bzα−2
Table 8: Known R-transforms for random matrix ensembles relevant to modeling heavy-tailed
spectral densities (eigenvalues or singular values squared). The Multiplicative-Wishart model has
two real, non-zero parameters, ϵ and ϕ; for more details, see [111]. For the Inverse MarchenkoPastur, as given by Bun [112], κ =
1
2
(Q − 1) where, q =
1
Q
=
M
N
≤ 1. The Lévy-Wigner (LW)
model describes Wigner-like square random matrices (as opposed to Wishart-like or Correlation
Matrices), where the elements are drawn from a Lévy-Stable distribution. The resulting LW
ESD is Heavy-Tailed Power Law, and characterized by the Lévy exponent αl
. The LW R(z) is
parameterized by a (real) shift parameter a, a complex phase factor b (that depends on 3 real
parameters αl
, β, and γ), and, of course, αl
. The Free Cauchy (FC) model is a special case of the
IW model, corresponding to the Lévy αl = 1, and the HTSR α = 2. We will extend the LW models
to the rectangular case for our modeling purposes here by making the association α = αl + 1 for
α ≤ 2. (Also, we take thew variance σ = 1 for all models.) Generally speaking, the Lévy R(z) are
more complicated; for more details, see [113, 114, 115].
and the Blue function for each individual term i is 1
z−λ˜
i
, i.e., B(z) = λ˜
i+
1
z
. Now, using the additive
property of the R-transform, we can express the total R(z) as the sum of the R-transforms for
the individual terms i, giving
R(z) =
M˜
∑
i=1
((λ˜
i +
1
z
) −
1
z
) =
M˜
∑
i=1
λ˜
i
. (141)
This gives the Norm Generating Function G(λ) as
G(λ) = ∫
λ
λ˜min
M˜
∑
i=1
λ˜
idλ (142)
=
M˜
∑
i=1
λ˜
i ∫
λ
λ˜min
1dλ
=
⎛
⎝
M˜
∑
i=1
λ˜
i
⎞
⎠
(λ − λ˜min)
Seeing that λ˜min is usually quite small, we make the approximation
G(λ) ≈
⎛
⎝
M˜
∑
i=1
λ˜
i
⎞
⎠
(λ) , (143)
72
which gives the Quality-Squared approximately as
Q¯2
=
M˜
∑
i=1
G(λ˜
i) ≈
⎛
⎝
M˜
∑
i=1
λ˜
i
⎞
⎠
2
. (144)
We now see that we can define Q¯ ∶=
√
Q¯2 = ∑
M˜
i=1 λ˜
i
is what we call a Tail Norm, the Frobenius
norm of the tail eigenvalues.
5.4.4 Free Cauchy Model (α = 2)
For the Free Cauchy (FC) model the R-transform is a constant
R(z)[FC] = a + iγ, (145)
where the shift parameter a > 0 translates the FC spectrum (i.e. so it overlaps with the the Power
Law tail of the ESD), and the scale parameter γ > 0 sets the tail width. We do not attempt to
fit a and γ to a real-world ESD here but rather simply use this model formally,
Because R(z) is independent of z, the integral is straightforward:
G(λ)[FC] = ∫
λ
λ˜min
R(z)[FC]dz = (a + iγ)(λ − λ˜min). (146)
As explained in Appendix A.6, when R(z) is complex, we keep the Real part G(λ˜
i), giving,
R[G(λ)[FC]] = a(λ − λ˜min). (147)
To obtain a formal expression for Alpha, we make two approximations. First, we take the
lower cut-off to be near zero (λ˜min ≈ 0).
∣G(λ)[FC]∣
λ˜min≈0
≈ aλ (148)
Second, we assume the Layer Quality-Squared is dominated by the largest term in the sum:
Q¯2
FC =
M˜
∑
i=1
∣G(λ)[FC]∣
λ˜min≈0
= a
M˜
∑
i
λ˜
i ≈ aλmax (149)
For the Heavy-Tailed ESDs, we generally find that as λmax increases, the HTSR α decreases.
Juat for illustrative purposes, If we take log10 λmax ∼
1
α
(assuming log10 λmax > 1), and take the
square root of both sides, we obtain the desired result for the log Quality, namely, that it scales
inversely with α
log10 Q¯
FC ∼
1
α
(150)
If we extend this formally beyond the Ideal case to α ≥ 2, we now can explain why the HTSR α
makes a good Layer Quality metric in deep networks, as smaller α suggests higher quality layers
and therefore a higher overall model quality and/or accuracy.
5.4.5 Inverse Marchenko-Pastur Model of Ideal Learning
For a more realistic model of a layer ESD, here, we consider the Inverse Marchenko-Pastur (IMP)
model [112]. The IMP model treats the ESD of X−1 when the ESD of X itself is MP. As a
parametric model, it can be quite effective at treating VHT and HT (or Fat-Tailed) ESDs, α ≤ 4,
but works best when α = 2.0. To do this, one simply considers κ, as an adjustable parameter
73
(a) IMP Distribution, κ = 0.5 (b) IMP κ vs. HTSR α
Figure 12: (12(a)) plots a typical Inverse Marchenko-Pastur (IMP) ESD κ = 0.5, along with
Power Law (PL) fits, with PL exponent α = 1.86. (12(b)) depicts the linear relationship between
κ and α for a few randomly generated examples.
which effectively captures the rank or soft-rank of an ECS-like space in these non-ideal layers. It
is an excellent model for the ESD when α = 2.0 (and Q = 2). Using this model, we can derive
an expression for the HTSR AlphaHat Layer Quality metric, αˆ ∶= αlog10 λmax) as a leading order
term in the final expression for log10 Q¯2
.
In Figure 12, we generate a random IMP model ESD with κ = 0.5, Q = 2.0, fit the ESD to a
PL, and find α = 1.08; this is a reasonably accurate model of an PL ESD for α ≈ 2. For larger κ,
the fits are not as good, but the model is still useful to suggest a heuristic model for the Layer
Quality in terms of the HTSR α. In Figure 12(b), we show that there is a simple relationship
between κ and α, which allows us to formally state α = 2 κ.
Lets consider R(z) for the Inverse Marchenko-Pastur model, denoted R(z)[IMP]. To integrate this function, we require that it be analytic. At first glance, it may seem that that
√
R(z)[IMP] is not analytic because it has a pole at z = 0 and because the square-root term
κ(κ − 2z) creates branch cut at and z = κ/2 (and z = ∞). Figure 13 presents this in two ways:
Figure 13(a) shows the R-transform R(z)[IZ] for real z, highlighting its singular behavior and
the location of the branch cut at z = κ/2; and Figure 13(b) shows the corresponding branch cut
in the ESD of the Inverse MP model (for κ = 0.5). We select the branch cut starting at z = κ/2
and ending at z = ∞, which allows us to at least formally defined the integral along the physically
meaningful part of the ESD:
G(λ)[IMP] ∶= ∫
λ
λ˜min
R[R(z)][IMP]dz, (151)
noting that we expect λ˜min ≥ κ/2 and we take the Real part of R(z), R[R(z)].
It turns out, however, that due to the branch cut in R(z)[IMP], the function G(λ)[IMP]
is not analytic in the domain we need. To correct for this, we will instead model the Layer
Quality-Squared using the Real Part R[G(λ)[IMP]], which yields (See Appendix A.8):
G(λ)[IMP] = ∫
λ
λ˜min
κ
z
dz = κ [ln z]
λ
λ˜min
= κ (ln λ − ln λ˜min) . (152)
Notice this expression is very similar to the expression for the Free Cauchy model above (Eqn. 149),
74
(a) IMP R(z), real z. (b) Branch cut at z = κ/2
Figure 13: (a) The function R(z) of the Inverse Marchenko-Pastur (IMP) model, with a singularity at z = κ/2. (b) The branch cut in the Empirical Spectral Density (ESD) at κ = 0.5.
except now it is terms of the logarithm of λ, and rescaled by κ. By associating α = 2 κ, we recover
the AlphaHat formular, albeit with a different normalization.
5.4.6 The Multiplicative-Wishart (MW) model
The Multiplicative-Wishart (MW) model has two real, adjustable parameters ϵ, ϕ. It treats X
as resulting from a product of random matrices, and is good for modeling an ESD with a Very
Heavy Tail (VHT). It has been used previously to model the heavy tail of the Hessian matrix in
NNs [111]. This model would work better for fitting HT ESDs that decay slower than a PL or
TPL. We note that unlike the IMP, the MW model does not have a branch cut at the ECS, but
it does have 2 poles, complicating the integration of the R-transform. We will not consider this
model further here and instead leave this for future work.
5.4.7 Levy-Wigner Models and the AlphaHat Metric
Here, we consider General Levy-Wigner (LW) model.. We show how to obtain the WeightWatcher
AlphaHat metric by modeling VHT ESDs with an approximation to a Levy distribution, suitable
for cases where the HTSR α ≤ 2.
The AlphaHat metric has been developed to adjust for Scale anomalies that arise from issues
like Correlation Traps, rank collapse, and overfitting, which can in many cases make Alpha
smaller than expected, and even smaller than 2. We model these VHT ESDs as if they follow
a Levy-Stable distribution, generated from a Levy-Wigner random matrix. The Levy-Wigner
(LW) model treats X as if it were a Wigner matrix (and not actually a rectnalgyle correlation
matrix), s we must decide how to map the HTSR α to the Lévy αl
; for this, we select αl = α − 1.
(See in Table 8.) The LW model is defined in terms of 3 parameters: a is a shift parameter, and
b is a complex phase factor depending on 2 real factors, β and γ. Strictly the ESD for an LW
model, ρLW (λ), is defined by its characteristic function (i.e., the Fourier Transform of ρLW (λ)),
but when the ESD is VHT, ρLW (λ) ∼ λ
−αl−1
, and when αl ≈ 1, the ESD resembles a PL HT ESD
with the HTSR α ≈ 2.
Let us model the R-transform of our Very Heavy-Tailed (VHT) ESDs as
R(z)[LW] = bzαl−1
, αl ∈ (0, 1) (153)
= bzα−2
, α ∈ (0, 2),
75
where b is an unspecified constant (possibly negative and/or complex). This is not a particularly
good model for VHT ESDs in practice; it is simply an approximate model, and we choose to
obtain a formal expression.
Integrating R(z)[LW], and (as above) taking the approximation λ˜min ≈ 0, we obtain
G(λ)[LW] ≈
b
α−2
λ
α−2
. (154)
For simplicity, if we now choose b = αl = α − 2, then Q¯2
LW takes the form of a Shatten Norm
Q¯2
LW =
1
M˜
M˜
∑
i
λ
α−1
. (155)
Selecting the largest eigenvalue as the dominant term in the sum gives
Q¯2
LW ≈ λ
α−1
max. (156)
Taking the logarithm of Q¯2
HT, we obtain
log Q¯2
[LW] ≈ (α − 1) log λmax (157)
As with the Free Cauchy (FC), let us approximate Q¯2 by the largest term in the sum over
G(λ˜
i), and then let λ = λmax, giving
αˆ = log10 Q¯2
LW ≈ (α − 1) log λmax. (158)
We present this as a formal example, noting that it is slightly different from the result for the
IMP model. (See Table 9). We do not claim this is a valid empirical model, as we have not
attempted to fit a real-world ESD to Levy-stable distribution. We leave this to a future study,
noting however, there has been some work doing such fits [116].
Ideally, we would like to have a rigorous expression for R(z) not just in the case of Ideal
Learning but also for the entire Fat-Tailed Universality class. This is nontrivial to obtain and we
will attempt this in a future work. For now, we will take a different approach and evaluate R(z)
explicitly using numerical techniques.
Scaling insight. For the VHT Universality class, we may expect λmax to be fairly large such
that λmax > 1 and therefore log10 λmax > 0 This implies that log10 Q¯2 decreases as α (and thus
αl) decreases, even though log10 λmax simultaneously increases in the VHT class. This opposing
interplay explains why correlation traps produce small α yet yield deceptively moderate quality
scores.
Historical remark. An earlier study on AlphaHat, reported the opposite trend, with AlphaHat
decreasing with increasing model quality. This is because we employed an un–normalized covariance, suggesting that λmax < 1 and log10 λmax < 0 (although we have not rigorously checked
this). The present M normalization clarifies that the VHT regime is meaningful only when α < 2,
indicating the layer and therefore the model is overfit (in some unspecified way).
5.4.8 Summary of heuristic models.
Using the results for different R-transforms, we can construct several different, related heuristic
models for the Layer Quality that resemble the very successful WeightWatcher HTSR Alpha and
AlphaHat metrics. These heuristic models allow us extend the SETOL results for Ideal learning
76
Model Tail Norm WW Metric log Q (Log Quality)
Bulk+Spikes (BS) Frobenius Norm N/A log
⎛
⎝
M˜
∑
i=1
λ˜
i
⎞
⎠
Free Cauchy (FC) Spectral Norm Alpha α log λmax ∼ 1/α
Lévy Wigner (LW) Shatten Norm AlphaHat αˆ (α − 1) log λmax
Inverse Marchenko-Pastur (IMP) ECS Boundary AlphaHat αˆ 2α[log λmax − log λmin]
Table 9: Closed-form or leading-order expressions for the log Layer Quality log Q derived from
the integrated R–transform for each core tail-model, simplified to show the relation to the
WeightWatcher Alpha and AlphaHat metrics. For each model, we interpret the final result.
Bulk+Space (BS): Sums the λ˜
i
in the ECS, giving a Frobenius Tail norm. Free Cauchy (FC):
Yields the Spectral Norm, λmax. Since this scales as 1/α, this explaines the HTSR Alpha metric as
it shows why smaller α yields higher Q. Lévy Wigner (LW): Yields a Shatten Norm, which can
be approximated by AlphaHat, αˆ, Being for the VHT Universality class, it implies that heavier
tails (α ∈ (1, 2)) depress Layer Quality. Inverse Marchenko-Pastur (IMP): Also gives AlphaHat,
and, importably, a branch cut that defines the ECS Boundary
(i.e., α = 2) to a wider range of NN layers, covering the HTSR HT and VHT Universality classes
and correspondingly, NN layers that are both underfit (α > 2) and overfit (α < 2).
Table 9 consolidates the derived expressions for the log Layer Quality, log Q¯, across multiple
R–transform models, each capturing distinct heavy-tail characteristics of the ESD within the
SETOL framework. The Discrete (Spikes) model yields a Frobenius-like tail norm by summing
the effective eigenvalues in the ECS, directly tying quality to tail magnitude (cf. Eqn. 144). The
Free Cauchy (FC) model links log Q¯ inversely to the HTSR metric Alpha, demonstrating that
smaller α values enhance quality (cf. Eqn. 150). The Lévy-Wigner (LW) model connects log Q¯ to
AlphaHat through the term (α − 1) log λmax, revealing how very heavy tails depress layer quality
(cf. Eqn. 158). The Inverse MP (IMP) model defines the ECS boundary via a branch cut at κ/2
and approximates log Q¯ as 0.5 log λmax, providing a robust metric for Ideal Learning scenarios.
Each model thus offers a unique lens on layer quality, enabling direct mapping between theoretical
R–transform assumptions and empirical WeightWatcher metrics.
Practical Implications for Neural Network Analysis These models empower researchers
to tailor layer-quality assessments to specific HTSR universality classes, boosting the predictive
power of WeightWatcher metrics like Alpha and AlphaHat. The Discrete model excels when the
ESD exhibits clear eigenvalue spikes, delivering a sharp quality signal for Bulk+Spikes layers.
The FC model supports Ideal Learning by linking quality directly to 1/α, which simplifies crosslayer comparisons. The LW model helps detect overfitting by adjusting for Correlation Traps via
AlphaHat in very heavy-tailed regimes. Finally, the IMP model offers a parametric fit for both HT
and VHT tails, with κ tuning alignment to real-world ESDs (cf. Figure 12). By integrating these
models, practitioners can compute precise layer-quality metrics that optimize both performance
and interpretability.
77
5.5 Computational Random Matrix Analysis
The R-transform is the generating function for the Free Cumulants of RMT. Formally–and if
we assume a model without a branch-cut or troublesome poles– one can define R(z) as a series
expansion in z,
R(z) ∶= κ1 + κ2z + κ3z
2 + . . . (159)
where the coefficients κk are the free cumulants, which can be expressed in terms of the matrix
moments mk [73], defined (here) as
mk ∶= Tr [X˜ k
] =
M˜
∑
i=1
(λ˜)
k
(160)
where λ˜
k is the k-th eigenvalue of the effective correlation matrix X˜ , which has been mean-centered
and normalized by its standard deviation.
The free cumulants are defined recursively as
κk ∶= mk − ∑
partitions of n
∏
blocks B
m∣B∣
(161)
The first 5 Cumulants are, explicitly,
k1 = m1 (162)
k2 = m2 − m2
1
k3 = m3 − 3m2m1 + 2m3
1
k4 = m4 − 4m3m1 − 2m2
2 + 10m2m2
1 − 5m4
1
k5 = m5 − 5m4m1 + 15m3m2
1 + 15m2
2m1 − 35m2m3
1 − 5m3m2 + 14m5
1
Using these definitions, we can estimate the Layer Quality matrix Q¯2
for our experimental
models (in Section 6 by computing G(λ) for the effective correlation space (i.e., the tail of the
layer ESD,) however it may be defined. That is, we use
G(λi) = κ1
λ˜
M˜
+
κ2
2
(
λ˜
M˜
)
2
+ ⋯ (163)
This is implemented in the WeightWatcher package. In Section 6.4, we examine some examples
of this approach to computing Layer Qualities directly.
78
6 Empirical Studies
In this section, we present empirical results. Our goals are to justify key technical claims, including
key assumptions underlying our SETOL approach, and to illustrate the behavior of SETOL with
respect to various parameters and hyperparameters. Importantly, it is not our goal to demonstrate
that layer PL exponent Alpha and AlphaHat perform well for diagnostics and predicting model
quality for SOTA NN models, as that has been demonstrated previously [26, 24, 27].
Since the SETOL theory presented in Section 5 is (effectively) a single layer theory, in order
to carefully test (as opposed to simply use) SETOL, we need to limit the degree of inter-layer
interactions present in the model. To do so, we consider a three-layer Multi-Layer Perceptron
(MLP3), trained on MNIST [117]. We refer to the hidden layers as “FC1 and “FC2. Their output
sizes and parameter counts are shown in Table 10.
Layer Units Weight Parameters % of total
FC1 300 768 × 300 = 230, 700 88.2%
FC2 100 300 × 100 = 30, 000 11.4%
out 10 10 × 100 = 1000 0.38%
Table 10: Dimensions of each FC layer in the MLP3 model, along with weight matrix parameter
count and fraction of the total.
The following are the main topics we consider.
6.1 Model Quality: HTSR phenomenology. The HTSR phenomenology provides a metric of
model quality in the form of the PL exponent α.
40 In particular, smaller values of α (e.g.,
values of α closer to 2 than 3 or 4) should correspond to better models, e.g., having smaller
test errors; and a minimal error should be obtained when α = 2. See Section 6.1.
6.2 Effective Correlation Space. The SETOL theory is based on the notion of an Effective
Correlation Space, in which the learning and generalization occurs. This is the low-rank
subspace Wecs of each layer W that approximates the teacher T. In particular, our measure
of model quality should be restricted to Wecs. The Effective Correlation Space can be
identified from the tail of the ESD, ρtail(λ), and it can be chosen according to one of several
related Model Selection Rules. See Section 6.2.
6.3 Evaluating the ERG Condition. In the HTSR phenomenology, when a model is very
well-trained, the layer PL exponent α ≃ 2. In the SETOL theory, when a model is very
well-trained, the eigenvalues in the tail will satisfy the Empirical ERG Condition, given in
Eqn. (126). In Section 6.3, we provide a detailed analysis of this effect.
6.4 Computational Model Qualities. In Section 6.4, we empirically compare the HTSR
layer-quality exponent α against the computational R-transform-derived metric Q¯2
on fully
trained MLP3 models. We show that for the FC1 layer, Q¯2
closely tracks α with small error
bars and the expected batch-size trend, whereas for FC2, Q¯2
exhibits much larger variability—demonstrating that α provides a more stable and robust measure of layer quality.
6.5 Correlation Traps. Recall, from Section 3.3.1, that if a layer weight matrix W has
a Correlation Trap (and, in particular, those arising from SGD training with very large
40Prior work has shown that the AlphaHat metric (αˆ) accurately describes variations in model quality as a
function of architecture changes [24]. Since we do not vary the depth of the model in our evaluations, the Alpha
metric (α) is of interest in this setting.
79
(a) Batch size experiment (b) Learning rate experiment
Figure 14: Train / test errors in the MLP3 model as a function of batch size, and learning rate.
Observe the inverse relationship between batch size (a) and learning rate (b). As batch size
decreases, test error generally decreases, until batch size reached bs = 1. Similarly, as learning
rate increases, test error decreases until lr = 32× the SGD default value of 0.01.
learning rates) then it is likely that the test (and train) accuracy will be degraded, and α
will drop below its optimal value. See Section 6.5 for an empirical demonstration of this.
6.6 Overloading and Hysteresis Effects. The experiments described so far validate that
SETOL makes valid predictions in the α ≳ 2 range. Beyond that point, SETOL only predicts
“atypicality, in the sense of spin glass theory [118]. See Section 6.6 for an examination of
how the MLP3 behaves when it is pushed further out of that range of validity, e.g., by
training only one layer, while keeping the others frozen. In particular, we compare results
when a single layer is either over- vs under-parameterized.
We trained the MLP3 model independently, using both the Tensorflow 2.0 framework (using the
Keras api, and on Google Colab) and pytorch, with the goal of consistent, reproducible results.
Each setting of batch size, learning rate, and trainable layer was trained with 5 separate starting
random seeds, and error bars shown in plots below represent one standard deviation taken over
these 5 random seeds. Each training run used the same early stopping criteria on the train loss:
training was halted when train loss did not decrease by more than ∆Etrain = 0.0001, over a
period of 3 epochs. In doing so, each model was trained with a different number of epochs; and,
at the end, the best weights were chosen for the model. See Appendix A.5 for more details on
the MLP3 model and the training setup. We provide a Google Colab notebook with the exact
details, allowing the reader to reproduce the results as desired.
The dominant generalizing components of W reside in Wecs such that it captures the functional contribution of W to the NN; and thus
6.1 HTSR Phenomenology: Predicting Model Quality via the Alpha metric
Here, we want to determine how the quality of our MLP3 model varies with the Alpha metric.
From previous work [26, 24, 27], we expect that Alpha metrics for the FC1 and FC2 layers should
be well-correlated with the test accuracy, while varying some suitable training knob, such as
learning rate or batch size, that can modulate the test accuracy.41
41Since we do not change the depth of the model here, we expect the Alpha metric to follow the AlphaHat metric,
also predicting the test accuracies [24].
80
(a) αF C1 (b) αF C2
Figure 15: Train / test errors in the MLP3 model in the Learning Rate experiment as a
function of αF C1 (a) and αF C2 (b). Observe the regular downward progression of α and error
as the learning rate increased in both (a) and (b). When learning rate was 32×, (shown in red),
αF C1 fell below 2, coinciding with a drastic increase in both train and test error. The results here
almost perfectly replicate those of the Batch Size experiment, shown in Figure 16. for Figure 16.
In doing this, the goal is not to achieve Thermodynamic equilibrium, but, instead, by using a
common stopping rule, to simulate in more realistic situations where models may not have fully
converged. This allows us to test the theory outside of its more limited apparent range of validity,
thereby demonstrating the robustness of SETOL.
We vary the batch size from small to large, i.e., bs ∈ [1, 2, 4, 8, 16, 32], following the setup of
previous work on the HTSR phenomenology [25]. We expect similar effects by varying the learning
rate, as it is known that small batch sizes correspond directly to large learning rates [119, 120].
Thus, we conducted a second set of experiments where the learning rate was varied by a factor of
[1×, 2×, 4×, 8×, 16×, 32×], relative to the SGD default value of 0.01. Adjusting the learning rate
or batch size allows us to systematically vary the layer PL exponent α between roughly 2 and
4, i.e., within the range in which SETOL should make the most reliable predictions. As an added
benefit, it also allows us to use the very small batch size of 1 to force the model into a state of
over-regularization, which we also analyze below.
Consider Figure 14, which plots the final train and test accuracies as a function of the hyperparameter (batch size or learning rate) used during training for the MLP3 model. Figure 14(a)
varies batch size, and Figure 14(b) varies learning rate. Recall that error bars represent one
standard deviation taken over 5 independent starting random seeds. In Figure 14(a), we see
that by decreasing the batch size (bs), and holding other knobs constant, we can systematically
improve the train and test accuracy, up to a point. In particular, for bs ≥ 2, both the test and
train accuracies increase with decreasing batch size, consistent with previous work [25]. Further
decrease beyond bs = 2 leads to lower model quality, i.e., larger error and larger error variability.
In Figure 14, we see that increasing the learning rate (lr) by a factor x has an exactly analogous
effect as decreasing the batch size by 1/x.
42
The transition between lr = 16× (or bs = 2), which is locally optimal for the setting of other
hyperparameters, and lr = 32× normal (or bs = 1), which is not, provides a demonstration of a
distinct change in the behavior of Alpha, concordant with the sudden increase in the error and
error variability. To explore this in the context of SETOL, consider Figure 15 and Figure 16, which
42One could, of course, mitigate this by fiddling with other knobs of the training process, but that is not our
goal. Our goal here is not to use a toy model to demonstrate the properties and predictions of SETOL.
81
(a) αF C1 (b) αF C2
Figure 16: Train / test errors in the MLP3 model in the Batch Size experiment as a function
of αF C1 (a) and αF C2 (b). Observe the regular downward progression of α and error as the batch
size decreased in both (a) and (b). When batch size was 1, (shown in red), αF C1 fell below 2,
coinciding with a drastic increase in both train and test error. The results here almost perfectly
replicate those of the Learning Rate experiment, shown in Figure 15.
plot error as a function of Alpha, for different learning rates and batch sizes, respectively.
Figure 15 plots the Alpha metrics αF C1 and αF C2, as learning rate is varied, demonstrating
that both metrics are well-correlated with the test accuracies, for all learning rates less than 16×
normal. In particular, as we drive Alpha in FC1 down to an Ideal value of α ≃ 2, the test error
decreases monotonically (Figure 15(a)). Beyond that point, further decrease of the batch size sees
Alpha decrease below its Ideal value of 2 in the FC1 layer. This corresponds not only with higher
errors, but also with larger error bars, on both train and test error. The dramatic increase in
train error and train error variability is particularly telling, because it suggests that when αF C1
passes below 2, the model enters into a “glassy state, and is unable to relax down to 0 train error.
In Figure 15(b), we consider Alpha for FC2, and we see that αF C2 approaches 2, but does
not reach it, even for lr = 32×. This failure to achieve αF C2 ≈ 2, along with the much greater size
of FC1, (See Table 10,) suggests that FC1 is the critical layer for the models performance. This
also highlights some of the interplay between the layers, (which is not captured by a single layer
theory) – as αF C1 has narrow error bars throughout, αF C2 shows much more variation by way
of its wider error bars. Thus, while model accuracy kept improving as learning rate increased up
to 16×, this was likely driven by a better αF C1, more than by αF C2.
In Figure 16, we consider batch size, and we see a near identical replication of these results,
in terms of the relation of train error, test error and Alpha in the two layers. Consequently,
the remainder of the experiments will focus on the learning rate experiment, as both produced
substantially the same results.
6.2 Testing the Effective Correlation Space
Here, we will address the question:
How shall we test the assumption of the Effective Correlation Space?
Recall that the SETOL theory estimates model quality by evaluating the ST Generalization Error
as an integral over the theoretical training data ξ. This integral assumes each layer weight matrix
can be replaced with an effectively lower rank form, i.e., W → Wecs, corresponding to the span
82
of the eigencomponents defined by the tail of the ESD, ρtail(λ). In the HTSR phenomenology,
the tail is defined by the fact that ρtail(λ) follows a PL distribution, above some minimal value
λmin. In our SETOL theory, the tail is defined by choosing the minimal value λmin to satisfy the
Empirical ERG condition. These methods of realizing Wecs are essentially Model Selection Rules
(MSRs) for the Effective Correlation Space. Importantly, in neither approach is λmin just some
“rank parameter to be chosen by yet some other MSR on the basis of rank, or magnitude alone,
(that, in particular, does not know about HTSR or SETOL, which consider the shape of the ESD).
Thus, to test the assumption of the Effective Correlation Space, we want to show that the
models predictions are in fact controlled predominantly by the tail, where the specific choice of the
rank parameter depends on HTSR or SETOL as we expect. We can emulate this theoretical construct
and estimate (trends in the) test accuracies by evaluating the train and/or test accuracies of the
trained MLP3 model – after replacing the MLP3 layer weight matrices WF C1 and WF C2 with a
low-rank approximation consisting of only the tail:
Wecs
F C1
∶= PtailWF C1
Wecs
F C2
∶= PtailWF C2,
where Ptail is a projection operator selecting only the tail of the ESD with TrucnatedSVD. (That
is, we will use the low-rank TruncatedSVD approximation at the inference step, not at the training
step, as is more common.) A Truncated model is one whose weight matrices W∗ have been
replaced by truncated matrices Wecs
∗
. We denote the difference between the original models
accuracy and the Truncated models train and test accuracy as ∆Etrain and ∆Etest, respectively:
∆Etrain ∶= Etrain(D) − E
ecs
train(D)
∆Etest ∶= Etest(D) − E
ecs
test(D),
where E
ecs
train denotes the error of the TruncatedSVD model on the training portion of the dataset
D,a and E
ecs
test denotes corresponding test error for the TruncatedSVD model.
The PowerLaw and ERG Model Selection Rules If we use good MSRs, then we expect that
∆Etrain → 0 and ∆Etest → 0 as the models approach Ideal Learning. We consider the following
MSRs,43 which are associated with the HTSR and SETOL approaches.
• The PowerLaw MSR: All eigenvalues lying in the tail of the ESD, λi ≥ λ
P L
min, where λ
P L
min is
the start of the PL tail, as determined by the WeightWatcher PL fit, which is based on [68].
• The ERG MSR: All eigenvalues lying in the tail of the ESD, such that they satisfy the ERG
Condition, i.e., λi ≥ λ
∣detX∣=1
min , where ∏ λ
i∶λi≥λ
∣detX∣=1
min
≃ 1.
6.2.1 Train and test errors by epochs
To see how the Effective Correlation Space forms, we plot how ∆Etrain and ∆Etest evolve over
training, for each of the various learning rates considered.44
We start with the effect of the PowerLaw MSR. See Figure 17, where we see that ∆Etrain and
∆Etest generally trend downwards as they approach minimum train error. When the learning
rate is larger, the models converge more quickly, and ∆Etrain and ∆Etest also converge to lower
43We considered other MSRs that do not “know about HTSR or SETOL, but they (expectedly) perform in trivial
or uninteresting ways for testing the assumption of the Effective Correlation Space. Thus, we are not introducing
just some arbitrary low-rank approximation, as is common, but instead that the specific SETOL-based MSR matters.
44When batch size was varied, results did not significantly differ, and so they are omitted.
83
(a) lr = 1× (b) lr = 2× (c) lr = 4×
(d) lr = 8× (e) lr = 16× (f) lr = 32×
Figure 17: ∆Etrain (blue) and ∆Etest (orange) for various learning rates, using the
PowerLaw MSR. As learning rate increases we can see that ∆Etrain and ∆Etest both tend towards lower asymptotic minima, which they reach after fewer epochs of training. We can also see
that (after the first few epochs,) ∆Etrain (blue) is always higher than ∆Etest (orange). Observe
that in the bottom row (d–f) the yaxis is contracted to make the variation more visible. In (f) we
can see that as learning rate surpasses its optimal setting, the gap between ∆Etrain and ∆Etest
begins to increase again, and has wider error bars, suggesting that the excessively large learning
rate is disrupting the MLP3s ability to lear the Effective Correlation Space.
values. Recall from Figure 14(b) that lr = 16× had the lowest test error. In Figure 17(e), we
see that it also has the lowest ∆Etrain and ∆Etest. A lower ∆Etrain or ∆Etest means that more
of the models correct predictions are due to the low rank tail, meaning that the tail generalizes
better, and we see here that when the tail generalizes best, the model was the most accurate.
In each plot, we also see that the error bars are wide early on, before suddenly becoming
much narrower. This transition is more visible in the larger learning rates shown in 17(d)–17(f),
but can also be seen in 17(a)–17(c), albeit less clearly. Most interestingly of all, this transition
is preceded by a brief period, sometimes a single epoch, in which the error bars are drastically
wider, in a way that is reminiscent of a first-order phase transition. Again, this phenomenon can
be seen most clearly in 17(d)–17(f).
We next consider the effect of the ERG MSR. See Figure 18, which also shows the development
of ∆Etrain and ∆Etest over epochs, where we see a very different pattern in the train error and
test error. The difference in the train error is because, as the model is untrained in the early
epochs, the ERG MSR over-estimates the tail by choosing a λmin that is too small. Thus, ∆Etrain
actually increases to its asymptotic value at the final epoch. In the earliest epochs, the truncated
train error is even less than the full MLP3 models error, suggesting that signal is forming in the
large eigenvalues in these early epochs, but is swamped by the randomness of the early initial
weights, some of which is then removed by truncation. As epochs progress, this effect disappears.
Here again we can see the “phase-transition-like behavior of the train error, as the error bars are
84
(a) lr = 1× (b) lr = 2× (c) lr = 4×
(d) lr = 8× (e) lr = 16× (f) lr = 32×
Figure 18: ∆Etrain (blue) and ∆Etest (orange) for selected learning rates, using the ERG MSR.
For all learning rates, ∆Etest is centered on 0, meaning that the ERG Effective Correlation Space
explains almost all variation in out-of-sample predictions, but it does not explain all of the training
set predictions (blue). NOTE: The y axis is the same in all plots, and is much narrower than in
Figure 17. In (a–e) we can see that ∆Etrain converges to approximately 0.002. Compare with
Figure 17(e), which reaches a minimum of approximately 0.0025. As in Figure 17(f), the learning
rate of 32× normal disrputs the MLP3s ability to discover the ERG Effective Correlation Space.
wide early on, up to a transition having an abnormally large error bar, after which they stabilize.
Perhaps most interestingly of all, we see that under the ERG Model Selection Rule (MSR),
∆Etest is flat throughout training, and for all learning rates. This implies that there is no point
in training where the ERG tail generalizes badly. In other words, almost none of the out-of-sample
variance falls outside of the Effective Correlation Space, as the ERG MSR over-estimates the
Effective Correlation Space. It also bears noticing that under the PowerLaw MSR, (Figure 17,) this
is decidedly not the case, because the PowerLaw MSR under-estimates the Effective Correlation
Space. As we will see in 6.3, as Alpha approaches 2, the gap between the two MSRs goes to 0.
There are two final points of comparison between Figures 17 and 18. First, although it appears
in Figure 18 that ∆Etrain converges to a larger value than in Figure 17, this is because the scale
of the y-axis is 10× smaller. That is, the PowerLaw MSR is biased towards over-estimating λmin,
which means it over-truncates, producing a larger ∆Etrain or ∆Etest than the ERG MSR, which is
biased towards under-estimating λmin. Second, in both Figure 17 and 18, we can see that ∆Etest
is consistently lower than ∆Etrain. Clearly, truncating has a larger effect on train predictions,
meaning that no matter how long the model is trained, some of the train predictions are still
derived from the bulk. The fact that they remain is evidence that the gradient does not degrade
them, meaning that they were most likely contributing spuriously correct answers. Yet, the test
predictions are far less affected, meaning that only the Effective Correlation Space contributes to
the models ability to generalize.
85
Figure 19: Train and test error gaps using the PowerLaw MSR, as a function of alpha in the
FC1 and FC2 layers of MLP3 models, at the final epoch of training. We can see that as alpha
decreases towards 2, (right to left), ∆Etrain and ∆Etest generally decrease as well, meaning that
the closer Alpha is to 2, the more the Effective Correllation Space explains the train and test
predictions. The gap between un-truncated train and test error, (Eqn.84,) decreases as well until
αF C1 < 2.
6.2.2 Truncation and Generalization
Given that ∆Etest is always lower than ∆Etrain for the PowerLaw MSR, and similarly for ERG after
a certain point in training, it is clear that the Effective Correlation Space has something to do
with generalization. However, this leaves open the question of what role precisely Alpha plays.
In Figure 19, we plot ∆Etrain and ∆Etest with the PowerLaw MSR as a function of Alpha (rather
than epochs) for layers FC1 and FC2, as well as the Generalization Gap– that is, Etest − Etrain.
(Recall Eqn. 84.) Learning rate is not explicitly shown, but its effect can be seen in the clusters
of points that each learning rate generates.
In both layers, ∆Etrain and ∆Etest steadily decrease with αF C1, until it passes below 2, after
which the relation deteriorates somewhat. This is especially prominent in FC2. Recall from Figure 15, Section 6.1, that when αF C1 passed below 2, the train error and test error both increased
and exhibited larger variability. From this we interpret Alpha as a measure of regularization
(which is consistent with its introduction as a measure of implicit self-regularization [25]). Regularization has the effect of keeping train and test accuracy closer together, and generally, as
Alpha in the dominant layer decreases towards 2 from above, the train-test error gap decreases.
6.3 Evaluating the ERG Condition
Having established that the PL tail of the ESD, defined by eigenvalues above λ
P L
min, is a major
factor in determining model quality in the MLP3 model, we now examine how well the ERG
Condition compares with it. In particular, we demonstrate that when the tail of a layer ESD is
described well by the HTSR phenomenology, i.e., when it is well-fit by a PL with ρ(λ)tail ∼ λ
−α
,
with PL exponent α ≃ 2, then the eigenvalues in the tail defined by the PL fit, i.e., λ ≥ λ
P L
min, also
satisfy the ERG Condition of Eqn. 126—a key assumption of the SETOL theory. This is a rather
remarkable empirical result that couples HTSR and SETOL; it has its basis in our SETOL derivation;
and it provides the basis for an inductive principle that is based on the product of eigenvalues
rather than an eigenvalue gap.
We can denote the eigenvalue that best fits the ERG Condition as λ
∣detX∣=1
min . Then, to measure
86
(a) LR = 8× (b) LR = 16× (c) LR = 32×
Figure 20: Log-Linear ESDs for three learning rates in the FC1 layer of MLP3. The red line
shows λ
P L
min, and the purple line shows λ
∣detX∣=1
min . Observe that the purple line is to the left of the
red line, but as the LR increases they move closer together. However, when LR is 32×, where
both train and test accuracy suffered (c), the red line is to the left of the purple line. This is
often a signature of an Over-Regularized layer, and indeed the FC1 layer in this model had α < 2.
(See Figure 15(a) in Section 6.1.)
how well this condition holds, we can compute
∆λmin = λ
P L
min − λ
∣detX∣=1
min . (164)
In Sections 6.3.1 and 6.3.2, we will see the trend that as α approaches 2, λ
P L
min and λ
∣detX∣=1
min
also approach one another, and hence ∆λmin goes to 0, from above, both for our toy MLP3
model as for SOTA models. In our MLP3 model, we will see that a crossing of the equality
condition coincides with over-regularization and a degradation in model accuracy. In pre-trained
ResNet[121], VGG[122] and ViT[123] models, we will also see, empirically, that in general ∆λmin
remains positive, just as α remains above 2.
6.3.1 The MLP3 model
Consider Figure 20, which shows λ
P L
min and λ
∣detX∣=1
min in the FC1 layer of three MLP3 models, each
sharing a common starting random seed, that were trained with the largest learning rates. The
λ
P L
min and λ
∣detX∣=1
min eigenvalues are marked by red and purple vertical lines, respectively; and thus
∆λmin is the distance between red and purple lines. As learning rate increases, the red and purple
lines draw closer, and they are closest for lr = 32× (Figure 20(c)). (Compare this with Figure 15,
Section 6.1, which shows that this corresponds with an increase in test accuracy, up to lr = 16×,
but at lr = 32× Alpha fell below 2 and accuracy suffered.) In Figure 20(a)-20(b), the purple line is
left of the red line; but in Figure 20(c), the red and purple lines cross, such that λ
P L
min < λ
∣detX∣=1
min .
This is analogous to the case where α crosses below 2. This suggests that the absolute ERG is
minimized when α ≃ 2, which (remarkably) is exactly when the HTSR phenomenology predicts the
layer is Ideal.
(Observe that Ideal does not necessarily mean optimal under a finite sized training set, but
rather that the finite-sized system behaves the most similarly to its infinite limit.)
We can compare α and ∆λmin more broadly by plotting ∆λmin directly as a function of α in a
single plot spanning all random seeds and learning rates or batch sizes. This is shown in Figure 21.
Critical values of α = 2 and ∆λmin = 0 are shown as vertical and horizontal red lines, respectively.
Values for various learning rates are plotted for layer FC1 (Figure 21(a)) and FC2 (Figure 21(b)),
as well as for various batch sizes in layer FC1 (Figure 21(c)) and FC2 (Figure 21(d)).
87
(a) Layer FC1 for various learning rates (b) Layer FC2 for various learning rates
(c) Layer FC1 for various batch sizes (d) Layer FC2 for various batch sizes
Figure 21: MLP3 Model: Comparison of the PL Alpha (x-axis), with the difference between
λ
P L
min and λ
∣detX∣=1
min (y-axis). The thin red lines indicate critical values of α = 2 and ∆λmin = 0. As
learning rate increases (a–b) or batch size decreases (c–d), we can see that in layer FC1, which
dominates the model, (See Table 10,) α goes to 2, and ∆λmin goes to 0. Observe that both
critical values are crossed at the most extreme hyper-parameter selection, (red,) corresponding
with over-training. Layer FC2 shows a weaker tendency towards the critical values (b, d), and is
disrupted at the most extreme hyper-parameter values (red).
For layer FC1 (Figures 21(a) and 21(c)), in both cases we see near-linear march towards
the critical tuple of (α, ∆λmin) = (2, 0). In addition, passing this critical value coincides with
diminished train and test accuracy, (recall Figure 14), suggesting that just as α = 2 is a threshold of
over-regularization, λ
P L
min < λ
∣detX∣=1
min may be as well. Since FC1 is the dominant layer, comprising
roughly 8/9 of the weights of the model, (Table 10,) we expect FC1 to most closely match the
performance of the model as a whole.
For layer FC2, which comprises roughly the other 1/9 of the models weights, there is a similar
coevolution, but it is weaker. As learning rate or batch size exceeds their critical values, rather
than going to (2, 0) as in FC1, we instead see that the relationship simply breaks down, with the
gap growing larger even as α decreases. Given that FC1 has passed the critical α = 2 threshold, we
conjecture that the breakdown of the relationship between α and ∆λmin is due to FC1 becoming
atypical.
88
6.3.2 State-of-the-Art (SOTA) models
Here, we consider SOTA models, in particular VGG pre-trained models [122], the ResNet series [121], the ViT series [123], and the DenseNet series [124]. We show that as α approaches 2,
the Log-Trace Condition holds better and better, i.e., ∆λmin approaches 0.
Figure 22 plots α versus the difference ∆λmin, (Eqn. 164). Layer matrices from all models
in each series are pooled to generate the plots.45 Notice that in Figure 22(a) – 22(d), ∆λmin
approaches zero as α → 2 from above. Individual points may have a large ∆λmin for an α near to
2, but the overall trend is apparent. The rapid decrease of ∆λmin as α approaches 2 from above
implies that the PL tail rapidly takes on a unit ERG (if it doesn’t already have it.)
As we saw in comparison of Figures 17 and 18, (Section 6.2.1,) the ERG tail is generally larger,
and always has highly generalizing components. Thus, it is plausible that as layers reach the limit
of the amount of information that can be encoded in them, i.e. as α goes to 2, the PowerLaw tail
expands to fill the ERG tail. This effect can be seen clearly in Figure 22 in the condensing of the
“funnel shape.
Recall from Figure 21 that layer FC1 dominated the model, (Table 10,) producing a clear
progression of α and ∆λmin towards (2, 0), as a function of learning rate or batch size, whereas
FC2 showed a slightly less clear relationship. In larger models having dozens or hundreds of layers,
we would not expect any one layer to dominate as thoroughly. Moreover, it is the architecture
that varies between models in each series, not (necessarily) the hyperparameters, meaning that
there would not be a straight line, as in Figures 21(a) and 21(c). However, with all of the layers
contributing to varying degrees, we nevertheless see a clear trend in all plots of Figure 22. These
results show how the single-layer SETOL theory extends from the MLP3 model, which is dominated
by a single layer, to larger models where many layers interact in complex ways, but still reflect
the same overall trend.
The overall pattern of relationship between ∆λmin and α can also be seen in Figure 23, which
shows plots for Large Language Models (LLMs) of the Falcon [125] and LLAMA [126] model
families, for different numbers of parameters. Observe that each subfigure 23(a)–23(d) shows
a single model, rather than a collection of models in a family, as in Figure 22. The y-axis is
the same between models in the same family. As in Figure 22, there is a general outline of a
“funnel shape pointing towards the critical point (2, 0), with the exception that it is only reached
in the case of LLAMA-65b, (Figure 23(d)). This suggests that these LLMs are larger than they
necessarily need to be, consistent with prior work [80], but also that they are well guarded against
Over-Regularized layers beyond the critical point (α = 2 and ∆λmin = 0).
6.4 Layer Qualities with Computational R-transforms
In this Section, we look at how the HTSR layer quality HT PL metric α compares to computing the
Layer Quality-Squared Q¯2 using the Computational R-transform method proposed in Section 5.5
for fully trained MLP3 model(s). Figure 24 presents results for FC1 and FC2 layers, comparing
both the batch size and the mean α metric to the mean Q¯2
, and the results are quite different.
For FC1, as shown in Figure 24(a), the quality metric Q¯2
increases as the batch size decreases,
following the expected trend. Notably, for batch size = 1, the quality metric exceeds 100%, which
suggests that the layer may be overfit in this scenario, which is similar to results obtained earlier.
Furthermore, in Figure 24(b), the quality metric Q¯2
shows a strong correlation with the α metric,
45For convolutional layers, the WeightWatcher tool first computes eigenvalues for all channels-to-channels linear
operators separately, and then pools them in order to compute α, λ
P L
min and λ
∣detX∣=1
min . For instance, in a 64×64×3×3
weight tensor, there would be 9 separate linear operators of 64 × 64, giving 576 eigenvalues, which would then be
pooled to compute α, λ
P L
min and λ
∣detX∣=1
min .
89
consistent with the theoretical predictions of the HTSR framework. Earlier results suggest that
the FC1 layer captures most of the correlation in the data, so it is interesting that this layer also
has much smaller error bars, indicating more consistent quality across different batch sizes.
In contrast, for FC2, while the general trends are similar (as shown in Figures 24(c) and 24(d)),
the error bars on the quality metric Q¯2
are much larger. This indicates significantly higher
variability in computational quality compared to FC1. The large error bars for FC2 suggest
that, while the Q¯2 metric is theoretically grounded, it is less effective than the existing HTSR
layer quality metric α, which exhibits greater stability and reliability in capturing layer behavior.
These differences highlight the distinct computational characteristics of the two layers, with FC1
demonstrating more predictable trends that align closely with theoretical expectations.
6.5 Inducing a Correlation Trap
Previous work on the HTSR phenomenology [26, 24] has shown that one can look for quantitative
deviations from the necessary pre-conditions of tradtitional RMT (particularly that the weights
are 0-mean and finite variance) to detect when a model layer suffers from some other anomaly in
the elements, which we call a “Correlation Trap (see Section 3.3.1 and [26, 24]). A Correlation
Trap may cause, or be caused by, the over-regularization leading Alpha to fall below 2, (see
Section 3.3.2). Here, we explore this in greater detail, in light of our SETOL.
Lets look at the ESDs of the FC1 layer of the MLP3 model, for learning rates lr = 16×
and lr = 32× normal. We will be interested in the general shape of the ESD of 1
NWTW. For
the purposes of detecting a Correlation Trap, we will randomize WTW element-wise, and then
observe its largest eigenvalue λ
max
rand.
Figure 25 shows the ESDs of the original matrix (green) and the element-wise randomized
rand(
1
NWTW) (red). Observe in particular λ
max
rand for each learning rate factor. For lr = 16×,
Figure 25(a) shows that the ESD of 1
NWTW is HT, whereas the ESD of the randomized matrix is
essentially a distorted semi-circle—as expected from the well-known MP result; and that λ
max
rand lies
at the edge of the random MP Bulk ESD. (A similar result is seen for smaller learning rates.) In
contrast, for lr = 32×, Figure 25(b) shows that while the original ESD is again HT, the ESD of the
randomized has one large element, λ
max
rand, that pulls out from the MP bulk. This is the signature
of a Correlation Trap; and it co-occurrs with the exact learning rate setting that degraded the
train and test accuracies, pushing Alpha below its optimal value of α ≃ 2. When this happens,
both the estimation of Alpha and the formation of a PL tail are potentially disrupted.
Correlation Traps have been observed previously [26, 24], using the HTSR phenomenology.
However, SETOL provides an explanation for why this would be expected to occur — non-standard
element-wise distributions will tend to interfere with the properties of the spectrum which SETOL
analyzes. Our derivation in Section 6 suggests that in order to apply the SETOL effectively one
must avoid (or remove) such traps. This too has been observed previously [26, 24].
6.6 Overloading and the Hysteresis Effect
Obtaining a value of α outside the α ≳ 2 range is indicative of “overloading [8] that layer. This
can be accomplished, e.g., by training only one layer in the MLP3 model. As the two layers have
very different sizes, we see markedly different behaviors, that are nevertheless consistent with
theory. In particular, this lets us test the theory in both the Strongly Over-Parameterized Regime
(n ≫ N × M) and the Under-Parameterized Regime (n ≪ N × M)
The SETOL theory is based on the idea that NNs undergoing training behave like Statistical
Mechanic systems relaxing to an equilibrium. So far, we have tested the theory under conditions
that are approaching Ideal. However, for the theory to be useful in practice, we must also
90
examine how it performs in non-Ideal situations. Of particular interest, we would like to examine
the theory under conditions where the training dynamics slows down, i.e., when it is in a “glassy”
or meta-stable state. One way we can do this is to train only one layer, and freeze the rest. Doing
so overloads the single trainable weight matrix, as a function of the ratio of examples to trainable
parameters [8, 6, 93], and we expect this to cause αF C1 or αF C2 to drop well below 2. As in 6.2,
we will be examining this effect over the entire course of training.
6.6.1 Baseline: Loading onto both FC1 and FC2
To start, Figure 26 shows αF C1 and αF C2, binned in units of 0.05, versus train and test error over
all epochs of training 46 for each of the different learning rates considered. (Cf. Figures 15 and 16,
Section 6.2, which show only the final epoch.) Binning was done so as to facilitate averaging over
the 5 starting random seeds; linear fits are shown separately for each learning rate; and error
bars represent one standard deviation within each bin. The critical value of α = 2 is shown as a
vertical red line in all plots. For each learning rate, train error and α decrease together during
training, which can be seen for both αF C1 (a) and αF C2 (c), reading each line from the top right
to bottom left. Test errors, (b, d) show a similar trend, but with wider error bars. Observe that
the range of the y-axis is narrower for test error to make detail more visible.
We see in Figure 26 (a,b) that the 32× learning rate causes αF C1 to decrease faster than any
other, putting it on course to fall below 2 before train error reaches ≃ 0. (See Figure 14, at the
beginning of this Section.) We also see that the slower learning rates cause train error to reach
≃ 0 well before αF C1 can reach 2, and explains as to why their test error was higher.
6.6.2 Overloading FC1: Strongly Over-Parameterized Regime (n ≫ N × M)
In contrast, when only FC1 is trained, Figure 27 shows that, as expected, αF C1 decreases well
below 2. Training error (a) generally trends downward as αF C1 decreases, but no matter the
learning rate, the relation is the same, or nearly so, because only one layer is being trained.
Consequently, all learning rates were pooled to produce one linear fit for enhanced visibility.
In Figure 27 we can see demonstration of a crucial claim of SETOL theory: that for αF C1 > 2,
(vertical red line,) the test error declines linearly; (b) shows that test error is almost perfectly
linear with decline in αF C1. However, when αF C1 < 2, the curve bends upward. Furthermore,
the precision with which the trajectory changes as αF C1 passes the threshold provides ample
validation that the estimator of [68] is indeed accurate. We observe that test error may continue
to decrease after αF C1 < 2, however the rate of decrease is significantly less. We can also see that
in some sense, the model is “doomed” to always have train error reach ≃ 0 when αF C1 ≃ 1.7, i.e.,
after α = 2, because of the number of trainable parameters, and perhaps because of the lack of a
modulating influence of FC2 seen in Figure 26.
6.6.3 Overloading FC2: Under-Parameterized Regime (n ≪ N × M)
The MNIST dataset has 60, 000 training examples, which means that an FC1-only model is overparameterized, but FC2 is substantially under-parameterized [127]. (Table 10) This drastically
changes the meaning of an experiment where only FC2 is trained. Figure 26 (Section 6.6.1) shows
αF C2 vs. train error (c) and test error (d) when all layers are trained. There we see a different
relationship between αF C2 and train and test error for each learning rate. None of the training
runs reached an αF C2 of 2, as the load was split between both layers FC1 and FC2.
46Excluding the first four epochs when the matrix is still essentially random.
91
Figure 28, however, shows a starkly different relationship. When only FC2 is trained, each
random seed produces a different trajectory, meaning that they cannot be interpretably plotted
as a single curve, even with error bars. Train (a, c) and test (b, d) error rates are shown as
a function of αF C2 for the two highest learning rate factors, 16× (a, b) and 32× (c, d). Lower
learning rate factors, (not shown,) showed the same trend, except that they did not progress as
far as those shown in Figure 28.
First we see, for all seeds, αF C2 decreases all the way down to 1.5, after which it begins to
rebound to the right. As it does, train error continues to decrease. We can also see that test
error continues to decrease as well, down to its minimum value of slightly more than 0.03, as
αF C2 continues to increase for a short time. However, at the exact point where test error reaches
a minimum, the PL tail itself begins to fracture, leading to different estimates of αF C2 for each
seed. As each of the five starting random seeds are shown separately, we can see that each of
them terminates at a different point, some of which are closer to 2 than others.
Such a reversion to a more typical value of αF C2, prior to the fracturing of the PL tail, is
reminiscent of a spin glass system relaxing towards its minimum energy configuration, in a way
that retains some memory of the path taken on the way to its current state. We conjecture that
if the model were trained for sufficiently many epochs, (perhaps many thousands,) then the tail
would re-form, and αF C2 would reform, and revert all the way back to its stable value of 2.
92
(a) VGG Series (b) ResNet Series
(c) ViT Series (d) DenseNet Series
Figure 22: Difference between the two λmin estimates, ∆λmin, (Eqn. 164), as a function or α,
for linear and convolutional layers in series of VGG [122], ResNet [121], ViT models [123] and
DenseNet models [124]. Layer matrices for all models in the series were pooled to create each
plot. In (a), (VGG,) we see three clusters of points – those with ∆λmin close to 0 and α close
to 2, those with ∆λmin above 2 and α > 2.5, and those with ∆λmin above 6 and α > 3.5. In (b),
(ResNet,) we see that in general, as α shrinks towards 2, ∆λmin tends towards 0, overshooting
slightly. We also see that the difference ∆λmin is almost always positive, with few exceptions, and
even the layers that do not overshoot form a kind of “funnel shape pointing towards the critical
point (2, 0). In (c), (ViT,) we also see the same general relationship between α and ∆λmin across
layers of several ViT models. Observe that ViT models do not have convolutional layers, and in
spite of this, the overall pattern is similar. In (d), (DenseNet,) we see a similar overall trend as
in (b), except that ∆λmin tends to decrease sooner, but there are also more layers with α < 2 and
∆λmin above 0.
93
(a) Falcon 7B (b) Falcon 40B
(c) Llama 13B (d) Llama 65B
Figure 23: Difference between the two λmin estimates, ∆λmin = λ
P L
min − λ
∣detX∣=1
min , as a function
of α, for all linear layers in the FALCON[125](a-b) and LLAMA [126](c-d) language models for
varying numbers of parameters. As in Figure 22, we see that in recent Large Language Models,
∆λmin remains positive, except where α < 2 (d). Otherwise, a “funnel shape can still be seen
leading towards the critical point (2, 0) as in Figures 22(b) and 22(c) Observe that the x- and
y-axes are different between sub-figures due to the differences in scale of each model.
94
(a) FC1 batch size vs Q¯2
(b) FC1 Layer α vs Q¯2
(c) FC2 batch size vs Q¯2
(d) FC2 Layer α vs Q¯2
Figure 24: Evaluation of the computational R-transform Layer Quality-Squared metric Q¯2
on
the fully trained MLP3 model(s) for different batch sizes.
(a) MLP3 Learning Rate 16× (b) MLP3 Learning Rate 32×
Figure 25: ESD plots for learning rate lr = 16× and lr = 32× normal, shown on Log-Lin scale, as
computed using the WeightWatcher tool, for the FC1 weight matrix W (green) and an elementwise randomized rand(W) (red). This provides an example of inducing a Correlation Trap in the
MLP3 model, simply by increasing the learning rate used during model training. See Section 3.3.1.
95
(a) (b)
(c) (d)
Figure 26: Train (a, c) and test (b, d) accuracy as a function of αF C1 (a, b) and αF C2 (c, d) when
all layers are trained. Red vertical lines show the critical value of α = 2, and dashed black lines
show linear fits of error, using only points where α > 2 and train error > 0.001. For FC1 (a, b),
we can see that each learning rate produces a different trajectory of train error (a) and test error
(b) as a function of αF C1, showing that even though FC1 dominates overall, (Table 10,) FC2 still
plays a modulating role. (Cf. Figure 27 where there is only one trajectory.) In (c, d) we can see
that αF C2 never goes below 2. As in (a, b), each learning rate produces a different trajectory,
though there is greater overlap for lower learning rates. See discussion in Section 6.6.1.
96
(a) (b)
Figure 27: Train and test accuracy as a function of αF C1 when only FC1 is trained (a, b). Red
vertical lines show the critical value of αF C1 = 2, and dashed black lines show linear fits of error,
using only points where αF C1 > 2. In contrast with Figure 26, when only FC1 is trained, we
can see that no matter the learning rate, there is ony one trajectory, for both train error (a) and
test error (b). Hence, for visibility, only one linear fit, using all learning rates pooled, is shown.
Crucially, we can see in (b) that as αF C1 passes below 2, the test error trajectory changes, for
all learning rates, even as the train error trajectory does not, until it reaches ∼ 0. This suggests
that even though test accuracy can still decrease when αF C1 < 2, it does so at a decreased rate
relative to αF C1. See discussion in Section 6.6.2.
97
(a) (b)
(c) (d)
Figure 28: Train error (a, c) and test error (b, d) as a function of αF C2 when all other layers
are frozen, for the two largest learning rates, lr = 16×, (a, b) and lr = 32×(c, d). Cf. Figure 26,
wherein all layers were trained. Due to the markedly different behavior of each random seed, they
cannot be plotted as means and error bars, and are instead shown separately. The path taken
by all seeds up to αF C2 = 1.5 has a slight curvature characteristic of a hysteresis-like behavior.
Observe also that the fragmenting into separate paths, due to the breakdown of the PL tail,
coincides roughly with each seeds path reaching its minimum test error. The y-axis is scaled
differently for train and test error to make variation more visible. See discussion in Section 6.6.3.
98
7 Conclusion and Future Directions
In this work, we have introduced SETOL, a Semi-Empirical Theory of (Deep) Learning that unifies concepts from Statistical Mechanics (StatMech), Heavy-Tailed (HT) Random Matrix Theory
(RMT), and quantum-chemistry-inspired approaches to strongly correlated systems [53, 54, 55].
SETOL aims to provide a solid theoretical foundation for the Heavy-Tailed Self-Regularization
(HTSR) phenomenology, including the widely used Alpha and AlphaHat HT Power Law (PL) Layer
Quality metrics, which are implemented in the open-source WeightWatcher toolkit. Specifically,
SETOL reformulates the Neural Network (NN) learning problem for a single layer as a matrix generalization of the Student-Teacher (ST) model for Perceptron learning, where the Teacher is now
taken as empirical input, and the result is in Annealed Approximation (AA) at high temperatures
(high-T). This reformulation results in a Free Energy (i.e., Generating Function βΓ
IZ
Q¯2
) expressed
as an integral over random Student correlation matrices, commonly referred to as the HarishChandra–Itzykson–Zuber (HCIZ) integral. To evaluate this integral, we recast the solution using
a technique derived from first principles, analogous to a taking single step of the Wilson Exact
Renormalization Group (ERG) theory [56, 57]. Leveraging more recent results [83, 84], we express
the Layer Quality as a sum of integrated matrix-cumulants from RMT (i.e., R-transforms). Finally,
we conduct both direct and observational experiments to validate key assumptions of the SETOL
framework and empirically connect it to the HTSR theory.
Key Contributions and Observations.
A. Rigor for the HTSR Phenomenology. SETOL explains why power-law (PL) exponents in the
layer spectral densities (e.g. Alpha and AlphaHat) act as robust diagnostics of generalization,
even in large, complex architectures without access to training or test data. Our analysis ties
these Heavy-Tailed ESDs to a Scale-Invariant Volume Preserving Transformation associated
with an Effective Free Energy, and suggesting (in analogy with traditional StatMech phases
in learning theory) that the HTSR condition α ≈ 2 marks a phase boundary between optimal
generalization and overfitting.
B. Matrix-Generalized Student-Teacher (ST) Model. SETOL is formulated as Semi-Empirical
matrix generalization of the classical (vector-based) ST perceptron learning, taking the
Teacher now as empirical input, and incorporating N × M layer weight matrices, w → W.
Key to this generalization is isolating the top eigenvalue/eigenvector directions—called the
Effective Correlation Space (ECS)—before evaluating the resulting partition function (or
HCIZ integral). The ECS contains the HTSR PL tail, validating that the tail captures the
dominant layer generalizing components.
C. ERG Condition & Alpha = 2. A remarkable empirical observation, predicted by SETOL, is
that layers near ideal training also satisfy a ∏ λ˜
i ≈ 1 in their tail eigenvalues (i.e., ∣detX∣ = 1);
equivalently, ∑ ln λ˜
i = ln ∏ λ˜
i ≈ 0. We call this the ERG condition (i.e, the detX option in
WeightWatcher). Empirically, this condition appears when the HTSR Alpha ≈ 2. SETOL
thereby unifies two previously separate heuristics for “optimal” or so-called Ideal behavior.
D. Empirical Validation of SETOL. To validate the ERG and the ECS conditions, we trained
small (3-layer) Multi-Layer Perceptron (MLP3) on MNIST and under varying batch sizes
and learning rates. Using this, we verified that when the HTSR α ≈ 2 the SETOL ERG condition
also (usually) holds, and that one can reproduce the training accuracy by retaining only the
ECS. This is further confirmed by using the WeightWatcher tool to examine the Alpha and
DetX metrics common, open-source CV and NLP models, including modern LLMs (ResNets,
DenseNets, ViTs, and LLMs like LLaMA and Falcon).
99
E. Correlation Traps & OverFitting. We observe that when layer ESDs with α < 2, they
exhibit behavior interpreted as over-regularization and/or overfitting. For example, we
observe what we call Correlation Traps, large rank-one perturbations in the (randomized)
layer weight matrix W that can be induced by training with excessively small batch sizes
(bs=1) and/or large learning rates, and are associated with degraded test accuracy, and
which cause the HTSR to drop to α < 2. These results are consistent with other results,
applying HTSR theory to understand Grokking[108].
Additionally, we can induce overfitting by freezing all but one layer and then training,
which causes the layer α < 2. By training in the underparameterized regime, we observe
path-dependent, “glassy” behavior.
F. OverParameterized and UnderParameterized regimes. These above results indicate that the
HTSR and SETOL approaches can be applied effectively in the overparameterized regime, even
in conditions beyond the apparent range of validity of the theory (α ≫ 2). In the underparameterized regime, they are less effective. It is noted that modern transformers like LLMs
may appear to be underparameterized, but the multiplicative nature of their interactions
suggests they are overparameterized. [128], making HTSR and SETOL very applicable for such
AI models.
G. Connection to Semi-Empirical Methods. Conceptually, SETOL parallels well-known SemiEmpirical methods in quantum chemistry [53, 54, 55], wherein complicated many-body
Hamiltonians are approximated by effective theories, but fitted or validated using empirical
data. By retaining only the largest spectral modes (ECS) and imposing the ERG condition,
we can describe crucial low-rank correlations while discarding less relevant interactions,
much like in Freed–Martin Effective Hamiltonian and/or Wilson’s Exact Renormalization
Group (ERG) approach [51, 94, 53, 54, 52, 55, 57].
ERG Analogy: A One-Step View. From a Renormalization Group perspective, restricting
to the measure on the partition function to the ECS is akin to performing a single step of the
Wilson Exact Renormalization Group (ERG). In doing this, we are discarding bulk “uninteresting” degrees of freedom in favor of the strongly correlated HT long-ranged modes. This leads to
an effective model with fewer degrees of freedom but renormalized interactions–interactions that
are dominated by the largest eigenvalues. This analogy with ERG theory suggests that the HTSR
phenomenology, where α ∈ [2, 6] in the Fat-Tailed Universality Class, is essentially describing a
near-critical phase when α ≈ 2 and satisfies Tr [ln λ˜] = ln ∏ λ˜
i ≈ 0 in its ECS. Departing from this
point (α < 2) leads to suboptimal results, consistent with the multi-phase pictures in StatMech
spin glass theories of learning [8, 6, 4, 5].
Toward Understanding “Why Deep Learning Works.” A key question in deep learning
theory is why large neural networks achieve strong generalization despite operating in highly nonconvex optimization landscapes. From the perspective of ERG theory, this phenomenon can be partially understood through the concentration of generalization-relevant components. Specifically,
models trained in regimes exhibiting Power-Law (PL) correlations lead to the emergence of effective low-dimensional descriptions, where irrelevant modes are suppressed. The WeightWatcher
Alpha metric quantitatively captures this concentration by characterizing how the optimization
landscape stabilizes near criticality, with α ≈ 2 signaling optimal generalization.
Student-Teacher Knowledge Distillation. SETOL also provides an explanation of why StudentTeacher distillation works so well. When training a Student DNN S to emulate a larger Teacher
100
T, the Student inherits the inductive biases and generalization behavior of Teacher, which SETOL
explains is concentrated in T into smaller (lower rank) layer weight matrices. Thus, the Student
converges to a similar region of the optimization landscape, shaped by the same spectral and
statistical constraints that govern the Teacher, allowing for a smaller model to effectively mimic,
if not reproduce, the Teacher’s outputs.
A Novel Way to Characterize Generalization. Many results that use the AA / HighTemperature limit effectively state that “with enough data and a wide enough NN, one can
learn anything”. But how much is “enough”? SETOL takes this one step further by giving useful
empirical measures that converge to the same Ideal, whose progress can be precisely tracked in
finite settings. Our experimental results show that this occurs both in a simple model, as well as
in large production models. These measures strongly indicate the point at which further learning
in a particular layer will be counterproductive.
Relation to Levy Spin Glasses and Heavy-Tailed Random Matrix Models. One longstanding puzzle is why large NNs avoid the worst of highly non-convex optimization, despite
nominal exponential degeneracies. SETOL offers a partial explanation: if the trained model has
Levy-like or VHT PL correlations, then typical spin-glass degeneracies can be lifted, leaving
the layer in a finite number of near-critical minima. In analogy with older work on Levy Spin
Glasses[40], it is proposed WeightWatcher Alpha metric effectively measures how rugged yet stable the effective energy landscape is for each layer, with α ≈ 2 signifying a sweet spot of Ideal
generalization.
7.1 Future Directions
1. Multi-Layer ERG and Layer Interactions. While SETOL is formulated per-layer, modern
DNNs stack many layers, each potentially with different α. An improved approach would treat
layer-layer interactions (relax the IFA). It is possible that certain layers (e.g. final fully-connected
heads in LLMs) exhibit α far from 2, while others converge near 2—raising questions about how
best to address or combine them.
2.. Practical Diagnostics and Fine-Tuning. The open-source WeightWatcher tool has
already seen success diagnosing layer quality. Integrating SETOL ’s ERG condition may refine this
further, helping users identify correlation traps or “under-exploited” layers. There is also strong
potential for using Alpha and ERG-based signals during training or fine-tuning in the training of
very large LLMs: e.g. automatically adapting learning rates to push each layer closer to α = 2[27],
for fine-tuning models with significantly less memory[129], for compressing large LLMs[30], and
other practical applications.
3. Correlation Traps and Meta-Stable States. Although our experiments show how small
batch sizes or large learning rates can induce correlation traps, a quantitative theory of where
and why traps occur remains open. Clarifying these states could enable trap-avoidance strategies,
e.g. partial re-initialization or specialized regularizers that favor lower-rank updates in the ECS.
In large-language-model (LLM) contexts, correlation traps might manifest as hallucinations or
mode collapse, motivating deeper analysis.
4. Analyzing the Layer Null Space. One critical but often overlooked factor is the potential null space within model layers, which can emerge during overfitting. This null space
represents parameter directions that fail to contribute meaningfully to generalization but instead
101
encode redundant or overly specific patterns tied to the training data which might be ignored
or forgotten. Future work should examine if and when NN layers have components in their null
space, which contribute significantly to the performance of the model.
5. Layer-Layer Cross-Terms. SETOL is a single layer theory, however, as noted in Section 5.1, it would be desirable to extend the theory to including layer-layer cross terms. While
we don’t have an exact expression for this, we can propose a phenomenological guess that the
leading order term would be the integrated R-transform, defined for the overlap between nearestneighbor weight matrices (i.e., W1,W2) that can be algined along a common axis. This term
would take the form GR1,2
(λ1,2) where R1,2 ∼ W⊺
1W2 and λ1,2 is an eigenvalue of R1,2. Note that
the open-source WeightWatcher tool can identify and compute the intra-layer interactions.[67]
Concluding Remarks. SETOL as a Semi-Empirical theory merges first-principles methods from
StatMech and RMT with empirical insights from HTSR and the open-source WeightWatcher tool.
It clarifies how Heavy-Tailed layer weight matrices can emerge from training on realistic data,
and why their spectral exponents so reliably predict generalization quality without peeking at
training/test sets. In so doing, SETOL not only offers new insights into the “why does it work”
question of deep learning but also suggests a roadmap for improving DNN models by focusing
attention on that near-critical subspace of their largest eigenvalues. We are optimistic that
future developments along these lines—extending the single-step Wilson Exact Renormalization
Group analogy, understanding layer-layer interactions expansions, and systematically diagnosing
correlation traps, and developing better lagre-scale training methods—will yield more robust,
data-free metrics for training, fine-tuning, and compressing next-generation neural networks.
Acknowledgments. We would like to thank Matt Lee of Triaxiom Capital and Carl Page of
the Anthropocene Institute. We also thank Mirco Milletarì and Michael Mahoney for helpful
conversations.
References
[1] John Jumper, Richard Evans, Alexander Pritzel, Tim Green, Michael Figurnov, Olaf Ronneberger,
Kathryn Tunyasuvunakool, Russ Bates, Augustin Žídek, Anna Potapenko, et al. Highly accurate
protein structure prediction with alphafold. Nature, 596(7873):583–589, 2021.
[2] Nobel Prize Organization. The nobel prize in physics 2024, 2024.
[3] Nobel Prize Organization. The nobel prize in chemistry 2024, 2024.
[4] A. Engel. Complexity of learning in artificial neural networks. Theoretical Computer Science,
265(1–2):285–306, 2001.
[5] A. Engel and C. P. L. Van den Broeck. Statistical mechanics of learning. Cambridge University
Press, New York, NY, USA, 2001.
[6] E Gardner. The space of interactions in neural network models. Journal of Physics A: Mathematical
and General, 21(1):257, jan 1988.
[7] H. Sompolinsky, N. Tishby, and H. S. Seung. Learning from examples in large neural networks. Phys.
Rev. Lett., 65:1683–1686, Sep 1990.
[8] H. S. Seung, H. Sompolinsky, and N. Tishby. Statistical mechanics of learning from examples.
Physical Review A, 45(8):6056–6091, 1992.
[9] E. Levin, N. Tishby, and S. A. Solla. A statistical approach to learning and generalization in layered
neural networks. Proceedings of the IEEE, 78(10):1568–1574, 1990.
102
[10] Erin Grant, Sandra Nestler, Berfin Şimşek, and Sara Solla. Statistical physics, Bayesian inference
and neural information processing. arXiv e-prints, page arXiv:2309.17006, September 2023.
[11] V.N. Vapnik. Statistical Learning Theory. John Wiley & Sons, New York, 1998.
[12] J. J. Hopfield. Neural networks and physical systems with emergent collective computational abilities.
Proc. Natl. Acad. Sci. USA, 79(8):2554–2558, 1982.
[13] D. H. Ackley, G. E. Hinton, and T. J. Sejnowski. A learning algorithm for Boltzmann machines.
Cognitive Science, 9(1):147–169, 1985.
[14] G. E. Hinton and T. J. Sejnowski. Learning and relearning in Boltzmann machines. In D. E.
Rumelhart, J. L. McClelland, and CORPORATE PDP Research Group, editors, Parallel distributed
processing: explorations in the microstructure of cognition, vol. 1, pages 282–317. MIT Press, 1986.
[15] W. A. Little. The existence of persistent states in the brain. Math. Biosci., 19:101–120, 1974.
[16] M. Belkin, D. Hsu, S. Ma, and S. Mandal. Reconciling modern machine-learning practice and the
classical bias–variance trade-off. Proc. Natl. Acad. Sci. USA, 116:15849–15854, 2019.
[17] Marco Loog, Tom Viering, Alexander Mey, and David MJ Tax. A brief prehistory of double descent.
Proceedings of the National Academy of Sciences, 117(20):10625, 2020.
[18] M. Opper. Learning to generalize. In D. Baltimore, editor, Frontiers of Life: Intelligent Systems,
pages 763–775. Academic Press, Cambridge, 2001.
[19] David A. Roberts, Sho Yaida, and Boris Hanin. The Principles of Deep Learning Theory: An
Effective Theory Approach to Understanding Neural Networks. Cambridge University Press, 2022.
[20] V. Vapnik, E. Levin, and Y. Le Cun. Measuring the VC-dimension of a learning machine. Neural
Computation, 6(5):851–876, 1994.
[21] T. L. H. Watkin, A. Rau, and M. Biehl. The statistical mechanics of learning a rule. Rev. Mod.
Phys., 65(2):499–556, 1993.
[22] D. Haussler, M. Kearns, H. S. Seung, and N. Tishby. Rigorous learning curve bounds from statistical
mechanics. Machine Learning, 25(2):195–236, 1996.
[23] G. K. Dziugaite and D. M. Roy. Computing nonvacuous generalization bounds for deep (stochastic) neural networks with many more parameters than training data. Technical Report Preprint:
arXiv:1703.11008, arXiv, 2017.
[24] C. H. Martin and M. W. Mahoney. Post-mortem on a deep learning contest: a Simpson’s paradox
and the complementary roles of scale metrics versus shape metrics. Technical Report Preprint:
arXiv:2106.00734, arXiv, 2021.
[25] C. H. Martin and M. W. Mahoney. Implicit self-regularization in deep neural networks: Evidence
from random matrix theory and implications for learning. Journal of Machine Learning Research,
22(165):1–73, 2021.
[26] C. H. Martin, T. S. Peng, and M. W. Mahoney. Predicting trends in the quality of state-of-the-art
neural networks without access to training or testing data. Nature Communications, 12(4122):1–13,
2021.
[27] Yaoqing Yang, Ryan Theisen, Liam Hodgkinson, Joseph E. Gonzalez, Kannan Ramchandran,
Charles H. Martin, and Michael W. Mahoney. Evaluating natural language processing models with
generalization metrics that do not need access to any training or testing data. Technical Report
Preprint: arXiv:2202.02842, arXiv, 2022.
[28] Y. Yang, R. Theisen, L. Hodgkinson, J. E. Gonzalez, K. Ramchandran, C. H. Martin, and M. W.
Mahoney. Test accuracy vs. generalization gap: Model selection in NLP without accessing training
or testing data. In Proceedings of the 29th Annual ACM SIGKDD Conference, pages 3011–3021,
2023.
103
[29] Yefan Zhou, TIANYU PANG, Keqin Liu, Charles Martin, Michael W Mahoney, and Yaoqing Yang.
Temperature balancing, layer-wise weight analysis, and neural network training. In A. Oh, T. Naumann, A. Globerson, K. Saenko, M. Hardt, and S. Levine, editors, Advances in Neural Information
Processing Systems, volume 36, pages 63542–63572. Curran Associates, Inc., 2023.
[30] Haiquan Lu, Yefan Zhou, Shiwei Liu, Zhangyang Wang, Michael W. Mahoney, and Yaoqing Yang.
Alphapruning: Using heavy-tailed self regularization theory for improved layer-wise pruning of large
language models. In Advances in Neural Information Processing Systems, volume 37, pages 0000–
0000, 2024.
[31] John Negele. Hans Bethe and the theory of nuclear matter. Physics Today, 58(10):58, 2005.
[32] D. Ivanenko. The proton-neutron hypothesis of atomic nuclei. Nature, 129:798, 1932.
[33] Maria Goeppert-Mayer. On closed shells in nuclei. ii. Physical Review, 75(10):1969–1970, 1949.
[34] J. Hans D. Jensen, Otto Haxel, and Hans Suess. On the “magic numbers” in nuclear structure.
Physical Review, 75:1766, 1949.
[35] Eugene Wigner. Characteristic vectors of bordered matrices with infinite dimensions. Annals of
Mathematics, 62(3):548–564, 1955.
[36] V. A. Marchenko and L. A. Pastur. Distribution of eigenvalues for some sets of random matrices.
Mathematics of the USSR-Sbornik, 1(4):457–483, 1967.
[37] T. Guhr, A. Müller-Groeling, and H. A. Weidenmüller. Random matrix theories in quantum physics:
Common concepts. Physics Reports, 299:190, 1998.
[38] A. Zee. Law of addition in random matrix theory. Nuclear Physics B, 474(3):726–744, September
1996.
[39] Melih K. Sener and Klaus Schulten. General random matrix approach to account for the effect of
static disorder on the spectral properties of light harvesting systems. Physical Review E, 65(3):031916,
2002. Received 6 June 2001; revised manuscript received 29 August 2001; published 6 March 2002.
[40] S. Gallucio, J.-P. Bouchaud, and M. Potters. Rational decisions, random matrices and spin glasses.
Physica A, 259:449–456, 1998.
[41] R. Cherrier, D. S. Dean, and A. Lefèvre. Role of the interaction matrix in mean-field spin glass
models. Physical Review E, 67(4), April 2003.
[42] Rudolph Pariser and Robert G. Parr. A semi-empirical theory of the electronic spectra and electronic
structure of complex unsaturated molecules. i. The Journal of Chemical Physics, 21(3):466–471, 1953.
[43] J. Hubbard. Electron correlations in narrow energy bands. Proceedings of the Royal Society of
London. Series A, Mathematical and Physical Sciences, 276(1365):238–257, 1963.
[44] Michael J. S. Dewar and Walter Thiel. Ground states of molecules. 38. the mindo/3 method. approximations and parameters. Journal of the American Chemical Society, 97(16):4899–4907, 1975.
[45] John Ridley and Michael C. Zerner. Intermediate neglect of differential overlap spectroscopy: a
reexamination using a modified neglect of differential overlap approach. Theoretica Chimica Acta,
32:111–134, 1973.
[46] James J. P. Stewart. Mopac: A semiempirical molecular orbital program. Journal of Computer-Aided
Molecular Design, 4:1–103, 1990.
[47] Arieh Warshel and Michael Levitt. Theoretical studies of enzymic reactions: dielectric, electrostatic
and steric stabilization of the carbonium ion in the reaction of lysozyme. Journal of Molecular
Biology, 103(2):227–249, 1976.
[48] Walter Thiel. Chapter 21 - semiempirical quantum-chemical methods in computational chemistry.
In Clifford E. Dykstra, Gernot Frenking, Kwang S. Kim, and Gustavo E. Scuseria, editors, Theory
and Applications of Computational Chemistry, pages 559–580. Elsevier, Amsterdam, 2005.
[49] J. Hubbard. Calculation of partition functions. Physical Review Letters, 3(2):77–78, 1959.
104
[50] K. F. Freed. Theoretical basis for semiempirical theories. In G.A. Segal, editor, Semiempirical
Methods of Electronic Structure Calculation, volume 7 of Modern Theoretical Chemistry. Springer,
1977.
[51] Karl F. Freed. Is there a bridge between ab initio and semiempirical theories of valence? Accounts
of Chemical Research, 16:137–144, Mar 1983.
[52] Charles H. Martin and Karl F. Freed. Ab initio computation of semiempirical π-electron methods.
v. geometry dependence of hν π-electron effective integrals. The Journal of Chemical Physics,
105(4):1437–1450, 1996.
[53] Charles H. Martin. Highly accurate ab initio π-electron hamiltonians for small protonated schiff
bases. The Journal of Physical Chemistry, 100:14310–14315, 1996.
[54] Charles H Martin. Redesigning semiempirical-like pi-electron theory with second order effective
valence shell hamiltonian (hv) theory: application to large protonated schiff bases. Chemical Physics
Letters, 257(3-4):229–237, 1996.
[55] Charles H. Martin and Robert R. Birge. Reparametrizing mndo for excited-state calculations by
using ab initio effective hamiltonian theory: Application to the 2,4-pentadien-1-iminium cation. The
Journal of Physical Chemistry A, 102(5):852–860, 1998.
[56] Nobel Prize Committee. The nobel prize in physics 1982: Kenneth g. wilson. https://www.
nobelprize.org/prizes/physics/1982/wilson/, 1982. Accessed: 2024-12-09.
[57] Wolfgang Wenzel and Kenneth G. Wilson. Basis set reduction in hilbert space. Phys. Rev. Lett.,
69:800–803, Aug 1992.
[58] Alec Radford, Karthik Narasimhan, Tim Salimans, and Ilya Sutskever. Improving language understanding by generative pre-training. OpenAI, 2018.
[59] J. M. Jumper, K. F. Freed, and T. R. Sosnick. Maximum-likelihood, self-consistent side
chain free energies with applications to protein molecular dynamics. Technical Report Preprint:
arXiv:1610.07277, arXiv, 2016.
[60] D. A. Roberts, S. Yaida, and B. Hanin. The Principles of Deep Learning Theory. Cambridge
University Press, 2021.
[61] J. Lee, Y. Bahri, R. Novak, S. S. Schoenholz, J. Pennington, and J. Sohl-Dickstein. Deep neural
networks as Gaussian processes. Technical Report Preprint: arXiv:1711.00165, arXiv, 2017.
[62] G. Yang. Tensor programs III: Neural matrix laws. Technical Report Preprint: arXiv:2009.10685,
arXiv, 2021.
[63] C. H. Martin and M. W. Mahoney. Traditional and heavy-tailed self regularization in neural network
models. In Proceedings of the 36th International Conference on Machine Learning, pages 4284–4293,
2019.
[64] C. H. Martin and M. W. Mahoney. Heavy-tailed Universality predicts trends in test accuracies
for very large pre-trained deep neural networks. In Proceedings of the 20th SIAM International
Conference on Data Mining, 2020.
[65] B. Derrida. Random-energy model: An exactly solvable model of disordered systems. Physical
Review B, 24:2613–2626, Sep 1981.
[66] Joseph D. Bryngelson and Peter G. Wolynes. Spin glasses and the statistical mechanics of protein
folding. Proceedings of the National Academy of Sciences of the United States of America, 84:7524–
7528, Nov 1987.
[67] Charles H. Martin. Weightwatcher, 2021.
[68] A. Clauset, C. R. Shalizi, and M. E. J. Newman. Power-law distributions in empirical data. SIAM
Review, 51(4):661–703, 2009.
105
[69] J. Alstott, E. Bullmore, and D. Plenz. powerlaw: A python package for analysis of heavy-tailed
distributions. PLoS ONE, 9(1):e85777, 2014.
[70] Matthias Thamm, Max Staats, and Bernd Rosenow. Random matrix analysis of deep neural network
weight matrices. Physical Review E, 106(5):054124, 2022.
[71] J.-P. Bouchaud and M. Potters. Theory of Financial Risk and Derivative Pricing: From Statistical
Physics to Risk Management. Cambridge University Press, 2003.
[72] A. Edelman and Y. Wang. Random matrix theory and its innovative applications. In R. Melnik
and I. Kotsireas, editors, Advances in Applied Mathematics, Modeling, and Computational Science.
Springer, 2013.
[73] Marc Potters and Jean-Philippe Bouchaud. A First Course in Random Matrix Theory: for Physicists, Engineers and Data Scientists. Cambridge University Press, 2020.
[74] Francis R Bach and Michael I Jordan. Learning spectral clustering, with application to speech
separation. The Journal of Machine Learning Research, 7:1963–2001, 2006.
[75] Florent Benaych-Georges and Raj Rao Nadakuditi. The eigenvalues and eigenvectors of finite, low
rank perturbations of large random matrices. Advances in Mathematics, 227(1):494–521, 2011.
[76] Per Bak. How nature works: the science of self-organized criticality. Oxford University Press, Oxford,
UK, 1997.
[77] D. Sornette. Critical phenomena in natural sciences: chaos, fractals, selforganization and disorder:
concepts and tools. Springer-Verlag, Berlin, 2006.
[78] L. Laloux, P. Cizeau, J.-P. Bouchaud, and M. Potters. Noise dressing of financial correlation matrices.
Phys. Rev. Lett., 83(7):1467–1470, 1999.
[79] L. Laloux, P. Cizeau, M. Potters, and J.-P. Bouchaud. Random matrix theory and financial correlations. Mathematical Models and Methods in Applied Sciences, pages 109–11, 2005.
[80] Y. Yang, L. Hodgkinson, R. Theisen, J. Zou, J. E. Gonzalez, K. Ramchandran, and M. W. Mahoney.
Taxonomizing local versus global structure in neural network loss landscapes. Technical Report
Preprint: arXiv:2107.11228, arXiv, 2021.
[81] C. H. Martin and M. W. Mahoney. Heavy-tailed Universality predicts trends in test accuracies for
very large pre-trained deep neural networks. Technical Report Preprint: arXiv:1901.08278, arXiv,
2019.
[82] H. Sompolinsky, N. Tishby, and H. S. Seung. Learning from examples in large neural networks. Phys.
Rev. Lett., 65:1683–1686, 1990.
[83] T. Tanaka. On dualistic structure involving shannon transform and integrated r-transform. 2007
IEEE International Symposium on Information Theory, pages 1651–1654, 2007.
[84] T. Tanaka. Asymptotics of Harish-Chandra-Itzykson-Zuber integrals and free probability theory. J.
Phys.: Conf. Ser., 95(1):012002, 2008.
[85] R. A. Kievit, W. E. Frankenhuis, L. J. Waldorp, and D. Borsboom. Simpson’s paradox in psychological science: a practical guide. Frontiers in Psychology, 4(513):1–14, 2013.
[86] Didier Sornette. Dragon-kings, black swans and the prediction of crises, 2009.
[87] M. Gurbuzbalaban, U. Simsekli, and L. Zhu. The heavy-tail phenomenon in SGD. Technical Report
Preprint: arXiv:2006.04740, arXiv, 2020.
[88] Chaim Baskin, Evgenii Zheltonozhkii, Tal Rozen, Natan Liss, Yoav Chai, Eli Schwartz, Raja Giryes,
Alexander M Bronstein, and Avi Mendelson. Nice: Noise injection and clamping estimation for
neural network quantization. Mathematics, 9(17):2144, 2021.
[89] Jungwook Choi, Zhuo Wang, Swagath Venkataramani, Pierce I-Jen Chuang, Vijayalakshmi Srinivasan, and Kailash Gopalakrishnan. Pact: Parameterized clipping activation for quantized neural
networks. arXiv preprint arXiv:1805.06085, 2018.
106
[90] E. Gardner. Spin glasses with p-spin interactions. Nuclear Physics B, 257:747–765, 1985.
[91] A. Engel, C. Van den Broeck, and C. Broeck. Statistical Mechanics of Learning. Statistical Mechanics
of Learning. Cambridge University Press, 2001.
[92] Hanoch Gutfreund, Haim Sompolinsky, and Daniel Stein. Spin-glass models of neural networks.
Physical Review A, 32(2):1007–1018, 1985.
[93] C. H. Martin and M. W. Mahoney. Rethinking generalization requires revisiting old ideas: statistical
mechanics approaches and complex learning behavior. Technical Report Preprint: arXiv:1710.09553,
arXiv, 2017.
[94] Charles H. Martin and Karl F. Freed. Ab initio computation of semiempirical π-electron methods. iii. the benzene molecule, the zero-differential-overlap approximation, and the transferability of
parameters. The Journal of Chemical Physics, 101(7):5929–5941, 1994.
[95] B. H. Brandow. Many-body foundations of the nuclear shell model. Acta Physica Academiae Scientiarum Hungaricae, 19:289–294, 1965.
[96] G. Parisi and M. Potters. Mean-field equations for spin models with orthogonal interaction matrices.
Journal of Physics A: Mathematical and General, 28(18):5267–5286, 1995.
[97] Jordan Hoffmann, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya, Trevor Cai, Eliza
Rutherford, Diego de Las Casas, Lisa Anne Hendricks, Johannes Welbl, Aidan Clark, et al. Training
compute-optimal large language models. arXiv preprint arXiv:2203.15556, 2022.
[98] B. H. Brandow. Foundations of the nuclear shell model. Physics Letters, 4(8):294–296, 1963.
[99] Chengming Hu, Xuan Li, Dan Liu, Haolun Wu, Xi Chen, Ju Wang, and Xue Liu. Teacher-student
architecture for knowledge distillation: A survey, 2023.
[100] F. Vallet, J.-G. Cailton, and Ph Refregier. Linear and nonlinear extension of the pseudo-inverse
solution for learning boolean functions. Europhysics Letters, 9(4):315, jun 1989.
[101] M. Opper and W. Kinzel. Statistical mechanics of generalization. In E. Domany, J. L. van Hemmen, and K. Schulten, editors, Models of Neural Networks III: Association, Generalization, and
Representation, pages 151–209. Springer New York, 1996.
[102] A. Engel and C. Van den Broeck. Statistical Mechanics of Learning. Cambridge University Press,
Cambridge, UK, 2001.
[103] C. Zhang, S. Bengio, M. Hardt, B. Recht, and O. Vinyals. Understanding deep learning requires
rethinking generalization. Technical Report Preprint: arXiv:1611.03530, arXiv, 2016.
[104] C. H. Martin and M. W. Mahoney. Rethinking generalization requires revisiting old ideas:
statistical mechanics approaches and complex learning behavior. Technical Report Preprint:
arXiv:1710.09553v1, arXiv, 2017.
[105] D.Randall Wilson and Tony R. Martinez. The general inefficiency of batch training for gradient
descent learning. Neural Networks, 16(10):1429–1451, 2003.
[106] Yuki Tsukada and Hideaki Iiduka. Relationship between batch size and number of steps needed for
nonconvex optimization of stochastic gradient descent using armijo line search, 2024.
[107] Bradley Efron and Robert J Tibshirani. An Introduction to the Bootstrap. Chapman & Hall/CRC,
1993.
[108] Hari Kishan Prakash and charles h martin. Grokking and generalization collapse: Insights from
HTSR theory. In High-dimensional Learning Dynamics 2025, 2025.
[109] F. Rosenblatt. Principles of Neurodynamics. Spartan, New York, NY, USA, 1962.
[110] Andrew M Saxe, James L McClelland, and Surya Ganguli. Exact solutions to the nonlinear dynamics
of learning in deep linear neural networks. arXiv preprint arXiv:1312.6120, 2013.
107
[111] J. Pennington and P. Worah. Nonlinear random matrix theory for deep learning. In Annual Advances
in Neural Information Processing Systems 30: Proceedings of the 2017 Conference, pages 2637–2646,
2017.
[112] Joël Bun. Application of Random Matrix Theory to High Dimensional Statistics. Phd thesis, Université Paris Saclay (COmUE), 2016. NNT: 2016SACLS245, tel-01400544.
[113] Z. Burda, J. Jurkiewicz, M. A. Nowak, G. Papp, and I. Zahed. Lévy matrices and financial covariances. Technical Report Preprint: arXiv:cond-mat/0103108, arXiv, 2001.
[114] Z. Burda, J. Jurkiewicz, M. A. Nowak, G. Papp, and I. Zahed. Random Lévy matrices revisited.
Technical Report Preprint: arXiv:cond-mat/0602087, arXiv, 2006.
[115] Z. Burda and J. Jurkiewicz. Heavy-tailed random matrices. Technical Report Preprint:
arXiv:0909.5228, arXiv, 2009.
[116] Jipeng Li, Xueqiong Yuan, and Ercan Engin Kuruoglu. Exploring weight distributions and dependence in neural networks with α-stable distributions. IEEE Transactions on Artificial Intelligence,
5(11):5519–5535, November 2024.
[117] LeCun Yann. The mnist database of handwritten digits. R, 1998.
[118] H. Nishimori. Statistical Physics of Spin Glasses and Information Processing: An Introduction.
Oxford University Press, Oxford, 2001.
[119] S. L. Smith, P.-J. Kindermans, C. Ying, and Q. V. Le. Don’t decay the learning rate, increase the
batch size. Technical Report Preprint: arXiv:1711.00489, arXiv, 2017.
[120] M. Welling and Y. W. Teh. Bayesian learning via stochastic gradient Langevin dynamics. In
Proceedings of the 28th International Conference on Machine Learning, pages 681–688, 2011.
[121] K. He, X. Zhang, S. Ren, and J. Sun. Deep residual learning for image recognition. Technical Report
Preprint: arXiv:1512.03385, arXiv, 2015.
[122] K. Simonyan and A. Zisserman. Very deep convolutional networks for large-scale image recognition.
Technical Report Preprint: arXiv:1409.1556, arXiv, 2014.
[123] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas
Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, et al. An image is
worth 16x16 words: Transformers for image recognition at scale. arXiv preprint arXiv:2010.11929,
2020.
[124] Gao Huang, Zhuang Liu, Laurens Van Der Maaten, and Kilian Q Weinberger. Densely connected
convolutional networks. In Proceedings of the IEEE conference on computer vision and pattern
recognition, pages 4700–4708, 2017.
[125] Ebtesam Almazrouei, Hamza Alobeidli, Abdulaziz Alshamsi, Alessandro Cappelli, Ruxandra Cojocaru, Merouane Debbah, Etienne Goffinet, Daniel Heslow, Julien Launay, Quentin Malartic, Badreddine Noune, Baptiste Pannier, and Guilherme Penedo. Falcon-40B: an open large language model
with state-of-the-art performance, 2023.
[126] Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay
Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. Llama 2: Open foundation and
fine-tuned chat models. arXiv preprint arXiv:2307.09288, 2023.
[127] M. Dereziński, F. Liang, and M. W. Mahoney. Exact expressions for double descent and implicit
regularization via surrogate random design. Technical Report Preprint: arXiv:1912.04533, arXiv,
2019.
[128] Tamir David Hay and Lior Wolf. Dynamic layer tying for parameter-efficient transformers, 2024.
[129] Peijun Qing, Chongyang Gao, Yefan Zhou, Xingjian Diao, Yaoqing Yang, and Soroush Vosoughi.
Alphalora: Assigning lora experts based on layer training quality, 2024.
[130] François Chollet. keras. https://github.com/fchollet/keras, 2015.
108
[131] X. Glorot and Y. Bengio. Understanding the difficulty of training deep feedforward neural networks.
In Proceedings of the 13th International Workshop on Artificial Intelligence and Statistics, pages
249–256, 2010.
[132] Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor
Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, et al. Pytorch: An imperative style, highperformance deep learning library. Advances in neural information processing systems, 32, 2019.
[133] Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint
arXiv:1412.6980, 2014.
[134] R. Cherrier, D. S. Dean, and A. Lefèvre. Role of the interaction matrix in mean-field spin glass
models. Physical Review E, 67:046112, 2003.
109
A Appendix
A.1 Data Vectors, Weight Matrices, and Other Symbols
See Table 11 for a summary of various vectors and matrices, including their dimensions; see
Table 12 for a summary of various various symbols used throughout the text; and see Table 13
for a summary of types of “Energies” used throughout the text.
Number of NN Layers index L NL
Number of Data Examples index µ n
Number of (input) Features index i, j m
Actual Data (Matrix) D n × m
Model Data (Matrix) D n × m
Teacher Perceptron Weight Vector t m
Student Perceptron Weight Vector s m
Actual Input Data Vector xµ Nf × 1
Gaussian model of Input Data Vector ξµ Nf × 1
Actual Input Data Label yµ +1∣ − 1
Model Input Data Label yµ +1∣ − 1
General Weight Matrix W N × M
General Correlation Matrix X =
1
NW⊺W M × M
Input Layer Weight Matrix W1 N × M
Hidden Layer Weight Matrix W2 N × M
Output Layer Weight Matrix W3 M × 2
Teacher Weight Matrix T N × M
Student Weight Matrix S N × M
Student-Teacher Overlap Matrix R =
1
N
S
T T M × M
Inner Student Correlation Matrix AM =
1
N
S
⊺S M × M
Outer Student Correlation Matrix AN =
1
N
SS⊺ N × N
Inner ECS Student Correlation Matrix A˜ M M × M
Outer ECS Student Correlation Matrix A˜
N N × N
ECS Teacher Correlation Matrix X˜ M × M
Table 11: Summary of of various vectors and matrices, including their dimensions.
110
Perceptron Student-Teacher (ST) Overlap R = s
⊺
t = ∑i siti
Student-Teacher (ST) Overlap Operator R =
1
N
S
⊺T
Matrix Generalized ST Overlap 1
N2 Tr[R⊺R]
Student-Teacher Self-Overlap η(ξ
n
) = y
⊺
T
yS
ℓ2-Energy or ℓ2-Error Eℓ2
(s, t, ξ)
ℓ2-Energy Operator Form E
n
ℓ2
(s, t, ξ
n
) ∶= ∑ξ Eℓ2
(s, t, ξ)
ℓ2-Energy Matrix Operator Form E
n
ℓ2
(S, T) ∶= N(IM −
1
N
S
⊺T)
Annealed Error Potential ϵ(R) = ϵ(S, T) = ⟨E
n
ℓ2
(s, t, ξ
n
)⟩
ξ
n
Linear Perceptron ϵ(R) at high-T, large-N ϵ(R) = 1 − R
Annealed Approximation (AA) ⟨lnZ⟩
ξ
n ≈ ln⟨Z⟩
ξ
n
Annealed Hamiltonian Han(w)
Annealed Hamiltonian at high-T Han
hT (w) = ϵ(w)
Average Student-Teacher Generalization Error E¯ST
gen = ⟨ϵ(R)⟩β
s
Average Student-Teacher Generalization Accuracy 1 − E¯ST
gen = ⟨η(R)⟩β
s
Matrix Layer Quality-Squared Q¯2 = ⟨R⊺R⟩
β
S
Equivalent Notation for Averages ⟨⋯⟩A = ∫ ⋯dµ(A) = EA[⋯]
Projection Operator onto ECS Pecs ∶= ∑ ∣λ˜
i⟩⟨λ˜
i
∣, i = 1⋯M˜
Average over ECS Student Correlation Matrices ⟨⋯⟩
β
A˜
= ∫ ⋯dµ(A˜ ) = EA˜ [⋯]
ERG Trace-Log-Determinant Relation Tr[ln A] = ln det A
Effective Correlation Measure Transform dµ(S) → dµ(A˜ )
HCIZ Integral (Tanaka’s Notation) EA˜ [exp(nβ Tr[T⊺AT)]
HCIZ Integral (Large-N in N explicit) ⟨exp(nβN Tr[
1
N T⊺ A˜
N T])⟩A˜
Layer Quality-Squared Generating Function βΓ
IZ
Q¯2,N≫1
∶= nβ ∑
M˜
µ=1 ∫
λ˜µ
λ˜min
dzR(z)
Norm Generating Function GA(γ) = ∫
λ˜
λ˜min
RA(z)dz
Eigenvalue for X =
1
NW⊺W λ or λi for i = 1⋯M
Power Law ESD Tail for X ρtail(λ) ∼ λ
−α
Effective Correlation Space ESD Tail for X ρ
ECS
tail (λ˜), Tr [ln ∏
M˜
j=1 λ˜
j ] = 0
Schatten Norm ∥X∥
α
α = ∑j λ
α
j
ECS Tail Norm 1
M˜ ∑
M˜
i λ˜
i
, λ˜
i ∈ ρ
ECS
tail (λ)
Spectral Norm ∥X∥∞ = λmax
WeightWatcher Start of PL Tail λ
P L
min = λmin
Start of ECS Tail λ
ECS
min = λ
∣detX∣=1
min
ECS-PL Gap between start of tails ∆λmin ∶= λ
ECS
min − λ
∣detX∣=1
min
WeightWatcher Alpha (layer) quality metric α
WeightWatcher AlphaHat (layer) quality metric αˆ = αlog10(λmax)
Table 12: Summary of various various symbols used throughout the text.
111
Explanation Examples Refs
Energy Landscape or NN Output function E
out
NN Sec. 2.1
The output of the NN given a single input data point. 1,77,4.3.1,100
Energy or Student-Teacher (ST) Error EL, E
n
L
Sec. 4, A.2
The squared between the output of a Student NN and its
prescribed Teacher label y
T
µ
for a single data point
EL(ξµ) ∶= (y
T
µ − E
out
S
(ξµ))2
19
And as the total error for a sample of n data points E
n
L
= ∑
n
µ=1 EL(ξµ) 22
Or between the outputs of the Student and the Teacher NNs. E
n
L
∶= ∑
n
µ=1
(E
out
S
(ξµ) − E
out
T
(ξµ))2
86,101,183
Annealed Hamiltonian (and Potentials) Han (E(R), ϵ(R)) Sec. 4.2.4,A.2
The Annealed Error Potential (for the Error) is defined as: ϵ(R) =
1
n
E(R) = ⟨E
n
L
⟩
ξ
n 97
The Annealed Hamiltonian (for the Error): βHan ∶=
1
n
ln⟨e
−βE
n
L ⟩ξn 43, 168
At high-T, the relation between Han
hT and ϵ(R) is Han
hT (R) ∶= ϵ(R) 49
The full ST model Hamiltonian: βHan(β, R) ∶=
1
2
ln[1 + 2β(1 − R)] 175
At high-T, the ST model Hamiltonian is: βHan
hT (R) ∶= β(1 − R) 97, 176
The per-parameter ST model Hamiltonian: βhan(R) ∶=
1
2
ln[1 +
2βN
M Tr[IM − R]] 200
At high-T, the matrix-generalized ST model Hamiltonian is: Han
hT (R) ∶= N(IM − R) 206
The Layer Quality-Squared Hamiltonian (for the Accuracy): HQ¯2 ∶= R⊺R 16, 218
Different Average Model Errors E¯ Sec. 4.2.5
Empirical Training, Teacher, and Test Errors E¯emp
train, E¯T ≈ E¯emp
gen 78,79,83
Empirical Generalization Gap E¯emp
gap ∶= E¯emp
train − E¯emp
gen 84
Student-Teacher Training and Generalization Errors E¯ST
train, E¯ST
gen 171,174
Neural Network (MLP) Training and Generalization Errors,
the (abstract) matrix generalization of ST error
E¯NN
train, E¯NN
gen
Average Training and/or Generalization Error E¯
train, E¯
gen Sec. 4.2.5
In the AA and at High-T, these are the same, and are just
the Thermal Average of ϵ(R) E¯an,hT
train = E¯an,hT
gen = ⟨ϵ(R)⟩β
s
56,54
For the ST model, we always assume AA and High-T E¯ST
gen = E¯an,hT
gen
Likewise, when generalizing E¯ST
gen to matrices, E¯ST
gen → E¯NN
gen = E¯an,hT
gen
Layer Qualities Q¯, Q¯2 Sec. 5.2.1, A.3
For the ST Perceptron, Q¯ST is the generalization accuracy Q¯ST ∶= 1 − E¯ST
gen = 1 − ⟨ϵ(R)⟩β
s
in terms of the Self-Overlap η(R) Q¯ST ∶= ⟨η(R)⟩β
s
In the AA, and at high-T, ⟨ϵ(R)⟩β
s
= 1 − R Q¯ST ∶= 1 − E¯an,ht
gen = ⟨R⟩
β
s
99
For an MLP / NN, we approximate the total accuracy as a
product of layer qualities Q¯ (in the AA, at high-T)
Q¯NN ∶= ∏ Q¯NN
L
7
For a matrix, the Layer Quality-Squared Q¯2 Q¯2
∶= ⟨R⊺R⟩
β
S
110
We approximate Q¯ using the quality squared Q¯ ∶=
√
Q¯2 ≈ QNN
L
9,111
Table 13: Summary of types of “Energies,” with simplified examples of the notation, and references to definitions.
112
A.2 Summary of the Statistical Mechanics of Generalization (SMOG)
In this section, we derive the Annealed Hamiltonian for two variants of the ST model, in the highT limit: in Appendix A.2.1, we derive an expression for Han(R) for the ST Perceptron model,
when the students and teachers are modeled as N-vectors w (as in [8]); and in Appendix A.2.2,
we derive an expression for Han(R) for Matrix-Generalized case, i.e., when the students and
teachers are modeled as N × M matrices W (as our SETOL requires). From these, we will obtain
expressions for the Average ST Model Generalization Accuracy E¯ST
gen and the Average NN Model
Generalization Accuracy E¯NN
gen , as well as for the corresponding data-averaged errors. Although
the functional form for these quantities will be the same for the vector case and the matrix case,
there are several important differences in the derivation of Han(R), most notably having to do
with a normalization for the weight matrix.
A.2.1 Annealed Hamiltonian Han(R) when Student and Teachers are Vectors
In this section, we derive an expression for the Annealed Hamiltonian Han(R), in the AA and
the high-T approximation, when student and teachers are are modeled as N-vectors. From this,
we obtain an expression for the data-averaged ST error ϵ(R), which is the same as the expression
given in Eqn. 97.
The procedure starts by computing the associated quenched average of the Free Energy,
defined for the model error as
⟨−βF⟩
ξ
n ∶=⟨lnZ⟩
ξ
n
= ⟨ln∫ dµ(s)e
−βE
n
ℓ2
(s,t,ξ
n)
⟩
ξ
n
=
1
N
∫ dµ(ξ
n
) ln∫ dµ(s)e
−βE
n
ℓ2
(s,t,ξ
n)
, (165)
where dµ(ξ
n
) ∶= ∏
N
i=1 dξiP(ξ
n
) and where the data-dependent ST error, E
n
ℓ2
(s, t, ξ
n
), is defined
in Eqn. 86, with an L = ℓ2 loss. 47 If we apply the AA (see Eqn. 40 and Eqn. 41) to Eqn. 165,
then we obtain
⟨−βF⟩
ξ
n ≃ ln 1
n
∫ dµ(ξ
n
)∫ dµ(s)e
−βE
n
ℓ2
(s,t,ξ
n)
. (166)
Notice that we have interchanged the logarithm (ln) and and the data average (the “disorder
average”) ⟨⋯⟩
ξ
n over the data; this is the essence of the AA, as it lets the disorder fluctuate
rather than forcing the system to be quenched to the data. We will now switch the order of
integration in Eqn. 166, giving
⟨−βF⟩
ξ
n ≃ ln∫ dµ(s)
1
n
∫ dµ(ξ
n
)e
−βE
n
ℓ2
(s,t,ξ
n))
. (167)
We now recall the definition of the Annealed Hamiltonian, Han(R) (see Eqn. 43 in Section 4.2,
which is analogous to Eqn. (2.31) of [8]):
βHan(R) = βHan(β, s, t) ∶= −
1
n
ln∫ dµ(ξ
n
)e
−βE
n
ℓ2
(s,t,ξ
n)
. (168)
47Also, recall that the Teacher T is fixed and is not learned, so we do not integrate over dµ(t). In fact, for
the (vector) Perceptron model, the Teacher T weights are simply subsumed into the overlap R, and even in the
more general cases, such as non-Linear/Boolean Perceptron, in the full Replica calculations, etc. See the original
literature for more details. [18, 8, 102] as well as [93].
113
where we have denoted the Hamiltonian as Han(β, s, t) to indicate the explicit dependence on β,
and we have added β to the R.H.S. to because the L.H.S. is unitless. Using this definition, we can
express the Annealed Partition Function, Z
an
n
, in the AA in terms of the Annealed Hamiltonian
Han(R) (as in Eqn. 46 in Section 4.2, and as in Eqn. (2.31) of [8]:
Z
an
n
∶= n⟨Z⟩
ξ
n = ∫ dµ(s)e
−nβHan(β,s.t)
. (169)
Following Section 4.2.5, we can write the Average Model Training Error E¯ST
train, in the AA, in
terms of Annealed Partition Function, Z
an
n
:
E¯ST
train ∶= −
1
n
∂
∂β lnZ
an
n
(170)
This now lets us write the Average Model Training Error E¯ST
train in the AA in terms of the
Annealed Hamiltonian Han(R):
E¯ST
train =
1
Zan
n
∫ dµ(s)
∂βHan
∂β e
−nβHan(β,s,t)
=
1
Zan
n
∫ dµ(s)⟨E
n
ℓ
(s, t, ξ)⟩
ξ
n e
−nβHan(β,s,t)
, (171)
where E
n
ℓ
(s, t, ξ)⟩β
ξ
n
is a Thermal Average but defined over the specific ST error in the AA for the
chosen set of training data ξ
train = ξ
n
, and is denoted by
⟨E
n
ℓ
(s, t, ξ)⟩β
ξ
n
∶=
∂βHan
∂β . (172)
This is analogous to defining the average error E
n
L
(w, ξ
n
) as a Thermal Average, but as one over
the data ξ
n
instead of the weights. This can be seen by expanding Eqn. 43, setting w = s, fixing
t (implicitly), and taking the partial derivative:
∂βHan
∂β =
∂
∂β (−
1
n
ln∫ dµ(ξ
n
)e
−βE
n
ℓ2
(w,ξ
n)
) (173)
= −
1
n
∂
∂β ln∫ dµ(ξ
n
)e
−βE
n
ℓ2
(w,ξ
n)
= −
1
n
(∫ dµ(ξ
n
)e
−βE
n
ℓ2
(w,ξ
n)
)
−1 ∂
∂β ∫ dµ(ξ
n
)e
−βE
n
ℓ2
(w,ξ
n)
= −
1
n
(∫ dµ(ξ
n
)e
−βE
n
ℓ2
(w,ξ
n)
)
−1
∫ dµ(ξ
n
)(−E
n
L(w, ξ
n
))e
−βE
n
ℓ2
(w,ξ
n)
We can also write the Model Generalization Accuracy E¯ST
gen as Boltzmann-weighted average of
ϵ(R), weighted by Han(β, S; T) (as in (2.32) in [8]), as:
E¯ST
gen =
1
Zan
n
∫ dµ(s)ϵ(R)e
−nβHan(β,s,t)
, (174)
where ϵ(R) = ϵ(s) is the average ST error, for a fixed Teacher T, averaged over all possible data
inputs, i.e., not just over the specific training data. (Note that we have dropped the subscript
train on ξ since it is clear from the context.)
In the high-T (small β) limit, the two model errors become formally equivalent (i.e., E¯ST
train =
E¯ST
gen as T → ∞). To show this, consider the Annealed Hamiltonian Han(R), for the Linear
114
Perceptron with the ℓ2 loss. As shown in Eqn. (C6) of [8], this takes a simple analytic form–in
the Large-N limit in n–in terms of the ST overlap R:
H
an(R) =
1
2
ln [1 + 2β(1 − R)]. (175)
Eqn. 175 holds in the AA, but not in the High-T limit.
If we evaluate ∂Han
∂β in the High-T (small β) limit, then we can use the approximation
(ln[1 + x] ≃ x + ⋯) to obtain the High-T approximation:
βHan(R) ≃ βHan
hT (R) ∶= β(1 − R), β small. (176)
where we now see that Han
hT (R) no longer explicitly depends on β. By Eqn. 49, this gives
ϵ(R) = ⟨Eℓ2
(s, t, ξ⟩
ξ
n ≃ 1 − R as n → ∞, (177)
which we recognize as the same as the data-averaged ST error ϵ(R) in Eqn. 97.
A.2.2 Annealed Hamiltonian Han(R) for the Matrix-Generalized ST Error
In this section, we derive an expression for our matrix generalization of the Annealed Hamiltonian
of the Linear Perceptron, in the AA and the high-T approximation, when student and teachers
are are modeled as N × M matrices W, i.e., Han(R) → Han(R), which has the same form as
Eqn. 168 for the vector case.
From this, we obtain an expression for the data-averaged ST error ϵ(R), again when the
student and teachers are are modeled as matrices. There is a subtle normalization issue here,
about which we need to be careful. However, when we normalize appropriately, we will obtain an
expression for “data-averaged ST error” (i.e., Annealed Error Potential) ϵ(R) that is of the same
form as we obtained in the vector case (as given in Eqn. 177 and Eqn. 97). The difference will
be that in the vector case we take R =
1
N
s
⊺
t, while in the matrix case we take R =
1
N
S
⊺T.
We will need to evaluate an average over the n random M-dimensional training data vectors
ξ
n
, which are i.i.d Gaussian with 0 mean and σ
2 variance:
∥ξ
n
∥
2
∶=
n
∑
µ=1
ξµξ
⊺
µ = σ
2
IM, (178)
where each ξµ is a vector of length M, and IM is an M × M identity matrix. The expected value
of the squared norm is:
E[∥ξµ∥
2
] = Mσ2
. (179)
If we let σ
2 ∼
1
M , then E[∥ξµ∥
2
] = 1, i.e., the data vectors can be normalized to 1. Let the
probability distribution over the N data vectors be
P(ξ
n
) =
n
∏
µ=1
⎛
⎝
1
√
(2πσ2)M
⎞
⎠
e
−
∥ξµ∥
2
2σ2
=
⎛
⎝
1
√
(2πσ2)M
⎞
⎠
n
exp
⎡
⎢
⎢
⎢
⎢
⎣
−
M
2
n
∑
µ=1
∥ξµ∥
2
⎤
⎥
⎥
⎥
⎥
⎦
= N exp
⎡
⎢
⎢
⎢
⎢
⎣
−
M
2
n
∑
µ=1
∥ξµ∥
2
⎤
⎥
⎥
⎥
⎥
⎦
, (180)
where M = Nf is the number of features in the data, where the normalization N is
N ∶=
⎛
⎝
1
√
(2πσ2)M
⎞
⎠
n
= (
M
2π
)
nM/2
. (181)
115
The Total Data Sample Error (E
n
ℓ2
(S, T)) and the Matrix Normalization First, let us
express the matrix-generalized Total Data Sample Error, E
n
ℓ2
(S, T), for a single layer, in operator
form (for each of the n training examples)
1
n
E
n
ℓ2
(S, T) ∶= N Tr [IM −
1
N
S
⊺T] = NM − N Tr[R] (182)
where IM is a diagonal matrix of dimension M. Note that the matrices are by default dataaveraged empirical quantities, so we can drop the 1/n on the RHS.
Also, notice that E
n
ℓ2
(S, T) scales as N × M, the total number of parameters in the system.
Also, importantly, when all the overlaps are perfect, then the error is zero, i.e. if Tr[R] = M
then E
n
ℓ2
(S, T) = 0.
We can define the data-dependent form (i.e., in the basis of the data ξ) as
E
n
ℓ2
(S, T, ξ
n
) ∶=N
n
∑
µ=1
(ξ
µ
)
⊺
(IM −
1
N
S
⊺T) ξ
µ
=N
n
∑
µ=1
M
∑
i,j=1
ξ
µ
i
(δij −
1
N
[S
⊺T]ij) ξ
µ
j
. (183)
The Annealed Hamiltonian (per-parameter, h
an(R)) The definition of the Annealed
Hamiltonian, Han(R), for the idealized case must be extended to account for the N × M parameters per training example. We then have that the total energy is then the sum of the entries
of M (feature) vectors, as expected by Size-Extensivity in N and Size-Consistency in M:
Tr[H
an(R)] = M (N Tr[h
an(R)]) (184)
where the Annealed Hamiltonian per-parameter, h
an(R), is obtained from Eqn. 168 as
βhan(R) ∶= −
1
n
ln∫ Dξ
n
e
−βE
n
ℓ2
(S,T)
P(ξ
n
) (185)
= −
1
n
ln IH,
where
IH ∶= ∫ Dξ
n
e
−βE
n
ℓ2
(S,T)
P(ξ
n
). (186)
That is, h
an(R) represents the Energy or Error that each of the N × M parameters contributes
(averaged over the N training examples ξ
n
).
The goal will be to derive the high-Temperature Annealed Hamiltonian, Han
hT (R), which is
now defined such that:
Tr[H
an
hT (R)] ∶= MN ( Tr[h
an
hT (R)]) (187)
If examining the the trace of h
an
hT (R), then we can infer the functional form necessary to
define the matrix-generalized Annealed Error Potential for each parameter:
ϵ(R) ∶= Tr[h
an
hT (R)], (188)
which would be like a mean-field potential, but we need something different for the matrix case.
116
To evaluate the integral, notice that IH is really just an average over i.i.d. data, and so it is
just a product over n independent terms (1 for each training example).
IH ∶= ∫ Dξ
n
e
−βE
n
ℓ2
(S,T)
P(ξ
n
) → [∫ Dξ [⋯] ]n
, (189)
as in Eqn. 193 below. Moreover, when taking ln IH, the N term pulls down and become a prefactor
−ln IH = −ln [∫ Dξ [⋯] ]n
= −nln [∫ Dξ [⋯] ] . (190)
Thus, as with the vector case, Han(R) is like a mean-field average over the data ξ, indepedent
of the sample size N. Also, since the final result must scale as N × M, the integral should scale
as M, i.e., [∫ Dξ [⋯] ] ∼ M.
If we substitute E
n
ℓ2
(S, T, ξ
n
), Eqn. 183, into the integral IH, Eqn. 186, then we obtain
IH = ∫ Dξ
n
exp
⎛
⎝
−β
n
∑
µ=1
N(ξ
µ
)
⊺
(IM −
1
N
S
⊺T) ξ
µ
⎞
⎠
P(ξ
n
) (191)
= ∫ Dξ
n
exp
⎛
⎝
−β
n
∑
µ=1
N(ξ
µ
)
⊺
(IM −
1
N
S
⊺T) ξ
µ
⎞
⎠
N exp
⎛
⎝
−
N
∑
µ=1
∥ξ
µ
∥
2
2σ
2
⎞
⎠
= N ∫ Dξ
n
exp
⎛
⎝
−β
n
∑
µ=1
N(ξ
µ
)
⊺
(IM −
1
N
S
⊺T)(ξ
µ
) −
n
∑
µ=1
∥ξ
µ
∥
2
2σ
2
⎞
⎠
= N ∫ Dξ
n
exp
⎛
⎝
−
1
2σ
2
n
∑
µ=1
2βσ2N(ξ
µ
)
⊺
(IM −
1
N
S
⊺T)(ξ
µ
) + ∥ξ
µ
∥
2
⎞
⎠
= N ∫ Dξ
n
exp
⎛
⎝
−
1
2σ
2
n
∑
µ=1
(ξ
µ
)
⊺
[2βσ2N(IM −
1
N
S
⊺T) + IM](ξ
µ
)
⎞
⎠
. (192)
By combining the exponents, we obtain
IH = N ∫ Dξ
n
exp
⎡
⎢
⎢
⎢
⎢
⎣
−
1
2σ
2
N
∑
µ=1
(ξ
µ
)
⊺
(M) ξ
µ
⎤
⎥
⎥
⎥
⎥
⎦
= N ∫ Dξ exp [−
1
2σ
2
(ξ)
⊺
(M) ξ]
n
, (193)
where M = 2βσ2N(IM −
1
N
S
⊺T) + IM is an M × M matrix. We now use the familiar property of
multi-variant Gaussian integrals,
∫ dxe
−
1
2σ2
(x)
⊺M(x)
= (2πσ
2
)
M/2 1
√
det (M)
(194)
where x is an m-dim vector (with zero mean), and M is a square positive-definite matrix, and
det (M) is the determinant of M. Using Eqn. 194, we can rewrite IH in Eqn. 193 as
IH = N
⎡
⎢
⎢
⎢
⎣
(2πσ2
)
M/2
√
det (M)
⎤
⎥
⎥
⎥
⎦
n
(195)
= N (2πσ
2
)
NM/2
[
√
det (2βσ2N(IM −
1S⊺T) + IM)]
−n
= (
1
2πσ2
)
NM/2
(2πσ
2
)
M/2
[
√
det (IM + 2βσ2N(IM −
1
N
S⊺T))]
−n
, (196)
117
where Eqn. 196 follows by inserting N from Eqn. 181. We can now identify σ
2 =
1
M to obtain
IH = [
√
det (IM + 2βσ2N(IM −
1
N
S⊺T))]
−n
= [
√
det (IM +
2β
M N(IM −
1
N
S⊺T))]−n
= [det (IM +
2β
M N(IM −
1
N
S
⊺T))]−n/2
. (197)
The High-Temperature Limit. In the high-T approximation, β becomes small, giving the
expression det(IM + ϵΩ) ≈ 1 + ϵ Tr[Ω], which holds for an arbitrary matrix Ω for small ϵ. Using
this, we can evaluate the determinant in Eqn. 197 in the large-N approximation, which gives
IH ≈ [1 +
2β
M N( Tr [IM −
1
N
S
⊺T])]−n/2
. (198)
Inserting this into Eqn. 185, we obtain
βhan(R) = −
1
n
ln [1 +
2β
M N( Tr [IM −
1
N
S
⊺T])]−n/2
(199)
=
1
2
ln [1 +
2β
M N( Tr [IM −
1
N
S
⊺T])]
This form of the Hamiltonian, Han(R), however, is not symmetric, and we will eventually
want a symmetric operator or matrix. Fortunately, the high-T form, Han
hT (R), can be made
symmetric, as shown below. But first, let show that this result is consistent with the our previous
Percpetron result.
Matrix-Generalized ST Error Han
hT (R) for N = 1. To start, observe that when N = 1,
Eqn. 200 becomes
βHan(R)∣N=1 = βhan(R)∣N=1
= −
1
n
ln [1 + 2β
1
M Tr [(M − s
⊺
t]]−n/2
(200)
=
1
2
ln [1 + 2β
1
M Tr [M − s
⊺
t]]
=
1
2
ln [1 + 2β(1 − R)], (201)
where we recall that s and t are implicitly normalized to 1/m, where here m = M. This result
shows that Eqn. 200 reduces to Eqn. 175, as desired.
This ensures the Hamiltonian scales as M so the Free Energy scales as N × M, the number
of free paramaters in the system. Notice that for the final Layer Quality-Squared Hamiltonian
HQ¯2 , this will change.
IH ≈ [1 +
2β
M
N Tr [IM −
1
N
S
⊺T]]
−n/2
, (202)
for any M > 1. Given this, it follows from Eqn. 200 and Eqn. 185 that we can define
βHan(R)∣N=1 ∶=
M
2
ln det [1 +
2β
M
(IM − R)] , (203)
118
and reduces to the same functional form as Eqn. 175, as desired (recalling that s and t are
implicitly normalized by M = m).
To obtain the high-Temperature form, we use
ln det[IM + ϵM] ≈ Tr(ϵM) (ϵ ≪ 1), (204)
with ϵ =
2β
M and M = IM −R. We now obtain the following result for the matrix-generalized high-T
of Han
hT (R) using
Tr[H
an
hT (R)] ∣N=1 = Tr[IM − R] = M − Tr[R] (205)
The final expression for Han
hT (R) is
H
an
hT (R) = N(IM − R) (206)
A.3 Expressing the Layer Quality
In this section, we obtain an approximation expression for the Layer Quality-Squared from the
IZ Free Energy for the Generalization Error, given in Eqn. 113 in Section 5.2.1.
For the required Free Energy βF
IZ, we will use the matrix-generalized Hamiltonian from
Eqn. 206 for the Layer Quality, Han
hT (R) = N(IM − R). giving a Boltzmann distribution and the
corresponding Thermal Average. Expanding this out, we have
−βF
IZ
= − ln∫ dµ(S) exp [−nβ Tr[H
an
hT (R)]] (207)
(208)
We could also express βF
IZ In terms of the matrix-generalized Annealed Error Potential ϵ(R)
(Eqn. 188), giving
−βF
IZ
= − ln∫ dµ(S) exp [−nβN ϵ(R)] (209)
In analogy with Eqn. 49, as Han
hT (R) = M − R, write
−βF
IZ
= − ln∫ dµ(S) exp [−nβN Tr[M − R]] (210)
Using the approximation Tr[R] ≈
√
Tr[R⊺R], we have
−βF
IZ
≈ − ln∫ dµ(S) exp [−nβN(M −
√
Tr[R⊺R])] (211)
= − ln∫ dµ(S) exp[−nβNM] exp [nβN√
Tr[R⊺R]] , (212)
= − ln e
−nβNM ∫ dµ(S) exp [nβN√
Tr[R⊺R]] , (213)
= − ln e
−nβNM − ln∫ dµ(S) exp [nβN√
Tr[R⊺R]] , (214)
=nβNM − ln∫ dµ(S) exp [nβN√
Tr[R⊺R]] , (215)
Notice that, as expected, the Free Energy scales βF
IZ as n × N × M, the total number of
degrees of freedom of the theory. Since Eqn. 207 equals Eqn. 212, we can write the Free Energy
119
in terms of Tr[R⊺R]. From Eqn. 215, we can identify a generating function (ΓQ¯) for the layer
accuracy, or Quality. For example, to compute the average Quality Q¯, we would use
βΓ
IZ
Q¯
∶= ln∫ dµ(S) exp [nβN√
Tr[R⊺R]] , (216)
and to compute the average Quality (squared) Q¯2
, we would use
βΓ
IZ
Q¯2
∶= ln∫ dµ(S) exp [nβN Tr [R
⊺R]] . (217)
We have recovered Eqn. 113. We can now also define the Layer Quality-Squared Hamiltonian as
HQ¯2 ∶= R
⊺R (218)
which is a symmetric operator, as desired. Consequently, we may also write
βΓ
IZ
Q¯2
∶= ln∫ dµ(S) exp [nβN Tr[HQ¯2 ]] . (219)
A.4 Derivation of the ERG Condition
A.4.1 Setting up the Saddle Point Approximation (SPA)
As in Eqn. 118, we can write Eqn. 11 in terms of the AN = SS⊺
form of the Outer Student
Correlation matrix, giving
βΓ
IZ
Q¯2 = ln∫
S
dµ(S) exp (nβN Tr [
1
N T
⊺ANT]) (220)
where dµ(S) is the measure over all N ×M real-valued random matrices, although we really want
to limit this to all N × M real matrices that resemble the Teacher T, which we clarify below.
To transform βΓ
IZ
Q¯2
into a form we can evaluate using Tanaka’s result [84], we need to change
the measure from an integral over all random N ×M student weight matrices dµ(S) to an integral
over all N × N student correlation matrices dµ(AM), i.e., dµ(S) → dµ(AM). To accomplish this,
we can insert an integral over the Dirac Delta function
I ∶= ∫ dµ(AM)δ(NAM − S
⊺S). (221)
(This is simply a resolution of the Identity.) This gives
βΓ
IZ
Q¯2 = ln∫
S
dµ(S)∫A
dµ(AM)δ (NAM − S
⊺S) e
nβNT r[
1
N
T⊺ANT]
, (222)
where dµ(A) = Pr [A] dA and Pr [A] is the (still unspecified) probability density over the new
random matrix A. Let us express Eqn. 222 at Large-N limit in N as
lim
N≫1
βΓ
IZ
Q¯2 = lim
N≫1
ln∫ dµ(A)∫ dµ(S)δ(NAM − S
⊺S)e
nβNT r[
1
N
T⊺ANT])
. (223)
Now we assume we can first evaluate the term
lim
N≫1
∫ dµ(S)δ(NAM − S
⊺S) (224)
at Large-N in N using a Saddle Point Approximation (SPA). Using the relation,
δ(NAM − S
⊺S) = NM ∫Aˆ
dµ(Aˆ )e
iNT r[AAˆ M]
e
−iT r[ASˆ ⊺S]
, (225)
120
where Aˆ is an M × M auxiliary matrix, and the domain of integration dµ(Aˆ ) is all M × M
real-valued matrices, and where the normalization N1 is
N1 ∶=
1
(2π)M(M+1)/4
, (226)
because A is a symmetric matrix with M(M + 1)/2 constraints.
This is simply the matrix generalization of δ(x) =
1
2π
∫
∞
−∞ e
ixxˆ dxˆ, so we can express the delta
function as an exponential, giving
βΓ
IZ
Q¯2 = N1 ln∫
S
dµ(S)∫A
dµ(AM)∫Aˆ
dµ(Aˆ )e
iNT r[AAˆ M]
e
−iT r[ASˆ ⊺S]
e
nβNT r[
1
N
T⊺ANT]
. (227)
Rearranging terms, we obtain
βΓ
IZ
Q¯2 = ln∫A
dµ(AM)e
nβNT r[
1
N
T⊺ANT]
× Γ1, (228)
where we define Γ1 as
Γ1 ∶= Γ1(AM) = N1 ∫
S
dµ(S)∫Aˆ
dµ(Aˆ )e
iNT r[AAˆ M]
e
−iT r[ASˆ ⊺S]
. (229)
We can simplify the complex integral in Γ1 with the Wick Rotation iAˆ → Aˆ . The Wick rotation
ensures that the Gaussian integral converges (although this has not been rigorously checked). We
may expect dµ(Aˆ ) to be invariant to rotations in the complex plane, so the Wick rotation does
not introduce any complex prefactors. This gives
Γ1 = N1 ∫
S
dµ(S)∫Aˆ
dµ(Aˆ )e
NT r[AAˆ M]
e
−T r[ASˆ ⊺S]
(230)
= N1 ∫
S
dµ(S)∫Aˆ
dµ(Aˆ )e
NT r[AAˆ M]
e
−T r[SASˆ ⊺
]
, (231)
where the second line follows since the trace is invariant under cyclic permutations (i.e., Tr[ABC] =
Tr[BCA] = Tr[CAB]). Swapping the order of the integrals yields
Γ1 = Γ1(Aˆ ) = N1 ∫Aˆ
dµ(Aˆ )e
NT r[AAˆ M] × Γ2, (232)
where we define Γ2 as
Γ2 ∶= Γ2(Aˆ ) = ∫
S
dµ(S)e
−T r[SASˆ ⊺
]
.
To evaluate Γ2, we will make several mathematically convenient approximations. (These will
yield an approximate expression which can be verified empirically.) We first assume for the
purpose of changing measure that the (data) columns of S are statistically independent, so that
the measure dµ(S) factors into N Gaussian distributions
dµ(S) =
N
∏
µ=1
dµ(sµ) =
N
∏
µ=1
dsµ, (233)
where sµ is an M-dimensional vector. The singular values of S are invariant to random permutations of the columns or rows, so the resulting ESD does not change. This is very different from
permuting S element-wise, which will make the resulting ESD Marchenko Pastur (MP).
121
Using Eqn. 233, Γ2 reduces to a simple Gaussian integral, which can be evaluated as a product
of N Gaussian integrals (over the M × M matrix Aˆ )
Γ2 = [∫
s
dse
−
1
σ2 sAs ˆ ⊺
]
N
(234)
= [N2 det (Aˆ )
−1/2
]
N
, (235)
where the normalization term N2
N2 ∶= (πσ
2
)
M/2
, (236)
where σ
2 = s
⊺
s = 1/M
For any square, non-singular matrix Aˆ , Tr [ln Aˆ ] = ln det (Aˆ ), so it follows from Eqn. 234
that
ln Γ2 = N ln N2 [(det (Aˆ ))
−1/2
]
= N ln N2 −
N
2
Tr [ln Aˆ ] , (237)
so that
Γ2 = (N2)
N e
−
N
2
T r[ln Aˆ ]
(238)
Substituting Eqn. 238 into Eqn. 232, we can write Γ1 as
Γ1(Aˆ ) = CΓ1 ∫Aˆ dµ(Aˆ )e
NT r[AAˆ M]
e
−
N
2
T r[ln Aˆ ]
, (239)
where
CΓ1
∶= N1e
N2
. (240)
We can now evaluate the integral in Eqn. 232 over the Lagrange multiplier Aˆ (i.e., ∫Aˆ ). If
we call this Γ1(Aˆ ), then (following Tanaka [84]) we can define the Rate Function I(Aˆ , AM) such
that
Γ1(Aˆ ) = ∫Aˆ
dµ(Aˆ )e
−NI(Aˆ ,AM)
, (241)
where
I(Aˆ , AM) = − Tr [AAˆ M] +
1
2
Tr [ln Aˆ ] . (242)
We can formally evaluate the integral in Eqn. 241 in the large-N limit using a Saddle Point
Approximation (SPA) (see Section 4.2, Eqn. 68), as
Γ1(Aˆ ) →
¿ÁÁÀ(2π)
N/2
N∥I∥
e
−NI∗
(Aˆ ,AM)
, (243)
where I
∗
(Aˆ , A) is the maximum value over all Aˆ for fixed AM, obtained using
I
∗
(Aˆ , AM) ∶= lim
N≫1
I(Aˆ , AM) = sup
Aˆ
[− Tr [AAˆ M] +
1
2
Tr [ln Aˆ ]] , (244)
122
where at the SPA we have stationarity,
∂
∂Aˆ
I(Aˆ , AM) = − AM +
1
2Aˆ
= 0 (245)
and the Hessian of I becomes
∂
2
∂Aˆ2
I(Aˆ , AM) = −
1
2
(
1
2
Aˆ−1
) ⊗ (
1
2
Aˆ−1
) = −
1
8
AM ⊗ AM (246)
where ⊗ is the Kronecker product. Using Eqn. 245, we have that 1/8 AM ⊗AM is the saddle point
Hessian of I w.r.t. AM. Solving the SPA equation, we find that the prefactor (i.e. Hessian) is
given as det (−
1
8AM ⊗ AM) = (−
1
8
)
M2
(det (AM))M.
Substituting for Aˆ into Rate Function (Eqn. 242), I becomes
I
∗
(Aˆ , AM) = − Tr[IM] +
1
2
Tr[ln AM] (247)
= −M +
1
2
Tr[ln AM].
In order for this result to be physically meaningful, we need that if I
∗
(Aˆ , AM) grows, then it
must grow slower than N, and, more importantly, that det (A) be non-zero. Importantly, when
det (A) = 1 exactly, however, then Γ1 becomes a constant, and this simplifies things considerably!
A.4.2 Casting the Generating Function (βΓ
IZ
Q¯2 ) as an HCIZ Integral
In this section, we express the Generating Function βΓ
IZ
Q¯2
, given in Eqn. 118 (equivalently, in
Eqn. 113,) as an HCIZ Integral, as given in Eqn. 122.
Inserting I
∗
(Aˆ , A) from Eqn. 247 into βΓ
IZ
Q¯2
, we obtain
βΓ
IZ
Q¯2 = ln [CΓ1
e
−NM ∫ dµ(A)e
nβNT r[
1
N
T⊺ANT]
e
N
2
ln(det(AM))] (248)
= ln CΓ1 − NM + ln [∫ dµ(A)e
nβNT r[
1
N
T⊺ANT]
e
N
2
ln(det(AM))] .
So long as the second term Tr[IM] does not depend on N, it will vanish when we take the partial
derivative of βΓ
IZ
Q¯2
to obtain the E¯NN
gen , in which case it is not important. We can then simply
write the Generating Function βΓ
IZ
Q¯2 as in Eqn. 119 as:
βΓ
IZ
Q¯2 = ln [∫ dµ(AM)e
nβNT r[
1
N
T⊺ANT]
e
N
2
ln(det(AM))] , (249)
or, in Bra-Ket notation, as
βΓ
IZ
Q¯2 = ln ⟨e
nβNT r[
1
N
T⊺ANT]
e
N
2
ln(det(AM))⟩
AM
. (250)
A.5 MLP3 Model Details
The empirical MLP3 Model implements the assumptions described in Section 5 used the following
procedures:
A three-layer Multi-Layer Perceptron was trained for classification on the MNIST dataset[117].
The first Fully Connected (FC) hidden layer has 300 units, the second FC hidden layer has 100
123
units, and the third FC layer has ten units for classification, matching the ten digit classes of
MNIST. Input images are grayscale, and were rescaled to the [0, 1] range. Following the keras[130]
defaults, the weights were initialized using the Glorot Normal[131] method, and the biases were
initialized to 0. Each model was trained using Categorical Cross Entropy as the loss function.
The loss function was summed over each mini-batch, which is the default behavior for Keras,
rather than being averaged, which is the default for pytorch[132].
Optimization was carried out by either Stochastic Gradient Descent (SGD) without momentum, or the Adam algorithm [133]. The Learning Rate (LR) was set to 0.01 for SGD, and 0.001 for
Adam. The LR was held constant, i.e., there was no decay schedule. Each algorithm proceeded
epoch by epoch until the value of the loss function did not decrease by more than 0.0001 for three
consecutive epochs. At each epoch, the WeightWatcher tool was used to compute metrics for
each layer. Loss values reported are the average loss per labeled example, and not the summed
loss over each minibatch. Training loss is averaged over all batches in the epoch, whereas test
loss is evaluated once at the end of the epoch.
In some experiments, only one layer was trained, while the others were left frozen. In other
experiments all layers were trained. Models were trained using a series of mini-batch sizes ranging
from 1 to 32. For each separate training run, the models were re-initialized to the same starting
random weights, all random seeds were reset, and deterministic computations were used to train
the models.
Separate notebooks are provided for keras and pytorch implementations of the experiments.
A.6 Tanaka’s Result
In this section, we will rederive the result by Tanaka [83, 84] that we use in our main derivation,
and, importantly, explain how to address the missing Temperature term. For completeness, we
restate it here using the notation of the main text:
lim
N≫1
1
N
ln ⟨exp (
nβ
2
Tr [W⊺ANW])⟩
A
´ ¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¸¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¶
HCIZ Integral
=
nβ
2
M
∑
i=1
GA(λi) (251)
where W is the N ×M Teacher weight matrix, A = AN is the N ×N Student (correlation) matrix,
but β is now the inverse-Temperature (because we are working with real matrices), n is the size
of the training set, and we have added the (
1
2
) prefactor (which will be clear later). GA(λ) is
a complex analytic function of the eigenvalues λ of (the Teacher Correlation matrix) X, whose
functional form will depend on the structure of the limiting form of (the Student) ESD ρ
∞
A(λ). We
may also write it as GA(X) below. We call it perhaps somewhat imprecisely a Norm Generating
Function because the final results for the Layer Quality Q¯ will take the form of a Tail norm in
many cases.
To apply this result, we note that while the term β is just a constant in [84] (1 or 2, depending
on whether the random matrix is real or complex), it is not actually inverse Temperature β =
1
T
in the original derivation. Still, we seek a final result that is linear in β =
1
T
, so that we can
easily evaluate Q¯2
in the high-T limit, i.e. Q¯2 =
∂
∂n
1
β
βΓ
IZ
Q¯2,N≫1
=
∂
∂β
1
n
βΓ
IZ
Q¯2,N≫1
(see 11). We can
introduce the new term nβ by simply changing the scale of AN since the final result is a sum of
R-transforms, which by definition are linear, i.e., GA(nβλ) = nβGA(λ), however, it is instructive
to rederive the final result, with nβ explicitly included.
124
Notation. We start by rewriting the Tanaka result, Eqn. (251), in our notation for the expected
value ⟨⋯⟩A operator, as follows:
1
2N
βΓ
IZ
Q¯2,N≫1
=
1
N
lim
N≫1
ln∫ dµ(A) [exp (
nβ
2
Tr [W⊺ANW])]
´ ¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¸¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¶
HCIZ Integral
= nβ 1
2
M
∑
i=1
GA(λi). (252)
where we have added a 1
2
for technical convenience (to make the connection with the LDP, below).
If we denote the internal HCIZ integral as
Z
IZ ∶= ∫ dµ(A) [exp (
nβ
2
Tr [W⊺ANW])] , (253)
then it holds that
βΓ
IZ
Q¯2
∶=
1
N
lnZ
IZ
, (254)
from which it follows that
βΓ
IZ
Q¯2,N≫1
∶= lim
N≫1
1
N
lnZ
IZ
. (255)
The SPA approximates the Partition Function Z
IZ, which is now an HCIZ integral, by its
peak value. For this, GA(λ) itself must either not explicitly depend on N and/or at least not
grow faster than N.
The trick here is we can choose an R-transform of A that is a simple analytic expression based
on the observed empirical spectral density (ESD) of the X. And this can readily be done for the
ESDs for a wide range of layer weight matrices observed in modern DNNs because their ESDs are
Heavy-Tailed Power Law [63]. We can then readily express the Quality Q¯ of the Teacher layer in
a simple functional form, (i.e an approximate Shatten Norm).
Importantly, the matrices X and A must be well approximated by low rank matrices since
the derivation in Tanaka requires this. Fortunately, this appears to be generally true for the
layers in very well trained DNNs, which is what allows us to apply this withing the ECS. In fact,
technically we need to integrate over dµ(A˜ ); this is straightforward as this simply changes the
lower bound on the integral from 0 → λ˜min, both above and in the subsequent derivation.
Finally, we note that GA(X) is kind of Generalized Norm because it can be evaluated as a
sum over a function of the M eigenvalues λµ of the Teacher correlation matrix X =
1
NW⊺W.
GA(X) will turn out to be an expression similar to the Frobenius Norm or the Shatten Norm
of X, depending on the functional form we choose to represent the limiting form of the Student
ESD, ρ
∞
A(λ) and the associated R-transform R(z), and various approximations made thereafter,
A.6.1 Setup and Outline
To evaluate 252, we want to integrate over all Student Correlation matrices A that “resemble”
the Teacher Correlation matrix X. To formalize this idea, we need to define the measure over “all
desired” A, dµ(A), in terms of the actual M eigenvalues, {λi}
M
i=1
, of the Teacher. And WLOG,
the final result can be restricted to the M˜ eigenvalues of the Effective Correlation Space (ECS).
Randomness assumption. For real weights W, we assume an orthogonally invariant ensemble, dµ(W) = dµ(UWU⊺
) for all U ∈ O(M), mirroring the isotropic Gaussian initialisation
widely used in neural networks. Crucially, Tanaka’s large-N analysis shows the resulting HCIZ
exponent depends only on the eigenvalue spectrum, so the final integrated–R expression should
remain applicable even when full rotational invariance is later broken in training.
125
Using a source matrix D to represent dµ(A) with dµ(W). We consider all matrices A
with the same limiting spectral density, ρ
∞
A(λ), as the limiting (empirical) ESD of the Teacher.
That is, we want ρ
∞
A(λ) = ρ
∞W(λ), where T = W. Of course, there are infinitely many weight
matrices W with the same M eigenvalues, {λi}
M
i=1
, as the Teacher. Let us specify these matrices
with the measure dµ(W). Doing this lets us then write the measure dµ(A) in terms of dµ(W)
as:
dµ(A) ∶= e
−
nβ
2
T r[WDW⊺
]
dµ(W), (256)
where D is some M × M matrix, called the Source Matrix, to be specified below, and the 1
2
here
as well. Indeed, the key idea here will be to define D in such a way as to obtain the desired final
result. Notice also that we have added an nβ term; this will be factored out later.
We can now represent the partition function Z
IZ, by inserting Eqn. 256 into Eqn. 253. Z
IZ
is now defined as an integral over all possible (Teacher) weight matrices W
Z
IZ
= ∫ dµ(W) exp [
nβ
2
( Tr [W⊺ANW] − Tr [WDW⊺
])] , (257)
Observe that this integral only converges when all the eigenvalues of D, {ϑµ}
M
µ=1
, are larger than
the maximum eigenvalue of A, i.e., when ϑµ > λmax, for µ ∈ [1,M] (although below this will
become nβϑµ > λmax). Later, we will place D in diagonal form, and we will obtain an explicit
expression for its M eigenvalues in terms of the M non-zero eigenvalues of X. The eigenvalues
of D will turn out to Lagrange Multipliers, needed later.
The Saddle Point Approximation (SPA) and the Large Deviation Principle (LDP).
To evaluate the large-N case of βΓ
IZ
Q¯2
(see 254, 255), we assume that the distribution of possible
Teacher correlation matrices, µ(X), satisfies a Large Deviation Principle (LDP). An LDP applies
to probability distributions that take an exponential form, such that µ(X) = e
−NI(X)dµ(X),
where I(X) is Entropy or Rate function I(X).
In applying an LDP, we effectively restrict measure of student correlation matrices A to those
most similar to the empirically observed Teacher correlation matrix X. We expect the measure
over all Teacher correlation matrices follows an LDP because the ESD is far from Gaussian, the
dominant generalizing components reside in the tail of the ESD, and at finite-size the tail decays
at worst as an exponentially Truncated Power Law (TPL).
Three 3 steps to evaluate ⟨Z
IZ⟩A
in the Wide Layer large-N approximation. The goal
is to start with Eqn. 257 and obtain two separate, equivalent relations, Eqns. 258 and 260:
1. Obtaining an integral transform of ρ
∞
A(λ). First, we expand and reduce Eqn. 257
and evaluate the expected value of EA[Z
IZ] = EAN
[Z
IZ] in the Large-N limit in N by
expressing the ρA(λ) for the N×N matrix A = AN =
1
N
SS⊺
in the continuum representation,
i.e., as ] ρ
emp
A (λ) → ρ
∞
A(λ), to obtain:
lim
N≫1
1
N
lnEAN
[Z
IZ] = M ln(
2π
nβ
) −
M
∑
µ=1
∫ ln(δµ − λ)ρ
∞
A(λ)dλ. (258)
This gives us an EAN
[Z
IZ] in terms of an integral transform ρ
∞
A(λ), which we can model.48
48This integral of ρ
∞
A(λ) is related to the Shannon Transform, an integral transform from information theory
that is useful when analyzing the mutual information or the capacity of a communication channel [83].
126
2. Forming the Saddle Point Approximation (SPA). We evaluate Eqn. 257 as the expected value of EA[Z
IZ] = EAM [Z
IZ] for the M ×M matrix A = AM =
1
N
S
⊺S (but explicitly
in terms of dµ(X)). Then, taking in the large-N approximation using the SPA, (and which
can be done implicitly using the LDP), we obtain
lim
N≫1
EAM [Z
IZ] ≃ ∫ exp (nβN Tr[G(X)]) dµ(X) ≈ exp(nβNG
max) (259)
where G(X) depends on GA(X), and G
max = supX G(X). We can then write
lim
N≫1
1
N
lnEAM [Z
IZ] ≈ nβG
max
, (260)
3. Finding the Inverse Legendre Transform. To do this, we now equate
lim
N≫1
1
N
lnEAM [Z
IZ] = lim
N≫1
1
N
lnEAN
[Z
IZ] (261)
Then, we can form the inverse Legendre transform which we will let us relate GA(λ) in
Eqn. 251 to the integrated R-transform of ρ
∞
A(λ).
(See A.6.4.)
A.6.2 Step 1. Forming the Integral Transformation of ESD (ρ
∞
A(λ))
We first establish Eqn. 258, in Steps 1.1−1.4. This is done by changing variables under a Unitary
transformation, W → Wˇ , evaluating the resulting functional determinant, and then taking the
continuum limit of the ESD ρ˜A(λ) → ρ
∞
A(λ).
Step 1.1 To do so, let us first assume that Teacher correlation matrix X and the source matrix
D are simultaneously diagonalizable (i.e., their commutator is zero: [X, D] = 0). In this case, we
may write the generating function Z
IZ in Eqn. 257 as
Z
IZ
= ∫ dµ(W) exp
nβ
2
( Tr [W⊺U⊺ΛUW] − Tr [WV⊺∆VW⊺
] ), (262)
where we have defined
AN = U⊺ΛU, D = V⊺∆V, (263)
where U (N × N) and V (M × M) are Unitary matrices. Since U⊺U = I and V⊺V = I, we can
insert these identities into Z
IZ in 262, giving
Z
IZ
= ∫ dµ(W) exp
nβ
2
× (264)
( Tr [(V⊺V)W⊺U⊺ΛUW(V⊺V)] − Tr [(U⊺U)WV⊺∆VW⊺
(U⊺U)] ).
We can identify the reduced weight matrix Wˇ as
Wˇ = UWV⊺
, Wˇ ⊺
= VW⊺U⊺
, (265)
Rearranging parentheses, this gives
Z
IZ
= ∫ dµ(W) exp
nβ
2
× (266)
( Tr [VT
(VWT UT
)Λ(UWVT
)V] − Tr [U⊺
(UWV⊺
)∆(VW⊺U⊺
)U] ).
127
We can now express Z
IZ in terms of Wˇ as
Z
IZ
= ∫ dµ(W) exp
nβ
2
( Tr [V⊺Wˇ ⊺ΛWVˇ ] − Tr [U⊺W∆ˇ Wˇ ⊺U] ). (267)
Since the Trace operator Tr[⋅] is invariant to Unitary (Orthogonal) transformations, we can now
remove the U and V terms, giving the simplified expression for our generating function Z
IZ in
terms of the two diagonal matrices Λ,∆, the reduced weight matrix Wˇ , and the Jacobian J(Wˇ )
transformation for dµ(W) → dµ(Wˇ ), as:
Z
IZ
= ∫ dµ(Wˇ )J(Wˇ ) exp
nβ
2
( Tr [Wˇ ⊺ΛWˇ ] − Tr [W∆ˇ Wˇ ⊺
] ). (268)
Step 1.2 We can now evaluate the integral using the standard relation for the functional determinant for infinite-dimensional Gaussian integrals [102]
Z
IZ
= (
2π
nβ )
NM/2
det (∆ − Λ)
−1/2
(269)
where the Jacobian is unity for the Unitary transformation.
J(Wˇ ) = 1. (270)
since W ↦ Wˇ is an orthogonal transfomation. We now use the standard Trace-Log-Determinant
relation [102]
Tr[lnM] = ln detM. (271)
Let us insert (exp ln) on the R.H.S. of 269, to obtain
Z
IZ
= exp ln [ ( 2π
nβ )
NM/2
det (∆ − Λ)
−1/2
]
= exp [ (NM
2
) ln 2π
nβ
−
1
2
Tr[ln (∆ − Λ)] ]
= exp [
NM
2
ln 2π
nβ
−
1
2
ln det (∆ − Λ) ]. (272)
Step 1.3 We now want to express the generating function Z
IZ in 272 in terms of an integral
over the continuous limiting spectral density ρA(λ) of the correlation matrix AN.
First, we express the Determinant of the matrix ∆ − Λ in terms of discrete eigenvalues:
det (∆ − Λ)
−1/2
=
M
∏
µ=1
N
∏
i=1
(ϑµ − λi)
−1/2
. (273)
This gives the Log-Determinant in terms of the M (non-zero) eigenvalues of D and AN, as
ln det (∆ − Λ)
−1/2
= −
1
2
M
∑
µ=1
N
∑
i=1
ln (ϑµ − λi) . (274)
We can express the ESD, ρ˜A(λ), of the Student Correlation matrix AN in terms of the Dirac
delta-function, δ(x), as
ρ˜A(λ) =
1
N
N
∑
i=1
δ(λ − λi). (275)
128
Using this, the Expected Value of the Log-Determinant in 274 can be expressed in terms of the
ESD of AN as
⟨ln det (∆ − Λ)
−1/2
⟩
AN
= −
1
2
M
∑
µ=1
N
∑
i=1
∫ dλ ln(ϑµ − λ)δ(λ − λi)
= −
1
2
M
∑
µ=1
∫ dλ ln(ϑµ − λ)
N
∑
i=1
δ(λ − λi)
= −
1
2
M
∑
µ=1
∫ dλ ln(ϑµ − λ)Nρ˜A(λ). (276)
Let us insert this back into our expression for the generating function, 272, giving EAN
[Z
IZ]
in terms of the ESD ρ˜A as
EAN
[Z
IZ] = exp {
N
2
[M ln 2π
nβ
−
M
∑
µ=1
∫ dλ ln(ϑµ − λ)ρ˜A(λ)]}. (277)
We can now replace the sum over the N eigenvalues λi with an integral over the limiting ESD,
ρ(λ), to obtain
ρ
∞
A(λ) = lim
N→∞
ρ˜A(λ). (278)
Observe that this effectively means that we are taking a Large-N limit in N, N ≫ 1. This lets
us write the Expected Value of the generating function Z
IZ in 277 as
lim
N≫1
EAN
[Z
IZ] = exp {
N
2
[M ln 2π
nβ
−
M
∑
µ=1
∫ dλ ln(ϑµ − λ)ρ
∞
A(λ)]} (279)
Step 1.4 Using the Self-Averaging Property,
lnEAN
[Z
IZ] ≃ ⟨lnZ
IZ⟩AM
, (280)
It follows from Eqn. 279 that
lim
N≫1
lnEAN
[Z
IZ] ≃
NM
2
ln 2π
nβ
−
N
2
M
∑
µ=1
∫ dλ ln(ϑµ − λ)ρ
∞
A(λ). (281)
The N-dependence now cancels out, and we are left an approximate expression due to the remaining dependence of the continuum limiting density ρ
∞
A(λ) (for A = AN)
lim
N≫1
2
N
lnEAN
[Z
IZ] = M ln 2π
nβ
−
M
∑
µ=1
∫ dλ ln(ϑµ − λ)ρ
∞
A(λ). (282)
This completes the derivation of Eqn. 258; we have an expression for the expected value of Z
IZ,
evaluated in the Large-N (continuum) limit in N.
A.6.3 Step 2: The Saddle Point Approximation (SPA): Explicitly forming the Large
Deviation Principle (LDP)
We now evaluate EA[Z
IZ] in Eqn. 259 as EAN
[Z
IZ] to establish Eqn. 260, .
Using the LDP (and following similar approaches in spin glass theory [96]), below we will
show that we can write the expected value of Z
IZ in terms of dµ(X) now (which is equivalent to
dµ(AM)) and in the large-N approximation, as
lim
N≫1
EAM [Z
IZ] = ∫ exp (nβN Tr[GA(X)] − NI(X) + o(N)) dµ(X) (283)
where I(X) is Rate Function, defined below, and GA(X) is what we are eventually solving for.
129
Step 2.0 We start with the expected Partition Function
EAN
[Z
IZ] = ∫ dµ(A)∫ dµ(W) exp[
nβ
2
Tr [W⊺ANW] −
nβ
2
Tr [WDW⊺
]]. (284)
The average over AN affects only the first exponential; applying the SPA, we define a matrix
function G(X), depending solely on X =
1
NW⊺W, by
∫ dµ(A) exp[
nβ
2
Tr [W⊺ANW]] = exp[
nβN
2
Tr[G(X)]]. (285)
which will be valid in the large-N in N approximation below.
We now note that given the duality of measures, we can assert
EA[Z
IZ] = EAM [Z
IZ] = EAN
[Z
IZ]. (286)
This lets us insert (285) into (284) and then write
EA[Z
IZ] = ∫ dµ(W) exp[
nβN
2
Tr[G(X)] −
nβ
2
Tr [WDW⊺
]]. (287)
The now need to determine an explicit form for G(X). We introduce a new change of measure,
dµ(W) → dµ(X). Then, we show this lets us express EA[Z
IZ] as EX[Z
IZ] and to express it using
the LDP. Next, we apply a SPA to solve for G
max = max GA. Importantly, we also show how to
incorporate the inverse-Temperature nβ, which is new.
Step 2.1 To define the transformation dµ(W) → dµ(X), where (recall) X =
1
NW⊺W, we use
the (again) the integral representation of the Dirac delta-function δ(x):
δ(x) ∶=
1
2π
∫
∞
−∞
e
ixxˆ
dx. ˆ (288)
This lets us express the transformation of measure dµ(W) → dµ(X) (approximately) as
dµ(W) ∶= δ(
1
2
Tr [NX − W⊺W])dµ(X)
=
1
2π
∫
∞
−∞
e
i
1
2
Tr[Xˆ(NX−W⊺W)]
dµ(X)dµ(Xˆ), (289)
where Xˆ is a scalar (or really a matrix of scalars), and we have 1
2
term for mathematical consistency below. 49
Step 2.2 Next, we take a Wick Rotation50
, iXˆ → −Xˆ , so that the terms under the integral are
all real (not complex), giving:
dµ(W) = NWick ∫
i∞
−i∞
e
1
2
Tr[Xˆ(NX−W⊺W)]
dµ(X)dµ(Xˆ). (290)
where dµ(Xˆ) is a measure over M(M−1)
2 Lagrange multipliers, and the normalization is
NWick = (
1
2πi)
M(M−1)
2
(291)
49The full change of measure would require a delta function constraint for each matrix element Xi,j , i.e.,
δ (
1
2N (Xi,j − [W⊺W]i,j )). Here, we assume the Trace constraint is sufficient for our level of rigor.
50The Wick rotation converts an oscillatory integral into an exponentially decaying one which should be well
defined. Technically, this is an analytic continuation which needs to be checked, but following standard practice in
physics we will assume the resulting integral is analytic and therefore well defined and we will proceed onward.
130
Step 2.3 We now insert 290 into 287, which lets us express EA[Z
IZ] as an integral over the
Teacher Correlation matrices.
EA[Z
IZ] = NWick ∫X
∫
i∞
−i∞
e
N
nβ
2
Tr[G(X)]+
N
2
Tr[XˆX]
e
−
1
2
Tr[XˆW⊺W]
e
nβ
2
Tr[WDW⊺
]
dµ(Xˆ)dµ(X)
= NWick ∫X
∫
i∞
−i∞
e
N
nβ
2
Tr[G(X)]+
N
2
Tr[XˆX]
e
−
1
2
Tr[WXˆW⊺]+
nβ
2
Tr[WDW⊺
]
dµ(Xˆ)dµ(X)
= NWick ∫X
∫
i∞
−i∞
e
N
nβ
2
Tr[G(X)]+
N
2
Tr[XˆX]
e
1
2
Tr[W(nβD−Xˆ)W⊺]
dµ(Xˆ)dµ(X). (292)
Step 2.4 We can now rearrange terms to make this expression look like the Eqn. 283 In Large
Deviations Theory, the Rate Function is defined by the Legendre Transform,
I(X) = sup
Xˇ
[T r 1
2
X⊺Xˇ − lnM(Xˇ )] , (293)
where M(Xˇ ) is the Moment Generating Function, lnM(Xˇ ), is the Cumulant Generating Function,
and and Xˇ is a (matrix of) Lagrange Multiplier(s). M(Xˇ ) is defined in terms of the (unnormalized)
density p(x) as
M(Xˇ ) = exp (
1
2
x
⊺Xxˇ ) , p(x)dx (294)
which, in turn, is defined in terms of the source matrix D,
p(x) = exp (−
1
2
x
⊺nβDx) . (295)
The moment generating function M(Xˇ ) is then given by
M(Xˇ ) = ∫ exp (−
1
2
x
T
(nβD − Xˇ )x) dx = (2π)
M
2 det (nβD − Xˇ )
−
1
2
. (296)
Step 2.5 The Saddle Point Approximation (SPA) can be used to solve for I(Xˇ ) by solving for
the stationary conditions
∂
∂Xˇ
I(X, Xˇ ) = 0. (297)
First, let us compute lnM(Xˇ ) as:
lnM(Xˇ ) =
M
2
ln(2π) −
1
2
ln det (nβD − Xˇ ) . (298)
Substituting this into the expression for the Legendre transform, we obtain:
I(X, Xˇ ) = sup
Xˇ
[
1
2
Tr [XXˇ ] −
M
2
ln(2π) +
1
2
ln det (nβD − Xˇ )] . (299)
The supremum of this expression is attained at the value of Xˇ that satisfies:
∂
∂Xˇ
[
1
2
Tr [XXˇ ] +
1
2
ln det (nβD − Xˇ )] = 0. (300)
Taking the derivative, we obtain
1
2
X +
1
2
(nβD − Xˇ )
−1
= 0, (301)
131
which simplifies to:
X = (nβD − Xˇ )
−1 ⇒ Xˇ = nβD − X−1
. (302)
Substituting Xˇ = nβD − X−1 back into the expression for I(X), we obtain:
I(X) =
1
2
[ Tr [X(nβD − X−1
)] −
M
2
ln(2π) +
1
2
ln det (X−1
)] . (303)
Tr[XnβD − I] = Tr[XnβD] − N, (304)
ln det (X−1
) = −ln det (X) , (305)
we get:
I(X) =
1
2
[ Tr[XnβD] − ln det (X) − M − M ln(2π)]. (306)
Finally, we express I(X) in the form:
I(X) =
1
2
[−M(1 + ln(2π)) + Tr[XnβD] − ln det (X)]. (307)
Step 2.6
nβG(X) = M(1 + ln 2π) + nβ Tr[GA(X)] − Tr[XnβD] + ln det (X) . (308)
We restrict our solution to those where X and nβD can be diagonalized simultaneously. In
particular, this lets us write
Tr[XnβD] =
M
∑
µ=1
nβδµλµ, (309)
where nβδµ and λµ denote the eigenvalues of X and nβD, resp.
We can now write the maximum value of GA, G
max, as
nβG
max
= M (1 + ln 2π
nβ ) −
M
∑
µ=1
min
nβδµ
[nβδµλµ − nβGA(λµ) + ln λµ]. (310)
A.6.4 Expressing the Norm Generating Function (GA(λ)) as the Integrated Rtransform (R(z)) of the Correlation Matrix (A)
Having completed both steps, let us combine Eqns. 258, 282 with 260 and 310. We follow the
first arguments by Tanaka [83] (which follows Cherrier [134]).
M ln(
2π
nβ
) −
M
∑
µ=1
∫ ln(nβδµ − λ)ρ
∞
A(λ)dλ = M (1 + ln 2π
nβ ) −
M
∑
µ=1
min
nβδµ
[nβδµλµ − nβGA(λµ) + ln λµ].
(311)
By canceling the ln 2π
nβ term from both sides, we obtain
−
M
∑
µ=1
∫ ln(nβδµ − λ)ρ
∞
A(λ)dλ = M −
M
∑
µ=1
min
nβδµ
[nβδµλµ − nβGA(λµ) + ln λµ]. (312)
Since this is true for every µ, we can solve this for any arbitrary eigenvalue λµ.
Dropping the µ subscript, we have the following identity:
min
δ
[nβδλ − nβGA(λ) + ln λ] = 1 − ∫ ln(nβδ − λ)ρ
∞
A(λ)dλ. (313)
132
We need to invert 313 in order to find nβGA(λ). If we choose the eigenvalues of D such
that nβδµ > λmax for all µ, then this relation is concave and therefore invertible via a Legendre
transform.
This gives
nβGA(λ) = nβδ(λ)λ − ∫ ln[nβδ(λ) − λ]ρ
∞
A(λ)dλ − ln λ − 1, (314)
where we need to define nβδ(λ), which (not to be confused with the Dirac delta-function),
describes the functional dependence between the eigenvalues of the source matrix D and the
Student Correlation Matrix A.
GA(λ) is computed by minimizing over δ, ensuring the relationship holds for the entire spectrum. So let us take the derivative of nβGA w.r.t. λ. Term by term, this gives:
d
dλ
nβδ(λ)λ = nβδ(λ) +
dnβδ(λ)
dλ
λ (315)
d
dλ
ln λ =
1
λ
(316)
d
dλ ∫ ln[nβδ(λ) − λ]ρ
∞
A(λ)dλ = ∫
d
dλ
ln[nβδ(λ) − λ]ρ
∞
A(λ)dλ (317)
= ∫
dnβδ(λ)
dλ
ρ
∞
A(λ)
nβδ(λ) − λ
dλ
=
dnβδ(λ)
dλ ∫
ρ
∞
A(λ)
nβδ(λ) − λ
dλ
We can now simplify by defining δ(λ) implicitly by the integral relation
λ = ∫
ρ
∞
A(λ)
nβδ(λ) − λ
dλ. (318)
Combining terms, this gives
dnβGA(λ)
dλ
= nβδ(λ) −
1
λ
, (319)
Inverting the derivative, we obtain an integral equation for nβGA(λ)
nβGA(λ) = ∫
λ
0
(nβδ(z) −
1
z
) dz. (320)
Notice since nβδ(λ) ≈
1
λ
for λ ≪ 1, then as GA(0) = 0 and we set the lower integrand to 0
(for now). Even though Tanaka’s original proof assumes an analytic continuation without branch
cuts, a heavy-tailed spectrum merely shifts the lower limit of the R-transform integral, so the
expression for nβG(λ) continues to hold.
To further connect these to the R-transform RA(z), we recall that the Cauchy-Stieltjes (or
just Cauchy See 135) transform CA(z) is given by:
CA(z) = ∫
ρA(λ)
z − λ
dλ. (321)
The relationship between the Cauchy transform and the R-transform is then expressed as:
CA (RA(z) +
1
z
) = z, (322)
133
which implies:
nβGA(λ) = ∫
λ
0
RA(z)dz. (323)
WLOG, as mentioned earlier, we can replace the lower bound on λ from 0 → λ˜min to obtain
nβGA(λ) = ∫
λ
λ˜min
R[RA˜ (z])dz. (324)
where λ˜min corresponds to the start of the Effective Correlation Space (ECS), and, notably, take
the Real part of R(z). We take the Real part because the imaginary parts coming from the upper
and lower lips of the cut cancel.51
A.7 Existence of the Free R–Transform for Power-Law Spectra
In this subsection, we examine when the R-transform R(z) does and does not exist for power law
tails.
A.7.1 Analyticity criterion
Recall the Cauchy–Stieltjes transform defined in Eq. (135) of Sec. 5.4.1,
Gµ(z) = ∫
R
ρ(λ)
z − λ
dλ, z ∈ C ∖ suppµ. (325)
If Gµ(z) is holomorphic at z = ∞ one may invert the map z = Gµ(z) in a neighborhood of z = 0
and define
Rµ(z) = G
−1
µ
(z) −
1
z
. (326)
Obstructions to the existence of Rµ therefore coincide with non-analytic terms in the Laurent
series of Gµ about z = ∞. These can be easily removed, however, by considering that ρ(λ) is
always strictly bounded from above by λmax.
A.7.2 Model and notation
We first model a bare power-law tail regularized only by a hard lower cut-off λ0 > 0:
ρα(λ) = (α − 1) λ
α−1
0 λ
−α
, λ ≥ λ0, α ∈ {2, 3, 4}. (327)
Normalization is immediate:
∫
∞
λ0
ρα(λ) dλ = 1. (328)
The choice α ∈ {2, 3, 4} mirrors the Heavy-Tailed exponents most frequently observed in
neural-network weight and Hessian spectra.
51Alternatively, one might try to replace R(z) by its modulus; doing so breaks the Legendre relation G
′
= R and
spoils the additivity of free cumulants.
134
A.7.3 Stieltjes (Green’s) transform
For z ∈ C ∖ [0,∞)
Gα(z) = ∫
∞
λ0
ρα(λ)
z − λ
dλ. (329)
Carrying out the integration yields
G(2)(z) =
1
z
+
λ0 ln(1 −
z
λ0
)
z
2
, (330)
G(3)(z) =
1
z
+
2λ0
z
2
+
2λ
2
0
ln(1 −
z
λ0
)
z
3
, (331)
G(4)(z) =
1
z
+
3λ0
2 z
2
+
3λ
2
0
z
3
+
3λ
3
0
ln(1 −
z
λ0
)
z
4
. (332)
The logarithmic pieces carry the entire heavy-tail fingerprint; their placement in the expansion
dictates whether R(z) will be available.
A.7.4 Moments and free cumulants
Algebraic moments exist only up to order α − 2:
mk = ∫
∞
λ0
λ
k
ρα(λ) dλ =
α − 1
α − k − 1
λ
k
0
, k < α − 1. (333)
Hence
m1 =
α − 1
α − 2
λ0, m2 =
α − 1
α − 3
λ
2
0 α > 3). (334)
The first two free cumulants are κ1 = m1 and κ2 = m2 − m2
1
.
A.7.5 R-transform for the bare tail
Define w = Gα(z) and solve locally for R(z). Using the finite cumulants one obtains
• α = 2: the logarithm appears at order z
−2
; G(z) is not analytic at infinity and the inversion
fails. Conclusion: no R–transform. Truly 1/λ
2
tails are outside the remit of free addition.
• α = 3:
R(3)(z) = 2 λ0 constant). (335)
Only a zeroth-order free cumulant survives, but that is enough for free convolution.
• α = 4:
R(4)(z) =
3
2
λ0 +
3
4
λ
2
0 w. (336)
Here the series truncates after the linear term; higher cumulants diverge.
These three cases show explicitly how incremental changes in the tail exponent alter the
analytic status of G(z) and hence of R(z). Since the higher-order free cumulants diverge, the
R-transform cannot be a finite polynomial. It must either be an infinite series (where the terms
beyond a certain point don’t vanish) or, more strongly, exhibit non-analytic behavior (like the
logarithmic terms present in G(z)) because its Taylor series coefficients (the cumulants) become
infinite. Fortunately, R(z) can be defined if we ensure that ρ(λ) has compact support.
135
A.7.6 Truncated α = 2 power law
Introduce a cut-off λmax > λ0 and set
ρtr(λ) = C λ−2
, λ0 ≤ λ ≤ λmax, C =
1
λ
−1
0
− λ−1
max
. (337)
Exact integration gives
Gtr(z) = C
⎡
⎢
⎢
⎢
⎢
⎣
log λmax − log(λ − z) − log λ0 + log λ0 − z)
z
2
−
1
λmax z
+
1
λ0 z
⎤
⎥
⎥
⎥
⎥
⎦
, (338)
valid for z ∈ C ∖ [λ0, λmax].
Expansion at z = ∞. Using log(λ − z) = log z + log(1 − λmax/z) and log(λ0 − z) = log z + log(1 −
λ0/z), the two log z terms cancel and we find the regular series
Gtr(z) =
1
z
+
m1
z
2
+
m2
z
3
+ . . . , z → ∞, (339)
with finite moments of all orders. Consequently
Rtr(z) = G
−1
tr (z) −
1
w
(340)
is analytic for ∣w∣ small.
Interpretation. A truncation at any physically reasonable λmax—for instance the largest
finite eigenvalue observed in a data set—instantly restores full analyticity at infinity. From the
point of view of free probability the system now behaves as if it had all moments, even though
the raw tail is still 1/λ
2 within [λ0, λmax].
A.7.7 Key points and implications for SETOL
1. Cutting the tail at λmax = λ
ECS
max removes the non-analytic log z/z
2
obstruction and turns
the free-probability machinery back on.
2. Any model density ρ(λ) with compact support has G(z) analytic at z = ∞; hence its
R–transform equals the usual free-cumulant series and is available for algebraic manipulation.
3. In all theoretical derivations and numerical experiments in SETOL we therefore model empirical spectra as effectively as truncated power laws (i.e with finite bounds, not
necessarily exponentially truncated). This choice is both empirically justified (no spectrum
is truly infinite) and mathematically essential: it guarantees that R(z) always exists.
A.7.8 Explicit R–transforms for the truncated tail
We can also provide exact expressions for the leading terms (free cumulants) in R(z) for alpha =
2, 3, 4. The compact support [λ0, λmax] = [λ˜min, λECS
max ] guarantees that every algebraic moment
(and free cumulant) is finite. Hence the Voiculescu series
R
tr
α (z) =
∞
∑
n=1
κn z
n−1
, ∣z∣ small, (341)
136
converges for every α > 1. Below we list the free cumulants κ1, κ2 and the resulting (R(z)
for the three exponents most relevant to SETOL. Higher cumulants follow from the recursion in
Eqs. (333)–(334) and need not be written out.
—
α = 2.
C2 =
λ0λmax
λmax − λ0
, (342)
κ1 = C2 logλmax
λ0
, (343)
κ2 = C2 λmax − λ0) − κ
2
1
, (344)
R
tr
(2)
(z) = κ1 + κ2 z + O(z
2
) . (345)
α = 3.
C3 =
2 λ
2
0λ
2
max
λ2
max − λ
2
0
, (346)
κ1 =
2 λmaxλ0
λmax + λ0
, (347)
κ2 = C3 logλmax
λ0
− κ
2
1
, (348)
R
tr
(3)
(z) = κ1 + κ2 z + O(z
2
) . (349)
α = 4.
C4 =
3 λ
3
0λ
3
max
λ3
max − λ
3
0
, (350)
κ1 =
3 λmaxλ0 (λ
2
max − λ
2
0
)
2 (λ3
max − λ
3
0
)
, (351)
κ2 = C4 (
1
λ0
−
1
λmax
) − κ
2
1
, (352)
R
tr
(4)
(z) = κ1 + κ2 z + O(z
2
) . (353)
—
Interpretation. κ1 fixes the mean scale of the heavy tail; κ2 sets its leading spread. Because
both depend only on the empirical cut-offs λ0 and λmax, the two–term truncation already delivers
an accurate R–transform for free-probability manipulations inside the SETOL framework.
137
A.8 The Inverse-MP (IMP) Model
In this section, we rederive the integral G(λ)[IMP] for the Inverse Marchenko-Pastur (IMP)
model, focusing on the branch cut starting at z = κ/2 and extending to infinity. This branch cut
corresponds to the support of the ESD in this region. We will:
1. Explain the presence of the branch cut and its implications.
2. Show that R(z)[IMP] becomes complex along this branch cut because the term under the
square root becomes negative.
3. Perform the integral G(λ)[IMP], showing all steps.
4. Compute the Real part R[G(λ)[IMP]]
A.8.1 The Branch Cut in the IMP Model
The R-transform for the IMP model is given by:
R(z)[IMP] =
κ −
√
κ(κ − 2z)
z
, (354)
where κ > 0 is a parameter related to the dimensions of the random matrices under consideration.
The function √
κ(κ − 2z) introduces a branch point at z = κ/2 because the argument of the square
root becomes zero at this point:
κ − 2z = 0 ⇒ z =
κ
2
. (355)
For z > κ/2, the argument κ − 2z becomes negative, and thus the square root becomes imaginary.
This leads to a branch cut starting at z = κ/2 and extending to z = ∞ along the real axis. This
branch cut affects the analyticity of R(z)[IMP], and it must be carefully considered in the
integral G(λ)[IMP].
A.8.2 R(z)[IMP] is Complex Along the Branch Cut
For z > κ/2, we have:
κ − 2z < 0 ⇒
√
κ(κ − 2z) =
√
−κ(2z − κ) = i
√
κ(2z − κ). (356)
Therefore, R(z)[IMP] becomes complex:
R(z)[IMP] =
κ − i
√
κ(2z − κ)
z
=
κ
z
− i
√
κ(2z − κ)
z
. (357)
This expression shows that R(z)[IMP] has both real and imaginary parts when z > κ/2.
A.8.3 Calculation of G(λ)[IMP]
We aim to compute the integral:
G(λ)[IMP] ∶= ∫
λ
λ0
R[R(z)[IMP]], dz, (358)
where λ0 ≥ κ/2.
138
Where the Real part of R(z)[IMP] is
Re[R(z)[IMP]] =
κ
z
. (359)
The integral of this is
R[G(λ)[IMP]] = ∫
λ
λ0
κ
z
, dz = κ [ln z]
λ
λ0
= κ (ln λ − ln λ0) . (360)
139